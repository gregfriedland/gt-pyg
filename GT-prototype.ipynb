{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. https://github.com/xbresson/CS6208_2023/blob/main/codes/labs_lecture07/01_vanilla_graph_transformers.ipynb\n",
    "2. https://github.com/xbresson/CS6208_2023/blob/main/codes/labs_lecture07/03_graph_transformers_regression_exercise.ipynb\n",
    "3. https://github.com/pgniewko/pytorch_geometric/blob/master/torch_geometric/nn/conv/transformer_conv.py\n",
    "4. https://arxiv.org/abs/2012.09699\n",
    "5. https://arxiv.org/abs/1703.04977\n",
    "\n",
    "TDC:\n",
    "1. Test the model on this new set: https://practicalcheminformatics.blogspot.com/2023/06/getting-real-with-molecular-property.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch is available but CUDA is not. Defaulting to SciPy for SVD\n",
      "INFO:rdkit:Enabling RDKit 2022.09.5 jupyter extensions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rdkit version: 2022.09.5\n",
      "Torch version: 1.13.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb7a6e30d10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import weightwatcher as ww\n",
    "\n",
    "from tdc.single_pred import ADME\n",
    "from rdkit import RDLogger\n",
    "from rdkit import rdBase\n",
    "from rdkit import Chem\n",
    "import rdkit\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter.composite import scatter_softmax\n",
    "from torch_scatter.scatter import scatter_add\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn.aggr import MultiAggregation\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Dataset\n",
    "import os.path as osp\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Turn off majority of RDKit warnings\n",
    "from rdkit import RDLogger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "print(f'Rdkit version: {rdkit.__version__}')\n",
    "print(f'Torch version: {torch.__version__}')\n",
    "\n",
    "from gt_pyg.data.utils import get_tensor_data, get_node_dim, get_edge_dim, get_train_valid_test_data\n",
    "\n",
    "from gt_pyg.nn.model import GraphTransformerNet\n",
    "\n",
    "\n",
    "torch.manual_seed(192837465)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the ADME@TDC data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 628\n",
      "Number of validation examples: 89\n",
      "Number of test examples: 178\n"
     ]
    }
   ],
   "source": [
    "PE_DIM = 6\n",
    "(tr, va, te) = get_train_valid_test_data('Caco2_Wang', min_num_atoms=PE_DIM + 1)\n",
    "tr_dataset = get_tensor_data(tr.Drug.to_list(), tr.Y.to_list(), pe_dim=PE_DIM)\n",
    "va_dataset = get_tensor_data(va.Drug.to_list(), va.Y.to_list(), pe_dim=PE_DIM)\n",
    "te_dataset = get_tensor_data(te.Drug.to_list(), te.Y.to_list(), pe_dim=PE_DIM)\n",
    "NODE_DIM = get_node_dim()\n",
    "EDGE_DIM = get_edge_dim()\n",
    "\n",
    "print(f'Number of training examples: {len(tr_dataset)}')\n",
    "print(f'Number of validation examples: {len(va_dataset)}')\n",
    "print(f'Number of test examples: {len(te_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(tr_dataset, batch_size=128)\n",
    "val_loader = DataLoader(va_dataset, batch_size=128)\n",
    "test_loader = DataLoader(te_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and eval the GT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 356\n",
      "GraphTransformerNet(\n",
      "  (node_emb): Linear(in_features=79, out_features=200, bias=True)\n",
      "  (edge_emb): Linear(in_features=10, out_features=200, bias=True)\n",
      "  (pe_emb): Linear(in_features=6, out_features=200, bias=True)\n",
      "  (gt_layers): ModuleList(\n",
      "    (0): GTConv(200, 200, heads=8)\n",
      "    (1): GTConv(200, 200, heads=8)\n",
      "    (2): GTConv(200, 200, heads=8)\n",
      "    (3): GTConv(200, 200, heads=8)\n",
      "  )\n",
      "  (global_pool): MultiAggregation([\n",
      "    SumAggregation(),\n",
      "    MeanAggregation(),\n",
      "    MaxAggregation(),\n",
      "    StdAggregation(),\n",
      "  ], mode=cat)\n",
      "  (mu_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=800, out_features=200, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=200, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (std_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=800, out_features=200, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=200, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of params: 1954k\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphTransformerNet(node_dim_in=NODE_DIM,\n",
    "                            edge_dim_in=EDGE_DIM,\n",
    "                            pe_in_dim=PE_DIM,\n",
    "                            num_gt_layers=4, \n",
    "                            hidden_dim=200,\n",
    "                            norm='bn',\n",
    "                            aggregators=['sum','mean','max', 'std'],\n",
    "                            dropout=0.1,\n",
    "                            act='gelu').to(device)\n",
    "\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    model = torch_geometric.compile(model) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "\n",
    "print(model)\n",
    "print(f\"Number of params: {model.num_parameters()//1000}k\")\n",
    "\n",
    "def train(epoch, loss_func):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # randomly flip sign of eigenvectors\n",
    "        batch_pe = data.pe * ( 2 * torch.randint(low=0, high=2, size=(1, PE_DIM)).float() - 1.0) \n",
    "\n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch)\n",
    "        loss = loss_func(out.squeeze(), data.y)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, loss_func):\n",
    "    model.eval()\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        # randomly flip sign of eigenvectors\n",
    "        batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM)).float() - 1.0) \n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch)\n",
    "        total_error += loss_func(out.squeeze(), data.y).item()\n",
    "    return total_error / len(loader.dataset)\n",
    "\n",
    "train_loss = nn.L1Loss(reduction='mean')\n",
    "test_loss = nn.L1Loss(reduction='sum')\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    tr_loss = train(epoch, loss_func=train_loss)\n",
    "    va_loss = test(val_loader, loss_func=test_loss)\n",
    "    te_loss = test(test_loader, loss_func=test_loss)\n",
    "    scheduler.step(va_loss)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {tr_loss:.4f}, Val: {va_loss:.4f}, '\n",
    "          f'Test: {te_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
