{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. https://github.com/xbresson/CS6208_2023/blob/main/codes/labs_lecture07/01_vanilla_graph_transformers.ipynb\n",
    "2. https://github.com/pgniewko/pytorch_geometric/blob/master/torch_geometric/nn/conv/transformer_conv.py\n",
    "3. https://arxiv.org/abs/2012.09699\n",
    "4. https://arxiv.org/abs/1703.04977\n",
    "\n",
    "TDC:\n",
    "1. https://tdcommons.ai/benchmark/overview/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawel/anaconda3/envs/gt/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter.composite import scatter_softmax\n",
    "from torch_scatter.scatter import scatter_add\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn.aggr import MultiAggregation\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Dataset\n",
    "import os.path as osp\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "torch.manual_seed(192837465)\n",
    "\n",
    "# Generate example data\n",
    "x = torch.randn(6, 3)  # Node features (6 nodes, 3-dimensional features)\n",
    "edge_index = torch.tensor([[0, 1, 1, 2, 2, 3, 4, 5], [1, 0, 2, 1, 3, 2, 5, 4]], dtype=torch.long)  # Edge indices\n",
    "edge_attr = torch.randn(8, 2)  # Edge attributes (8 edges, 2-dimensional attributes)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims, output_dim, num_layers=2, dropout=0.0, act='relu', act_kwargs=None):\n",
    "        super(MLP, self).__init__()\n",
    "    \n",
    "        if isinstance(dims, int):\n",
    "            dims = [dims] * num_layers\n",
    "            \n",
    "        assert len(dims) == num_layers\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        for (i_dim, o_dim) in zip(dims[:-1], dims[1:]):\n",
    "            layers.append(nn.Linear(i_dim, o_dim, bias=True))\n",
    "            layers.append(activation_resolver(act, **(act_kwargs or {})))\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(p=dropout))\n",
    "                \n",
    "        layers.append(nn.Linear(dims[-1], output_dim, bias=True))\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "    \n",
    "class GTConv(MessagePassing):\n",
    "    def __init__(self, node_in_dim, hidden_dim, edge_in_dim=None, num_heads=1, dropout=0.0, norm='bn', act='relu'):\n",
    "        super(GTConv, self).__init__(node_dim=0, aggr='add')\n",
    "        \n",
    "        assert hidden_dim % num_heads == 0\n",
    "        assert (edge_in_dim is None) or (edge_in_dim > 0)\n",
    "        \n",
    "        self.WQ = nn.Linear(node_in_dim, hidden_dim, bias=True)\n",
    "        self.WK = nn.Linear(node_in_dim, hidden_dim, bias=True)\n",
    "        self.WV = nn.Linear(node_in_dim, hidden_dim, bias=True)\n",
    "        self.WO = nn.Linear(hidden_dim, node_in_dim, bias=True)\n",
    "        \n",
    "        if edge_in_dim is not None:\n",
    "            assert node_in_dim == edge_in_dim\n",
    "            self.WE = nn.Linear(edge_in_dim, hidden_dim, bias=True)\n",
    "            self.WOe = nn.Linear(hidden_dim, edge_in_dim, bias=True)\n",
    "            self.ffn_e = MLP(dims=edge_in_dim, output_dim=edge_in_dim,\n",
    "                             num_layers=2, dropout=dropout, act=act)\n",
    "            if norm.lower() in ['bn', 'batchnorm', 'batch_norm']:\n",
    "                self.norm1e = nn.BatchNorm1d(edge_in_dim)\n",
    "                self.norm2e = nn.BatchNorm1d(edge_in_dim)\n",
    "            elif norm.lower() in ['ln', 'layernorm', 'layer_norm']:\n",
    "                self.norm1e = nn.LayerNorm(edge_in_dim)\n",
    "                self.norm2e = nn.LayerNorm(edge_in_dim)\n",
    "        else:\n",
    "            self.WE = self.register_parameter('WE', None)\n",
    "            self.WOe = self.register_parameter('WOe', None)\n",
    "            self.ffn_e = self.register_parameter('ffn_e', None)\n",
    "            self.norm1e = self.register_parameter('norm1e', None)\n",
    "            self.norm2e = self.register_parameter('norm2e', None)\n",
    "        \n",
    "        if norm.lower() in ['bn', 'batchnorm', 'batch_norm']:\n",
    "            self.norm1 = nn.BatchNorm1d(node_in_dim)\n",
    "            self.norm2 = nn.BatchNorm1d(node_in_dim)\n",
    "        elif norm.lower() in ['ln', 'layernorm', 'layer_norm']:\n",
    "            self.norm1 = nn.LayerNorm(node_in_dim)\n",
    "            self.norm2 = nn.LayerNorm(node_in_dim)\n",
    "            \n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "            \n",
    "        self.ffn = MLP(dims=node_in_dim, output_dim=node_in_dim,\n",
    "                       num_layers=2, dropout=dropout, act=act)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.node_in_dim = node_in_dim\n",
    "        self.edge_in_dim = edge_in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.norm = norm.lower()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # TODO: init linear layers with xavier weights    \n",
    "        super().reset_parameters()\n",
    "        \n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        #print(edge_index)\n",
    "        x_ = x\n",
    "        Q = self.WQ(x).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        K = self.WK(x).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        V = self.WV(x).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        \n",
    "        out = self.propagate(edge_index, Q=Q, K=K, V=V,\n",
    "                             edge_attr=edge_attr, size=None)\n",
    "        out = out.view(-1, self.hidden_dim)\n",
    "        \n",
    "        ## NODES\n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.WO(out) + x_ # Residual connection\n",
    "        out = self.norm1(out)\n",
    "        # FFN-NODES\n",
    "        ffn_in = out\n",
    "        out = self.ffn(out)\n",
    "        out = self.norm2(ffn_in + out)\n",
    "        \n",
    "        if self.edge_in_dim is None:\n",
    "            out_eij = None\n",
    "        else:\n",
    "            out_eij = self._eij\n",
    "            self._eij = None\n",
    "            out_eij = out_eij.view(-1, self.hidden_dim)\n",
    "\n",
    "            ## EDGES\n",
    "            out_eij_ = out_eij\n",
    "            out_eij = self.dropout_layer(out_eij)\n",
    "            out_eij = self.WOe(out_eij) + out_eij_ # Residual connection\n",
    "            out_eij = self.norm1e(out_eij)\n",
    "            # FFN-EDGES\n",
    "            ffn_eij_in = out_eij\n",
    "            out_eij = self.ffn_e(out_eij)\n",
    "            out_eij = self.norm2e(ffn_eij_in + out_eij)\n",
    "        \n",
    "        \n",
    "        return (out, out_eij)\n",
    "        \n",
    "        \n",
    "    def message(self, Q_i, K_j, V_j, index, edge_attr=None):\n",
    "        d_k = Q_i.size(-1)\n",
    "        \n",
    "        if self.WE is not None:\n",
    "            assert edge_attr is not None\n",
    "            E = self.WE(edge_attr).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "            K_j = E * K_j\n",
    "        \n",
    "        qijk = (Q_i * K_j).sum(dim=-1) / math.sqrt(d_k)\n",
    "        self._eij = (Q_i * K_j) / math.sqrt(d_k)\n",
    "        alpha = softmax(qijk, index) # Log-Sum-Exp trick used. No need for clipping (-5,5)\n",
    "        \n",
    "        return alpha.view(-1, self.num_heads, 1) * V_j\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.node_in_dim}, '\n",
    "                f'{self.hidden_dim}, heads={self.num_heads})')\n",
    "    \n",
    "    \n",
    "class GraphTransformerNet(nn.Module):\n",
    "    def __init__(self, node_dim_in, edge_dim_in,\n",
    "                 hidden_dim=128, norm='bn',\n",
    "                 num_gt_layers=4, num_heads=8,\n",
    "                 aggregators=['sum'],\n",
    "                 act='gelu', dropout=0.0):\n",
    "        super(GraphTransformerNet, self).__init__()\n",
    "        \n",
    "        self.node_emb = nn.Embedding(node_dim_in, hidden_dim) #nn.Linear(node_dim_in, hidden_dim)\n",
    "        self.edge_emb = nn.Embedding(edge_dim_in, hidden_dim) #nn.Linear(edge_dim_in, hidden_dim)\n",
    "        \n",
    "        self.gt_layers = nn.ModuleList()\n",
    "        for _ in range(num_gt_layers): \n",
    "            self.gt_layers.append(GTConv(node_in_dim=hidden_dim,\n",
    "                                         hidden_dim=hidden_dim, edge_in_dim=hidden_dim,\n",
    "                                         num_heads=num_heads, act=act,\n",
    "                                         dropout=dropout))\n",
    "        \n",
    "        self.global_pool = MultiAggregation(aggregators, mode='cat')\n",
    "        \n",
    "        num_aggrs = len(aggregators)\n",
    "        self.mu_mlp = MLP(dims=[num_aggrs * hidden_dim, hidden_dim, hidden_dim], \n",
    "                          output_dim=1, num_layers=3, dropout=0.0, act=act)\n",
    "        self.std_mlp = MLP(dims=[num_aggrs * hidden_dim, hidden_dim, hidden_dim], \n",
    "                           output_dim=1, num_layers=3, dropout=0.0, act=act)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        #print(\"pre\",x.size())\n",
    "        x = self.node_emb(x.squeeze())\n",
    "        #print(\"post\",x.size())\n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "\n",
    "        for gt_layer in self.gt_layers:\n",
    "            (x, edge_attr) = gt_layer(x, edge_index, edge_attr=edge_attr)\n",
    "\n",
    "        x = self.global_pool(x, batch)\n",
    "        mu = self.mu_mlp(x)\n",
    "        log_var = self.std_mlp(x)\n",
    "        \n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * log_var)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps * std + mu\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def num_parameters(self):   \n",
    "        trainable_params = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        count = sum([p.numel() for p in trainable_params])\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4b/hq3_lmzj215_hw29qxb407dc0000gn/T/ipykernel_94141/3710679617.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGTConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_in_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_in_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4b/hq3_lmzj215_hw29qxb407dc0000gn/T/ipykernel_94141/3655685967.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_in_dim, hidden_dim, edge_in_dim, num_heads, dropout, norm, act)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0medge_in_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mnode_in_dim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0medge_in_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_in_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWOe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_in_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gt = GTConv(node_in_dim=3, hidden_dim=6, num_heads=2, edge_in_dim=2)\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 6])\n",
      "torch.Size([8, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4b/hq3_lmzj215_hw29qxb407dc0000gn/T/ipykernel_94141/2240589934.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/gt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4b/hq3_lmzj215_hw29qxb407dc0000gn/T/ipykernel_94141/2571153404.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mout_eij_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_eij\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mout_eij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_eij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mout_eij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWOe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_eij\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout_eij_\u001b[0m \u001b[0;31m# Residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mout_eij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_eij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;31m# FFN-EDGES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "gt(x, edge_index, edge_attr=edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_net = GraphTransformerNet(node_dim_in=3, edge_dim_in=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loader = DataLoader([data], batch_size=128, shuffle=False)\n",
    "for data in loader:\n",
    "    x = data.x\n",
    "    edge_index = data.edge_index\n",
    "    edge_attr = data.edge_attr\n",
    "    pred= gt_net(x, edge_index, edge_attr, data.batch)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET ZINC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = osp.join('.', 'data', 'ZINC')\n",
    "train_dataset = ZINC(path, subset=True, split='train')\n",
    "val_dataset = ZINC(path, subset=True, split='val')\n",
    "test_dataset = ZINC(path, subset=True, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "# Compute the maximum in-degree in the training data.\n",
    "max_degree = -1\n",
    "for data in train_dataset:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    max_degree = max(max_degree, int(d.max()))\n",
    "\n",
    "# Compute the in-degree histogram tensor\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "for data in train_dataset:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN AND EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformerNet(\n",
      "  (node_emb): Embedding(21, 128)\n",
      "  (edge_emb): Embedding(4, 128)\n",
      "  (gt_layers): ModuleList(\n",
      "    (0): GTConv(128, 128, heads=8)\n",
      "    (1): GTConv(128, 128, heads=8)\n",
      "    (2): GTConv(128, 128, heads=8)\n",
      "    (3): GTConv(128, 128, heads=8)\n",
      "  )\n",
      "  (global_pool): MultiAggregation([\n",
      "    SumAggregation(),\n",
      "  ], mode=cat)\n",
      "  (mu_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): GELU(approximate='none')\n",
      "      (4): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (std_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): GELU(approximate='none')\n",
      "      (4): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of params: 734k\n",
      "torch.Size([6448, 128])\n",
      "torch.Size([6448, 128])\n",
      "torch.Size([6448, 128])\n",
      "torch.Size([6448, 128])\n",
      "torch.Size([6448, 128])\n",
      "torch.Size([6448, 128])\n",
      "torch.Size([6448, 128])\n",
      "torch.Size([6448, 128])\n",
      "torch.Size([6088, 128])\n",
      "torch.Size([6088, 128])\n",
      "torch.Size([6088, 128])\n",
      "torch.Size([6088, 128])\n",
      "torch.Size([6088, 128])\n",
      "torch.Size([6088, 128])\n",
      "torch.Size([6088, 128])\n",
      "torch.Size([6088, 128])\n",
      "torch.Size([6386, 128])\n",
      "torch.Size([6386, 128])\n",
      "torch.Size([6386, 128])\n",
      "torch.Size([6386, 128])\n",
      "torch.Size([6386, 128])\n",
      "torch.Size([6386, 128])\n",
      "torch.Size([6386, 128])\n",
      "torch.Size([6386, 128])\n",
      "torch.Size([6438, 128])\n",
      "torch.Size([6438, 128])\n",
      "torch.Size([6438, 128])\n",
      "torch.Size([6438, 128])\n",
      "torch.Size([6438, 128])\n",
      "torch.Size([6438, 128])\n",
      "torch.Size([6438, 128])\n",
      "torch.Size([6438, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6256, 128])\n",
      "torch.Size([6256, 128])\n",
      "torch.Size([6256, 128])\n",
      "torch.Size([6256, 128])\n",
      "torch.Size([6256, 128])\n",
      "torch.Size([6256, 128])\n",
      "torch.Size([6256, 128])\n",
      "torch.Size([6256, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6348, 128])\n",
      "torch.Size([6348, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4b/hq3_lmzj215_hw29qxb407dc0000gn/T/ipykernel_94141/3246852079.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m301\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mval_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtest_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4b/hq3_lmzj215_hw29qxb407dc0000gn/T/ipykernel_94141/3246852079.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gt/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gt/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphTransformerNet(node_dim_in=21, edge_dim_in=4, num_gt_layers=4, hidden_dim=128, \n",
    "                           dropout=0.1).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "\n",
    "print(model)\n",
    "print(f\"Number of params: {model.num_parameters()//1000}k\")\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #print(data.x.size())\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        loss = (out.squeeze() - data.y).abs().mean()\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        total_error += (out.squeeze() - data.y).abs().sum().item()\n",
    "    return total_error / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 301):\n",
    "    loss = train(epoch)\n",
    "    val_mae = test(val_loader)\n",
    "    test_mae = test(test_loader)\n",
    "    scheduler.step(val_mae)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
    "          f'Test: {test_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
