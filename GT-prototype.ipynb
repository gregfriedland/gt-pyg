{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. https://github.com/xbresson/CS6208_2023/blob/main/codes/labs_lecture07/01_vanilla_graph_transformers.ipynb\n",
    "2. https://github.com/pgniewko/pytorch_geometric/blob/master/torch_geometric/nn/conv/transformer_conv.py\n",
    "3. https://arxiv.org/abs/2012.09699\n",
    "4. https://arxiv.org/abs/1703.04977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawel/anaconda3/envs/gt/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter.composite import scatter_softmax\n",
    "from torch_scatter.scatter import scatter_add\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "\n",
    "\n",
    "torch.manual_seed(192837465)\n",
    "\n",
    "# Generate example data\n",
    "x = torch.randn(6, 3)  # Node features (6 nodes, 3-dimensional features)\n",
    "edge_index = torch.tensor([[0, 1, 1, 2, 2, 3, 4, 5], [1, 0, 2, 1, 3, 2, 5, 4]], dtype=torch.long)  # Edge indices\n",
    "edge_attr = torch.randn(8, 2)  # Edge attributes (8 edges, 2-dimensional attributes)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers=2, dropout_p=0.0, \n",
    "                 act='relu', act_kwargs=None, final_act=True):\n",
    "        super(MLP, self).__init__()\n",
    "    \n",
    "        if isinstance(hidden_dim, int):\n",
    "            hidden_dim = [hidden_dim] * num_layers\n",
    "            \n",
    "        assert len(hidden_dim) == num_layers\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        for (in_dim, out_dim) in zip(hidden_dim[:-2], hidden_dim[1:]):\n",
    "            layers.append(nn.Linear(in_dim, out_dim, bias=True))\n",
    "            layers.append(activation_resolver(act, **(act_kwargs or {})))\n",
    "            if dropout_p > 0:\n",
    "                layers.append(nn.Dropout(p=dropout_p))\n",
    "        \n",
    "        if dropout_p > 0.0:\n",
    "            layers = layers[:-1]\n",
    "        if not final_act:\n",
    "            layers = layers[:-1]\n",
    "            \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.act = act\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "    \n",
    "class GTConv(MessagePassing):\n",
    "    def __init__(self, in_dim, out_dim, num_heads=1, dropout_p=0.0, norm='bn'):\n",
    "        super(GTConv, self).__init__(node_dim=0, aggr='add')\n",
    "        \n",
    "        assert out_dim % num_heads == 0\n",
    "        \n",
    "        self.WQ = nn.Linear(in_dim, out_dim, bias=True)\n",
    "        self.WK = nn.Linear(in_dim, out_dim, bias=True)\n",
    "        self.WV = nn.Linear(in_dim, out_dim, bias=True)\n",
    "        \n",
    "        self.WO = nn.Linear(out_dim, in_dim, bias=True)\n",
    "        \n",
    "        \n",
    "        if norm.lower() in ['bn', 'batchnorm', 'batch_norm']:\n",
    "            self.norm1 = nn.BatchNorm1d(in_dim)\n",
    "            self.norm2 = nn.BatchNorm1d(in_dim)\n",
    "        elif norm.lower() in ['ln', 'layernorm', 'layer_norm']:\n",
    "            self.norm1 = nn.LayerNorm(in_dim)\n",
    "            self.norm2 = nn.LayerNorm(in_dim)\n",
    "            \n",
    "        self.ffn = MLP(hidden_dim=in_dim, num_layers=2, \n",
    "                       dropout_p=dropout_p, final_act=False)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # TODO: init linear layers with xavier weights    \n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        #print(edge_index)\n",
    "        Q = self.WQ(x).view(-1, self.num_heads, self.out_dim // self.num_heads)\n",
    "        K = self.WK(x).view(-1, self.num_heads, self.out_dim // self.num_heads)\n",
    "        V = self.WV(x).view(-1, self.num_heads, self.out_dim // self.num_heads)\n",
    "        \n",
    "        out = self.propagate(edge_index, Q=Q, K=K, V=V, edge_attr=edge_attr, size=None)\n",
    "        \n",
    "        out = out.view(-1, self.out_dim)\n",
    "        \n",
    "        out = F.dropout(out, self.dropout_p)\n",
    "        out = self.WO(out) + x\n",
    "        out = self.norm1(out)\n",
    "    \n",
    "        # FFN\n",
    "        mlp_in = out\n",
    "        out = self.ffn(out)\n",
    "        out = self.norm2(mlp_in + out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def message(self, Q_i, K_j, V_j, index, edge_attr=None):\n",
    "        d_k = Q_i.size(-1)\n",
    "        qijk = (Q_i * K_j).sum(dim=-1) / math.sqrt(d_k)\n",
    "        \n",
    "        alpha = softmax(qijk, index) # Log-Sum-Exp trick used. No need for clipping.\n",
    "        \n",
    "        return alpha.view(-1, self.num_heads, 1) * V_j\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_dim}, '\n",
    "                f'{self.out_dim}, heads={self.num_heads})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GTConv(3, 6, heads=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = GTConv(in_dim=3, out_dim=6, num_heads=2)\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7639, -1.3601,  1.5337],\n",
       "        [ 1.7370,  0.9856,  0.9378],\n",
       "        [-0.4407,  0.6072,  0.3323],\n",
       "        [-1.1320, -0.8076, -0.7924],\n",
       "        [-0.9604, -0.7107, -0.9446],\n",
       "        [ 0.0323,  1.2855, -1.0667]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt(x, edge_index, edge_attr=edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
