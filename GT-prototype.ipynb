{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. https://github.com/xbresson/CS6208_2023/blob/main/codes/labs_lecture07/01_vanilla_graph_transformers.ipynb\n",
    "2. https://github.com/pgniewko/pytorch_geometric/blob/master/torch_geometric/nn/conv/transformer_conv.py\n",
    "3. https://arxiv.org/abs/2012.09699\n",
    "4. https://arxiv.org/abs/1703.04977\n",
    "\n",
    "TDC:\n",
    "1. https://tdcommons.ai/benchmark/overview/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter.composite import scatter_softmax\n",
    "from torch_scatter.scatter import scatter_add\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn.aggr import MultiAggregation\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Dataset\n",
    "\n",
    "torch.manual_seed(192837465)\n",
    "\n",
    "# Generate example data\n",
    "x = torch.randn(6, 3)  # Node features (6 nodes, 3-dimensional features)\n",
    "edge_index = torch.tensor([[0, 1, 1, 2, 2, 3, 4, 5], [1, 0, 2, 1, 3, 2, 5, 4]], dtype=torch.long)  # Edge indices\n",
    "edge_attr = torch.randn(8, 2)  # Edge attributes (8 edges, 2-dimensional attributes)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims, output_dim, num_layers=2, dropout=0.0, act='relu', act_kwargs=None):\n",
    "        super(MLP, self).__init__()\n",
    "    \n",
    "        if isinstance(dims, int):\n",
    "            dims = [dims] * num_layers\n",
    "            \n",
    "        assert len(dims) == num_layers\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        for (i_dim, o_dim) in zip(dims[:-1], dims[1:]):\n",
    "            layers.append(nn.Linear(i_dim, o_dim, bias=True))\n",
    "            layers.append(activation_resolver(act, **(act_kwargs or {})))\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(p=dropout))\n",
    "                \n",
    "        layers.append(nn.Linear(dims[-1], output_dim, bias=True))\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "    \n",
    "class GTConv(MessagePassing):\n",
    "    def __init__(self, in_dim, hidden_dim, num_heads=1, dropout=0.0, norm='bn'):\n",
    "        super(GTConv, self).__init__(node_dim=0, aggr='add')\n",
    "        \n",
    "        assert hidden_dim % num_heads == 0\n",
    "        \n",
    "        self.WQ = nn.Linear(in_dim, hidden_dim, bias=True)\n",
    "        self.WK = nn.Linear(in_dim, hidden_dim, bias=True)\n",
    "        self.WV = nn.Linear(in_dim, hidden_dim, bias=True)\n",
    "        self.WO = nn.Linear(hidden_dim, in_dim, bias=True)\n",
    "        \n",
    "        if norm.lower() in ['bn', 'batchnorm', 'batch_norm']:\n",
    "            self.norm1 = nn.BatchNorm1d(in_dim)\n",
    "            self.norm2 = nn.BatchNorm1d(in_dim)\n",
    "        elif norm.lower() in ['ln', 'layernorm', 'layer_norm']:\n",
    "            self.norm1 = nn.LayerNorm(in_dim)\n",
    "            self.norm2 = nn.LayerNorm(in_dim)\n",
    "            \n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "            \n",
    "        self.ffn = MLP(dims=in_dim, output_dim=in_dim, num_layers=2, dropout=dropout)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.norm = norm.lower()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # TODO: init linear layers with xavier weights    \n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        #print(edge_index)\n",
    "        x_ = x\n",
    "        Q = self.WQ(x).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        K = self.WK(x).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        V = self.WV(x).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        \n",
    "        out = self.propagate(edge_index, Q=Q, K=K, V=V, edge_attr=edge_attr, size=None)\n",
    "        \n",
    "        out = out.view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.WO(out) + x_ # Residual connection\n",
    "        out = self.norm1(out)\n",
    "    \n",
    "        # FFN\n",
    "        mlp_in = out\n",
    "        out = self.ffn(out)\n",
    "        out = self.norm2(mlp_in + out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def message(self, Q_i, K_j, V_j, index, edge_attr=None):\n",
    "        d_k = Q_i.size(-1)\n",
    "        qijk = (Q_i * K_j).sum(dim=-1) / math.sqrt(d_k)\n",
    "        \n",
    "        alpha = softmax(qijk, index) # Log-Sum-Exp trick used. No need for clipping.\n",
    "        \n",
    "        return alpha.view(-1, self.num_heads, 1) * V_j\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_dim}, '\n",
    "                f'{self.hidden_dim}, heads={self.num_heads})')\n",
    "    \n",
    "    \n",
    "class GraphTransformerNet(nn.Module):\n",
    "    def __init__(self, node_dim_in, edge_dim_in,\n",
    "                 hidden_dim=128, norm='bn',\n",
    "                 num_gt_layers=4, num_heads=8,\n",
    "                 aggregators=['sum']):\n",
    "        super(GraphTransformerNet, self).__init__()\n",
    "        \n",
    "        self.node_emb = nn.Linear(node_dim_in, hidden_dim)\n",
    "        self.edge_emb = nn.Linear(edge_dim_in, hidden_dim)\n",
    "        \n",
    "        self.gt_layers = [GTConv(in_dim=hidden_dim, hidden_dim=hidden_dim, \n",
    "                                 num_heads=num_heads) for _ in range(num_gt_layers)]\n",
    "        \n",
    "        self.global_pool = MultiAggregation(aggregators, mode='cat')\n",
    "        \n",
    "        num_aggrs = len(aggregators)\n",
    "        self.mu_mlp = MLP(dims=[num_aggrs * hidden_dim, hidden_dim, hidden_dim], \n",
    "                          output_dim=1, num_layers=3, dropout=0.0)\n",
    "        self.std_mlp = MLP(dims=[num_aggrs * hidden_dim, hidden_dim, hidden_dim], \n",
    "                           output_dim=1, num_layers=3, dropout=0.0)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = self.node_emb(x.squeeze())\n",
    "        \n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "\n",
    "        for gt_layer in self.gt_layers:\n",
    "            x = gt_layer(x, edge_index, edge_attr=edge_attr)\n",
    "\n",
    "        x = self.global_pool(x, batch)\n",
    "        mu = self.mu_mlp(x)\n",
    "        log_var = self.std_mlp(x)\n",
    "        \n",
    "        \n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu, mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GTConv(3, 6, heads=2)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = GTConv(in_dim=3, hidden_dim=6, num_heads=2)\n",
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8714, -1.5164,  1.4111],\n",
       "        [ 1.7645,  1.4195,  0.9956],\n",
       "        [-0.5366,  0.4443,  0.4392],\n",
       "        [-0.4399, -0.3694, -0.8469],\n",
       "        [-1.1963, -0.8110, -1.2334],\n",
       "        [-0.4631,  0.8331, -0.7656]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt(x, edge_index, edge_attr=edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_net = GraphTransformerNet(node_dim_in=3, edge_dim_in=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4718]], grad_fn=<AddBackward0>) tensor([[0.0691]], grad_fn=<AddmmBackward0>) tensor([[0.9552]], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader([data], batch_size=128, shuffle=False)\n",
    "for data in loader:\n",
    "    x = data.x\n",
    "    edge_index = data.edge_index\n",
    "    edge_attr = data.edge_attr\n",
    "    pred, mu, std = gt_net(x, edge_index, edge_attr, data.batch)\n",
    "    print(pred, mu, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
