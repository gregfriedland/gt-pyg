{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. https://github.com/xbresson/CS6208_2023/blob/main/codes/labs_lecture07/01_vanilla_graph_transformers.ipynb\n",
    "2. https://github.com/xbresson/CS6208_2023/blob/main/codes/labs_lecture07/03_graph_transformers_regression_exercise.ipynb\n",
    "3. https://github.com/pgniewko/pytorch_geometric/blob/master/torch_geometric/nn/conv/transformer_conv.py\n",
    "4. https://arxiv.org/abs/2012.09699\n",
    "5. https://arxiv.org/abs/1703.04977\n",
    "\n",
    "TDC:\n",
    "1. Test the model on this new set: https://practicalcheminformatics.blogspot.com/2023/06/getting-real-with-molecular-property.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawel/anaconda3/envs/gt/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rdkit version: 2023.03.1\n",
      "Torch version: 1.13.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb231a374f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import rdkit\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter.composite import scatter_softmax\n",
    "from torch_scatter.scatter import scatter_add\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn.aggr import MultiAggregation\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Dataset\n",
    "import os.path as osp\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Turn off majority of RDKit warnings\n",
    "from rdkit import RDLogger\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "print(f'Rdkit version: {rdkit.__version__}')\n",
    "print(f'Torch version: {torch.__version__}')\n",
    "\n",
    "torch.manual_seed(192837465)\n",
    "\n",
    "#\n",
    "# Generate example data\n",
    "#x = torch.randn(6, 3)  # Node features (6 nodes, 3-dimensional features)\n",
    "#edge_index = torch.tensor([[0, 1, 1, 2, 2, 3, 4, 5], [1, 0, 2, 1, 3, 2, 5, 4]], dtype=torch.long)  # Edge indices\n",
    "#edge_attr = torch.randn(8, 2)  # Edge attributes (8 edges, 2-dimensional attributes)\n",
    "#\n",
    "#data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims, \n",
    "                 num_hidden_layers=1,\n",
    "                 dropout=0.0, act='relu', \n",
    "                 act_kwargs=None):\n",
    "        super(MLP, self).__init__()\n",
    "    \n",
    "        if isinstance(hidden_dims, int):\n",
    "            hidden_dims = [hidden_dims] * num_hidden_layers\n",
    "        \n",
    "        hidden_dims = [input_dim] + hidden_dims\n",
    "        assert len(hidden_dims) - 1 == num_hidden_layers\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        for (i_dim, o_dim) in zip(hidden_dims[:-1], hidden_dims[1:]):\n",
    "            layers.append(nn.Linear(i_dim, o_dim, bias=True))\n",
    "            layers.append(activation_resolver(act, **(act_kwargs or {})))\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(p=dropout))\n",
    "                \n",
    "        layers.append(nn.Linear(hidden_dims[-1], output_dim, bias=True))\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "    \n",
    "class GTConv(MessagePassing):\n",
    "    def __init__(self, node_in_dim, hidden_dim, edge_in_dim=None, num_heads=1, dropout=0.0, norm='bn', act='relu'):\n",
    "        super(GTConv, self).__init__(node_dim=0, aggr='add')\n",
    "        \n",
    "        assert hidden_dim % num_heads == 0\n",
    "        assert (edge_in_dim is None) or (edge_in_dim > 0)\n",
    "        \n",
    "        self.WQ = nn.Linear(node_in_dim, hidden_dim, bias=True)\n",
    "        self.WK = nn.Linear(node_in_dim, hidden_dim, bias=True)\n",
    "        self.WV = nn.Linear(node_in_dim, hidden_dim, bias=True)\n",
    "        self.WO = nn.Linear(hidden_dim, node_in_dim, bias=True)\n",
    "        \n",
    "        if edge_in_dim is not None:\n",
    "            assert node_in_dim == edge_in_dim\n",
    "            self.WE = nn.Linear(edge_in_dim, hidden_dim, bias=True)\n",
    "            self.WOe = nn.Linear(hidden_dim, edge_in_dim, bias=True)\n",
    "            self.ffn_e = MLP(input_dim=edge_in_dim,\n",
    "                             output_dim=edge_in_dim,\n",
    "                             hidden_dims=hidden_dim,\n",
    "                             num_hidden_layers=1,\n",
    "                             dropout=dropout, act=act)\n",
    "            if norm.lower() in ['bn', 'batchnorm', 'batch_norm']:\n",
    "                self.norm1e = nn.BatchNorm1d(edge_in_dim)\n",
    "                self.norm2e = nn.BatchNorm1d(edge_in_dim)\n",
    "            elif norm.lower() in ['ln', 'layernorm', 'layer_norm']:\n",
    "                self.norm1e = nn.LayerNorm(edge_in_dim)\n",
    "                self.norm2e = nn.LayerNorm(edge_in_dim)\n",
    "        else:\n",
    "            self.WE = self.register_parameter('WE', None)\n",
    "            self.WOe = self.register_parameter('WOe', None)\n",
    "            self.ffn_e = self.register_parameter('ffn_e', None)\n",
    "            self.norm1e = self.register_parameter('norm1e', None)\n",
    "            self.norm2e = self.register_parameter('norm2e', None)\n",
    "        \n",
    "        if norm.lower() in ['bn', 'batchnorm', 'batch_norm']:\n",
    "            self.norm1 = nn.BatchNorm1d(node_in_dim)\n",
    "            self.norm2 = nn.BatchNorm1d(node_in_dim)\n",
    "        elif norm.lower() in ['ln', 'layernorm', 'layer_norm']:\n",
    "            self.norm1 = nn.LayerNorm(node_in_dim)\n",
    "            self.norm2 = nn.LayerNorm(node_in_dim)\n",
    "            \n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "            \n",
    "        self.ffn = MLP(input_dim=node_in_dim,\n",
    "                       output_dim=node_in_dim,\n",
    "                       hidden_dims=hidden_dim,\n",
    "                       num_hidden_layers=1,\n",
    "                       dropout=dropout, act=act)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.node_in_dim = node_in_dim\n",
    "        self.edge_in_dim = edge_in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.norm = norm.lower()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "           \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.WQ.weight)\n",
    "        nn.init.xavier_uniform_(self.WK.weight)\n",
    "        nn.init.xavier_uniform_(self.WV.weight)\n",
    "        nn.init.xavier_uniform_(self.WO.weight)\n",
    "        if self.edge_in_dim is not None:\n",
    "            nn.init.xavier_uniform_(self.WE.weight)\n",
    "            nn.init.xavier_uniform_(self.WOe.weight)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        x_ = x\n",
    "        Q = self.WQ(x).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        K = self.WK(x).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        V = self.WV(x).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        \n",
    "        out = self.propagate(edge_index, Q=Q, K=K, V=V,\n",
    "                             edge_attr=edge_attr, size=None)\n",
    "        out = out.view(-1, self.hidden_dim)\n",
    "        \n",
    "        ## NODES\n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.WO(out) + x_ # Residual connection\n",
    "        out = self.norm1(out)\n",
    "        # FFN-NODES\n",
    "        ffn_in = out\n",
    "        out = self.ffn(out)\n",
    "        out = self.norm2(ffn_in + out)\n",
    "        \n",
    "        if self.edge_in_dim is None:\n",
    "            out_eij = None\n",
    "        else:\n",
    "            out_eij = self._eij\n",
    "            self._eij = None\n",
    "            out_eij = out_eij.view(-1, self.hidden_dim)\n",
    "\n",
    "            ## EDGES\n",
    "            out_eij_ = out_eij\n",
    "            out_eij = self.dropout_layer(out_eij)\n",
    "            out_eij = self.WOe(out_eij) + out_eij_ # Residual connection\n",
    "            out_eij = self.norm1e(out_eij)\n",
    "            # FFN-EDGES\n",
    "            ffn_eij_in = out_eij\n",
    "            out_eij = self.ffn_e(out_eij)\n",
    "            out_eij = self.norm2e(ffn_eij_in + out_eij)\n",
    "\n",
    "        return (out, out_eij)\n",
    "        \n",
    "        \n",
    "    def message(self, Q_i, K_j, V_j, index, edge_attr=None):\n",
    "        if self.WE is not None:\n",
    "            assert edge_attr is not None\n",
    "            E = self.WE(edge_attr).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "            K_j = E * K_j\n",
    "        \n",
    "        d_k = Q_i.size(-1)\n",
    "        qijk = (Q_i * K_j).sum(dim=-1) / math.sqrt(d_k)\n",
    "        self._eij = (Q_i * K_j) / math.sqrt(d_k)\n",
    "        alpha = softmax(qijk, index) # Log-Sum-Exp trick used. No need for clipping (-5,5)\n",
    "        \n",
    "        return alpha.view(-1, self.num_heads, 1) * V_j\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.node_in_dim}, '\n",
    "                f'{self.hidden_dim}, heads={self.num_heads})')\n",
    "    \n",
    "    \n",
    "class GraphTransformerNet(nn.Module):\n",
    "    def __init__(self, node_dim_in, edge_dim_in=None,\n",
    "                 hidden_dim=128, norm='bn',\n",
    "                 num_gt_layers=4, num_heads=8,\n",
    "                 aggregators=['sum'],\n",
    "                 act='relu', dropout=0.0):\n",
    "        super(GraphTransformerNet, self).__init__()\n",
    "        \n",
    "        self.node_emb = nn.Linear(node_dim_in, hidden_dim)\n",
    "        if edge_dim_in:\n",
    "            self.edge_emb = nn.Linear(edge_dim_in, hidden_dim)\n",
    "        else:\n",
    "            self.edge_emb = self.register_parameter('edge_emb', None)\n",
    "        \n",
    "        self.gt_layers = nn.ModuleList()\n",
    "        for _ in range(num_gt_layers):\n",
    "            self.gt_layers.append(GTConv(node_in_dim=hidden_dim,\n",
    "                                         hidden_dim=hidden_dim, \n",
    "                                         edge_in_dim=hidden_dim,\n",
    "                                         num_heads=num_heads,\n",
    "                                         act=act,\n",
    "                                         dropout=dropout,\n",
    "                                         norm='bn'))\n",
    "        \n",
    "        self.global_pool = MultiAggregation(aggregators, mode='cat')\n",
    "        \n",
    "        num_aggrs = len(aggregators)\n",
    "        self.mu_mlp = MLP(input_dim=num_aggrs * hidden_dim, output_dim=1,\n",
    "                          hidden_dims=hidden_dim,\n",
    "                          num_hidden_layers=1, dropout=0.0, act=act)\n",
    "        self.std_mlp = MLP(input_dim=num_aggrs * hidden_dim, output_dim=1,\n",
    "                           hidden_dims=hidden_dim,\n",
    "                           num_hidden_layers=1, dropout=0.0, act=act)\n",
    "        \n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.node_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.edge_emb.weight)\n",
    "            \n",
    "            \n",
    "    def forward(self, x, edge_index, edge_attr, batch, return_std=False):\n",
    "        x = self.node_emb(x.squeeze())\n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "\n",
    "        for gt_layer in self.gt_layers:\n",
    "            (x, edge_attr) = gt_layer(x, edge_index, edge_attr=edge_attr)\n",
    "\n",
    "        x = self.global_pool(x, batch)\n",
    "        mu = self.mu_mlp(x)\n",
    "        log_var = self.std_mlp(x)\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        \n",
    "        if self.training:\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + std * eps, std\n",
    "        else:\n",
    "            return mu, std\n",
    "        \n",
    "    def num_parameters(self):   \n",
    "        trainable_params = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        count = sum([p.numel() for p in trainable_params])\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gt = GTConv(node_in_dim=3, hidden_dim=6, num_heads=2, edge_in_dim=2)\n",
    "#gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#gt(x, edge_index, edge_attr=edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gt_net = GraphTransformerNet(node_dim_in=3, edge_dim_in=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#loader = DataLoader([data], batch_size=128, shuffle=False)\n",
    "#for data in loader:\n",
    "#    x = data.x\n",
    "#    edge_index = data.edge_index\n",
    "#    edge_attr = data.edge_attr\n",
    "#    pred= gt_net(x, edge_index, edge_attr, data.batch)\n",
    "#    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET ZINC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = osp.join('.', 'data', 'ZINC')\n",
    "#train_dataset = ZINC(path, subset=True, split='train')\n",
    "#val_dataset = ZINC(path, subset=True, split='val')\n",
    "#test_dataset = ZINC(path, subset=True, split='test')\n",
    "\n",
    "#train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "## Compute the maximum in-degree in the training data.\n",
    "#max_degree = -1\n",
    "#for data in train_dataset:\n",
    "#    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "#    max_degree = max(max_degree, int(d.max()))\n",
    "#\n",
    "## Compute the in-degree histogram tensor\n",
    "#deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "#for data in train_dataset:\n",
    "#    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "#    deg += torch.bincount(d, minlength=deg.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solubility data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gt_pyg.data.utils import get_tensor_data, get_node_dim, get_edge_dim\n",
    "biogen_data_file = './data/BioGen/biogen_solubility.csv'\n",
    "df = pd.read_csv(biogen_data_file)\n",
    "dataset = get_tensor_data(df.SMILES.to_list(), df.logS.to_list())\n",
    "NODE_DIM = get_node_dim()\n",
    "EDGE_DIM = get_edge_dim()\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=128)\n",
    "val_loader = DataLoader(dataset, batch_size=128)\n",
    "test_loader = DataLoader(dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN AND EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformerNet(\n",
      "  (node_emb): Linear(in_features=79, out_features=128, bias=True)\n",
      "  (edge_emb): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (gt_layers): ModuleList(\n",
      "    (0): GTConv(128, 128, heads=8)\n",
      "    (1): GTConv(128, 128, heads=8)\n",
      "    (2): GTConv(128, 128, heads=8)\n",
      "    (3): GTConv(128, 128, heads=8)\n",
      "  )\n",
      "  (global_pool): MultiAggregation([\n",
      "    SumAggregation(),\n",
      "  ], mode=cat)\n",
      "  (mu_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (std_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of params: 709k\n",
      "Epoch: 01, Loss: 1.5729, Val: 1.4888, Test: 1.4888\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4b/hq3_lmzj215_hw29qxb407dc0000gn/T/ipykernel_65953/2956329829.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mval_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtest_mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4b/hq3_lmzj215_hw29qxb407dc0000gn/T/ipykernel_65953/2956329829.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4b/hq3_lmzj215_hw29qxb407dc0000gn/T/ipykernel_65953/511654770.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, batch, return_std)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgt_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4b/hq3_lmzj215_hw29qxb407dc0000gn/T/ipykernel_65953/511654770.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# FFN-EDGES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mffn_eij_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_eij\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mout_eij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_eij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mout_eij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_eij_in\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout_eij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4b/hq3_lmzj215_hw29qxb407dc0000gn/T/ipykernel_65953/511654770.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gt/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gt/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gt/lib/python3.7/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gt/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphTransformerNet(node_dim_in=NODE_DIM, edge_dim_in=EDGE_DIM, num_gt_layers=4, hidden_dim=128, \n",
    "                           dropout=0.1).to(device)\n",
    "\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    model = torch_geometric.compile(model) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "\n",
    "print(model)\n",
    "print(f\"Number of params: {model.num_parameters()//1000}k\")\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        loss = (out.squeeze() - data.y).abs().mean()\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        total_error += (out.squeeze() - data.y).abs().sum().item()\n",
    "    return total_error / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    loss = train(epoch)\n",
    "    val_mae = test(val_loader)\n",
    "    test_mae = test(test_loader)\n",
    "    scheduler.step(val_mae)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
    "          f'Test: {test_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
