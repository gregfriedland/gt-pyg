{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. https://github.com/xbresson/CS6208_2023/blob/main/codes/labs_lecture07/01_vanilla_graph_transformers.ipynb\n",
    "2. https://github.com/xbresson/CS6208_2023/blob/main/codes/labs_lecture07/03_graph_transformers_regression_exercise.ipynb\n",
    "3. https://github.com/pgniewko/pytorch_geometric/blob/master/torch_geometric/nn/conv/transformer_conv.py\n",
    "4. https://arxiv.org/abs/2012.09699\n",
    "5. https://arxiv.org/abs/1703.04977\n",
    "\n",
    "TDC:\n",
    "1. https://tdcommons.ai/benchmark/overview/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawel/anaconda3/envs/gt/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_scatter.composite import scatter_softmax\n",
    "from torch_scatter.scatter import scatter_add\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn.aggr import MultiAggregation\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Dataset\n",
    "import os.path as osp\n",
    "from torch_geometric.datasets import ZINC\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import degree\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#torch.manual_seed(192837465)\n",
    "#\n",
    "# Generate example data\n",
    "#x = torch.randn(6, 3)  # Node features (6 nodes, 3-dimensional features)\n",
    "#edge_index = torch.tensor([[0, 1, 1, 2, 2, 3, 4, 5], [1, 0, 2, 1, 3, 2, 5, 4]], dtype=torch.long)  # Edge indices\n",
    "#edge_attr = torch.randn(8, 2)  # Edge attributes (8 edges, 2-dimensional attributes)\n",
    "#\n",
    "#data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims, \n",
    "                 num_hidden_layers=1,\n",
    "                 dropout=0.0, act='relu', \n",
    "                 act_kwargs=None):\n",
    "        super(MLP, self).__init__()\n",
    "    \n",
    "        if isinstance(hidden_dims, int):\n",
    "            hidden_dims = [hidden_dims] * num_hidden_layers\n",
    "        \n",
    "        hidden_dims = [input_dim] + hidden_dims\n",
    "        assert len(hidden_dims) - 1 == num_hidden_layers\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        for (i_dim, o_dim) in zip(hidden_dims[:-1], hidden_dims[1:]):\n",
    "            layers.append(nn.Linear(i_dim, o_dim, bias=True))\n",
    "            layers.append(activation_resolver(act, **(act_kwargs or {})))\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(p=dropout))\n",
    "                \n",
    "        layers.append(nn.Linear(hidden_dims[-1], output_dim, bias=True))\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "    \n",
    "class GTConv(MessagePassing):\n",
    "    def __init__(self, node_in_dim, hidden_dim, edge_in_dim=None, num_heads=1, dropout=0.0, norm='bn', act='relu'):\n",
    "        super(GTConv, self).__init__(node_dim=0, aggr='add')\n",
    "        \n",
    "        assert hidden_dim % num_heads == 0\n",
    "        assert (edge_in_dim is None) or (edge_in_dim > 0)\n",
    "        \n",
    "        self.WQ = nn.Linear(node_in_dim, hidden_dim, bias=True)\n",
    "        self.WK = nn.Linear(node_in_dim, hidden_dim, bias=True)\n",
    "        self.WV = nn.Linear(node_in_dim, hidden_dim, bias=True)\n",
    "        self.WO = nn.Linear(hidden_dim, node_in_dim, bias=True)\n",
    "        \n",
    "        if edge_in_dim is not None:\n",
    "            assert node_in_dim == edge_in_dim\n",
    "            self.WE = nn.Linear(edge_in_dim, hidden_dim, bias=True)\n",
    "            self.WOe = nn.Linear(hidden_dim, edge_in_dim, bias=True)\n",
    "            self.ffn_e = MLP(input_dim=edge_in_dim,\n",
    "                             output_dim=edge_in_dim,\n",
    "                             hidden_dims=hidden_dim,\n",
    "                             num_hidden_layers=1,\n",
    "                             dropout=dropout, act=act)\n",
    "            if norm.lower() in ['bn', 'batchnorm', 'batch_norm']:\n",
    "                self.norm1e = nn.BatchNorm1d(edge_in_dim)\n",
    "                self.norm2e = nn.BatchNorm1d(edge_in_dim)\n",
    "            elif norm.lower() in ['ln', 'layernorm', 'layer_norm']:\n",
    "                self.norm1e = nn.LayerNorm(edge_in_dim)\n",
    "                self.norm2e = nn.LayerNorm(edge_in_dim)\n",
    "        else:\n",
    "            self.WE = self.register_parameter('WE', None)\n",
    "            self.WOe = self.register_parameter('WOe', None)\n",
    "            self.ffn_e = self.register_parameter('ffn_e', None)\n",
    "            self.norm1e = self.register_parameter('norm1e', None)\n",
    "            self.norm2e = self.register_parameter('norm2e', None)\n",
    "        \n",
    "        if norm.lower() in ['bn', 'batchnorm', 'batch_norm']:\n",
    "            self.norm1 = nn.BatchNorm1d(node_in_dim)\n",
    "            self.norm2 = nn.BatchNorm1d(node_in_dim)\n",
    "        elif norm.lower() in ['ln', 'layernorm', 'layer_norm']:\n",
    "            self.norm1 = nn.LayerNorm(node_in_dim)\n",
    "            self.norm2 = nn.LayerNorm(node_in_dim)\n",
    "            \n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "            \n",
    "        self.ffn = MLP(input_dim=node_in_dim,\n",
    "                       output_dim=node_in_dim,\n",
    "                       hidden_dims=hidden_dim,\n",
    "                       num_hidden_layers=1,\n",
    "                       dropout=dropout, act=act)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.node_in_dim = node_in_dim\n",
    "        self.edge_in_dim = edge_in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.norm = norm.lower()\n",
    "        \n",
    "        self.reset_parameters()\n",
    "           \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.WQ.weight)\n",
    "        nn.init.xavier_uniform_(self.WK.weight)\n",
    "        nn.init.xavier_uniform_(self.WV.weight)\n",
    "        nn.init.xavier_uniform_(self.WO.weight)\n",
    "        if self.edge_in_dim is None:\n",
    "            nn.init.xavier_uniform_(self.WE.weight)\n",
    "            nn.init.xavier_uniform_(self.WOe.weight)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        x_ = x\n",
    "        Q = self.WQ(x).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        K = self.WK(x).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        V = self.WV(x).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "        \n",
    "        out = self.propagate(edge_index, Q=Q, K=K, V=V,\n",
    "                             edge_attr=edge_attr, size=None)\n",
    "        out = out.view(-1, self.hidden_dim)\n",
    "        \n",
    "        ## NODES\n",
    "        out = self.dropout_layer(out)\n",
    "        out = self.WO(out) + x_ # Residual connection\n",
    "        out = self.norm1(out)\n",
    "        # FFN-NODES\n",
    "        ffn_in = out\n",
    "        out = self.ffn(out)\n",
    "        out = self.norm2(ffn_in + out)\n",
    "        \n",
    "        if self.edge_in_dim is None:\n",
    "            out_eij = None\n",
    "        else:\n",
    "            out_eij = self._eij\n",
    "            self._eij = None\n",
    "            out_eij = out_eij.view(-1, self.hidden_dim)\n",
    "\n",
    "            ## EDGES\n",
    "            out_eij_ = out_eij\n",
    "            out_eij = self.dropout_layer(out_eij)\n",
    "            out_eij = self.WOe(out_eij) + out_eij_ # Residual connection\n",
    "            out_eij = self.norm1e(out_eij)\n",
    "            # FFN-EDGES\n",
    "            ffn_eij_in = out_eij\n",
    "            out_eij = self.ffn_e(out_eij)\n",
    "            out_eij = self.norm2e(ffn_eij_in + out_eij)\n",
    "\n",
    "        return (out, out_eij)\n",
    "        \n",
    "        \n",
    "    def message(self, Q_i, K_j, V_j, index, edge_attr=None):\n",
    "        if self.WE is not None:\n",
    "            assert edge_attr is not None\n",
    "            E = self.WE(edge_attr).view(-1, self.num_heads, self.hidden_dim // self.num_heads)\n",
    "            E_K_j = E * K_j\n",
    "        \n",
    "        d_k = Q_i.size(-1)\n",
    "        qijk = (Q_i * E_K_j).sum(dim=-1) / math.sqrt(d_k)\n",
    "        self._eij = (Q_i * E_K_j) / math.sqrt(d_k)\n",
    "        alpha = softmax(qijk, index) # Log-Sum-Exp trick used. No need for clipping (-5,5)\n",
    "        \n",
    "        return alpha.view(-1, self.num_heads, 1) * V_j\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.node_in_dim}, '\n",
    "                f'{self.hidden_dim}, heads={self.num_heads})')\n",
    "    \n",
    "    \n",
    "class GraphTransformerNet(nn.Module):\n",
    "    def __init__(self, node_dim_in, edge_dim_in=None,\n",
    "                 hidden_dim=128, norm='bn',\n",
    "                 num_gt_layers=4, num_heads=8,\n",
    "                 aggregators=['sum'],\n",
    "                 act='relu', dropout=0.0):\n",
    "        super(GraphTransformerNet, self).__init__()\n",
    "        \n",
    "        self.node_emb = nn.Embedding(node_dim_in, hidden_dim) #nn.Linear(node_dim_in, hidden_dim)\n",
    "        if edge_dim_in:\n",
    "            self.edge_emb = nn.Embedding(edge_dim_in, hidden_dim) #nn.Linear(edge_dim_in, hidden_dim)\n",
    "        else:\n",
    "            self.edge_emb = self.register_parameter('edge_emb', None)\n",
    "        \n",
    "        self.gt_layers = nn.ModuleList()\n",
    "        for _ in range(num_gt_layers):\n",
    "            self.gt_layers.append(GTConv(node_in_dim=hidden_dim,\n",
    "                                         hidden_dim=hidden_dim, \n",
    "                                         edge_in_dim=hidden_dim,\n",
    "                                         num_heads=num_heads,\n",
    "                                         act=act,\n",
    "                                         dropout=dropout,\n",
    "                                         norm='bn'))\n",
    "        \n",
    "        self.global_pool = MultiAggregation(aggregators, mode='cat')\n",
    "        \n",
    "        num_aggrs = len(aggregators)\n",
    "        self.mu_mlp = MLP(input_dim=num_aggrs * hidden_dim, output_dim=1,\n",
    "                          hidden_dims=hidden_dim,\n",
    "                          num_hidden_layers=1, dropout=0.0, act=act)\n",
    "        self.std_mlp = MLP(input_dim=num_aggrs * hidden_dim, output_dim=1,\n",
    "                           hidden_dims=hidden_dim,\n",
    "                           num_hidden_layers=1, dropout=0.0, act=act)\n",
    "        \n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.node_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.edge_emb.weight)\n",
    "            \n",
    "            \n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = self.node_emb(x.squeeze())\n",
    "        edge_attr = self.edge_emb(edge_attr)\n",
    "\n",
    "        for gt_layer in self.gt_layers:\n",
    "            (x, edge_attr) = gt_layer(x, edge_index, edge_attr=edge_attr)\n",
    "\n",
    "        x = self.global_pool(x, batch)\n",
    "        mu = self.mu_mlp(x)\n",
    "        log_var = self.std_mlp(x)\n",
    "        \n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * log_var)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps * std + mu\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def num_parameters(self):   \n",
    "        trainable_params = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        count = sum([p.numel() for p in trainable_params])\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gt = GTConv(node_in_dim=3, hidden_dim=6, num_heads=2, edge_in_dim=2)\n",
    "#gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#gt(x, edge_index, edge_attr=edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gt_net = GraphTransformerNet(node_dim_in=3, edge_dim_in=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#loader = DataLoader([data], batch_size=128, shuffle=False)\n",
    "#for data in loader:\n",
    "#    x = data.x\n",
    "#    edge_index = data.edge_index\n",
    "#    edge_attr = data.edge_attr\n",
    "#    pred= gt_net(x, edge_index, edge_attr, data.batch)\n",
    "#    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET ZINC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = osp.join('.', 'data', 'ZINC')\n",
    "train_dataset = ZINC(path, subset=True, split='train')\n",
    "val_dataset = ZINC(path, subset=True, split='val')\n",
    "test_dataset = ZINC(path, subset=True, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "# Compute the maximum in-degree in the training data.\n",
    "max_degree = -1\n",
    "for data in train_dataset:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    max_degree = max(max_degree, int(d.max()))\n",
    "\n",
    "# Compute the in-degree histogram tensor\n",
    "deg = torch.zeros(max_degree + 1, dtype=torch.long)\n",
    "for data in train_dataset:\n",
    "    d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "    deg += torch.bincount(d, minlength=deg.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN AND EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformerNet(\n",
      "  (node_emb): Embedding(21, 128)\n",
      "  (edge_emb): Embedding(4, 128)\n",
      "  (gt_layers): ModuleList(\n",
      "    (0): GTConv(128, 128, heads=8)\n",
      "    (1): GTConv(128, 128, heads=8)\n",
      "    (2): GTConv(128, 128, heads=8)\n",
      "    (3): GTConv(128, 128, heads=8)\n",
      "  )\n",
      "  (global_pool): MultiAggregation([\n",
      "    SumAggregation(),\n",
      "  ], mode=cat)\n",
      "  (mu_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (std_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of params: 701k\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphTransformerNet(node_dim_in=21, edge_dim_in=4, num_gt_layers=4, hidden_dim=128, \n",
    "                           dropout=0.1).to(device)\n",
    "\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    model = torch_geometric.compile(model) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "\n",
    "print(model)\n",
    "print(f\"Number of params: {model.num_parameters()//1000}k\")\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        loss = (out.squeeze() - data.y).abs().mean()\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        total_error += (out.squeeze() - data.y).abs().sum().item()\n",
    "    return total_error / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    loss = train(epoch)\n",
    "    val_mae = test(val_loader)\n",
    "    test_mae = test(test_loader)\n",
    "    scheduler.step(val_mae)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_mae:.4f}, '\n",
    "          f'Test: {test_mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
