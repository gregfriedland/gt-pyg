{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawel/anaconda3/envs/gt/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "PyTorch is available but CUDA is not. Defaulting to SciPy for SVD\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.21.6\n",
      "Rdkit version: 2022.09.5\n",
      "Torch version: 1.13.1\n",
      "WeightWatcher version: 0.7.1.5\n"
     ]
    }
   ],
   "source": [
    "# Standard\n",
    "import logging\n",
    "\n",
    "# Third party\n",
    "import numpy as np\n",
    "import rdkit\n",
    "from rdkit import RDLogger\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# GT-PyG\n",
    "from gt_pyg.data.utils import (\n",
    "    get_tensor_data, \n",
    "    get_node_dim, \n",
    "    get_edge_dim, \n",
    "    get_train_valid_test_data\n",
    ")\n",
    "from gt_pyg.nn.model import GraphTransformerNet\n",
    "from gt_pyg.util import get_ww_statistic\n",
    "from gt_pyg.util import get_correction_factor, prepare_params\n",
    "\n",
    "import weightwatcher as ww\n",
    "\n",
    "# Turn off majority of RDKit warnings\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "\n",
    "# Set a random seed for a reproducibility purposes\n",
    "torch.manual_seed(192837465)\n",
    "\n",
    "# Setup the logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Log the used versions of RDkit and torch\n",
    "print(f'Numpy version: {np.__version__}')\n",
    "print(f'Rdkit version: {rdkit.__version__}')\n",
    "print(f'Torch version: {torch.__version__}')\n",
    "print(f'WeightWatcher version: {ww.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the ADME@TDC data\n",
    "\n",
    "**Note**: To use the code below, make sure that the chosen endpoint is a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available endpoints:\n",
      "\n",
      "1. caco2_wang\n",
      "2. hia_hou\n",
      "3. pgp_broccatelli\n",
      "4. bioavailability_ma\n",
      "5. lipophilicity_astrazeneca\n",
      "6. solubility_aqsoldb\n",
      "7. bbb_martins\n",
      "8. ppbr_az\n",
      "9. vdss_lombardo\n",
      "10. cyp2d6_veith\n",
      "11. cyp3a4_veith\n",
      "12. cyp2c9_veith\n",
      "13. cyp2d6_substrate_carbonmangels\n",
      "14. cyp3a4_substrate_carbonmangels\n",
      "15. cyp2c9_substrate_carbonmangels\n",
      "16. half_life_obach\n",
      "17. clearance_microsome_az\n",
      "18. clearance_hepatocyte_az\n",
      "19. herg\n",
      "20. ames\n",
      "21. dili\n",
      "22. ld50_zhu\n"
     ]
    }
   ],
   "source": [
    "from tdc import utils\n",
    "names = utils.retrieve_benchmark_names('ADMET_Group')\n",
    "output = \"\\n\".join([f\"{index}. {name}\" for index, name in enumerate(names, start=1)])\n",
    "print(\"Available endpoints:\\n\\n\" + output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression endpoints with MAE metric:\n",
    "1. caco2_wang (Best: 0.285 ± 0.005)\n",
    "2. lipophilicity_astrazeneca (Best: 0.535 ± 0.012)\n",
    "3. solubility_aqsoldb (Best: 0.776 ± 0.008)\n",
    "4. ppbr_az (Best: 9.185 ± 0.000)\n",
    "5. ld50_zhu (Best: 0.588 ± 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 637\n",
      "Number of validation examples: 91\n",
      "Number of test examples: 182\n"
     ]
    }
   ],
   "source": [
    "PE_DIM = 6\n",
    "(tr, va, te) = get_train_valid_test_data('caco2_wang', min_num_atoms=0)\n",
    "tr_dataset = get_tensor_data(tr.Drug.to_list(), tr.Y.to_list(), pe_dim=PE_DIM)\n",
    "va_dataset = get_tensor_data(va.Drug.to_list(), va.Y.to_list(), pe_dim=PE_DIM)\n",
    "te_dataset = get_tensor_data(te.Drug.to_list(), te.Y.to_list(), pe_dim=PE_DIM)\n",
    "NODE_DIM = get_node_dim()\n",
    "EDGE_DIM = get_edge_dim()\n",
    "\n",
    "print(f'Number of training examples: {len(tr_dataset)}')\n",
    "print(f'Number of validation examples: {len(va_dataset)}')\n",
    "print(f'Number of test examples: {len(te_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(tr_dataset, batch_size=128)\n",
    "val_loader = DataLoader(va_dataset, batch_size=512)\n",
    "test_loader = DataLoader(te_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and eval the GT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loss_func, warmup_epochs=5):\n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    if epoch > warmup_epochs:\n",
    "        corrections = get_correction_factor(model, optimizer)\n",
    "        for idx, val in enumerate(corrections):\n",
    "            optimizer.param_groups[idx]['lr'] = val * optimizer.param_groups[idx]['base_lr']\n",
    "            \n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # randomly flip sign of eigenvectors\n",
    "        batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM)).float() - 1.0)\n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch, zero_var=False)\n",
    "        loss = loss_func(out.squeeze(), data.y)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, loss_func):\n",
    "    model.eval()\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        # randomly flip sign of eigenvectors\n",
    "        batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM)).float() - 1.0)\n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch)\n",
    "        total_error += loss_func(out.squeeze(), data.y).item()\n",
    "    return total_error / len(loader.dataset)\n",
    "\n",
    "train_loss = nn.L1Loss(reduction='mean')\n",
    "test_loss = nn.L1Loss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slighlty optimized Graph Transformer architecture\n",
    "\n",
    "1. `gelu` activation is used instead of `relu`\n",
    "2. Multiaggregator used for global pooling\n",
    "3. Multiaggregator used for message passing\n",
    "\n",
    "Number of params 873k instead of 701k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformerNet(\n",
      "  (node_emb): Linear(in_features=76, out_features=128, bias=False)\n",
      "  (edge_emb): Linear(in_features=10, out_features=128, bias=False)\n",
      "  (pe_emb): Linear(in_features=6, out_features=128, bias=False)\n",
      "  (gt_layers): ModuleList(\n",
      "    (0): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "    (1): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "    (2): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "    (3): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "    (4): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "    (5): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "    (6): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "  )\n",
      "  (global_pool): MultiAggregation([\n",
      "    SumAggregation(),\n",
      "    MeanAggregation(),\n",
      "    MaxAggregation(),\n",
      "    StdAggregation(),\n",
      "  ], mode=cat)\n",
      "  (mu_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (log_var_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of params: 1421 k\n",
      "Epoch: 01, Loss: 2.9883, Val: 2.4419, Test: 2.4132\n",
      "Epoch: 02, Loss: 1.0739, Val: 1.2166, Test: 1.4096\n",
      "Epoch: 03, Loss: 0.9701, Val: 1.1901, Test: 1.2551\n",
      "Epoch: 04, Loss: 0.8172, Val: 1.3458, Test: 1.3466\n",
      "Epoch: 05, Loss: 0.6997, Val: 1.0877, Test: 1.1323\n",
      "Epoch: 06, Loss: 0.6212, Val: 0.7521, Test: 0.7411\n",
      "Epoch: 07, Loss: 0.5680, Val: 0.8495, Test: 0.9047\n",
      "Epoch: 08, Loss: 0.6386, Val: 0.7610, Test: 0.7892\n",
      "Epoch: 09, Loss: 0.5252, Val: 0.5662, Test: 0.5964\n",
      "Epoch: 10, Loss: 0.5208, Val: 0.5812, Test: 0.6208\n",
      "Epoch: 11, Loss: 0.5006, Val: 0.5134, Test: 0.5387\n",
      "Epoch: 12, Loss: 0.4761, Val: 0.5500, Test: 0.6310\n",
      "Epoch: 13, Loss: 0.4384, Val: 0.4687, Test: 0.4879\n",
      "Epoch: 14, Loss: 0.4409, Val: 0.4545, Test: 0.4509\n",
      "Epoch: 15, Loss: 0.4448, Val: 0.5599, Test: 0.4970\n",
      "Epoch: 16, Loss: 0.4558, Val: 0.6117, Test: 0.7005\n",
      "Epoch: 17, Loss: 0.4222, Val: 0.5304, Test: 0.5784\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphTransformerNet(node_dim_in=NODE_DIM,\n",
    "                            edge_dim_in=EDGE_DIM,\n",
    "                            pe_in_dim=PE_DIM,\n",
    "                            num_gt_layers=7, \n",
    "                            hidden_dim=128,\n",
    "                            num_heads=8,\n",
    "                            norm='bn',\n",
    "                            gt_aggregators=['sum', 'mean'],\n",
    "                            aggregators=['sum','mean','max', 'std'],\n",
    "                            dropout=0.1,\n",
    "                            act='relu').to(device)\n",
    "\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    model = torch_geometric.compile(model) \n",
    "\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "lr = 0.001\n",
    "params = prepare_params(model, lr)\n",
    "optimizer = torch.optim.AdamW(params)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "print(model)\n",
    "print(f\"Number of params: {model.num_parameters()//1000} k\")\n",
    "\n",
    "\n",
    "best_epoch = 0\n",
    "best_validation_loss = np.inf\n",
    "test_set_mae = np.inf\n",
    "\n",
    "epochs_list = []\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "test_loss_list = []\n",
    "ww_list = []\n",
    "\n",
    "EPOCHS=25\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    tr_loss = train(epoch, loss_func=train_loss, warmup_epochs=2)\n",
    "    va_loss = test(val_loader, loss_func=test_loss)\n",
    "    te_loss = test(test_loader, loss_func=test_loss)\n",
    "    scheduler.step(va_loss)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {tr_loss:.4f}, Val: {va_loss:.4f}, '\n",
    "          f'Test: {te_loss:.4f}')\n",
    "    if va_loss < best_validation_loss:\n",
    "        best_epoch = epoch\n",
    "        best_validation_loss = va_loss\n",
    "        test_set_mae = te_loss\n",
    "    \n",
    "    epochs_list.append(epoch)\n",
    "    train_loss_list.append(tr_loss)\n",
    "    valid_loss_list.append(va_loss)\n",
    "    test_loss_list.append(te_loss)\n",
    "    ww_list.append(get_ww_statistic(model))\n",
    "    \n",
    "print(\"\\nModel's performance on the test set\\n\"\n",
    "        \"===================================\\n\"\n",
    "        f'MAE={test_set_mae}\\n'\n",
    "        f'Epoch={best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_loss_list, ww_list, label='WW statistic')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_loss_list, valid_loss_list, label='Valid loss')\n",
    "plt.scatter(test_loss_list, train_loss_list, label='Train loss')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_list[np.argmin(ww_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_list[np.argmin(valid_loss_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_list[np.argmin(train_loss_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(epochs_list, train_loss_list)\n",
    "#plt.plot(epochs_list, valid_loss_list)\n",
    "#plt.plot(epochs_list, test_loss_list)\n",
    "plt.plot(epochs_list, ww_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_id</th>\n",
       "      <th>name</th>\n",
       "      <th>D</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>Q</th>\n",
       "      <th>alpha</th>\n",
       "      <th>alpha_weighted</th>\n",
       "      <th>entropy</th>\n",
       "      <th>has_esd</th>\n",
       "      <th>...</th>\n",
       "      <th>rf</th>\n",
       "      <th>sigma</th>\n",
       "      <th>spectral_norm</th>\n",
       "      <th>stable_rank</th>\n",
       "      <th>status</th>\n",
       "      <th>sv_max</th>\n",
       "      <th>warning</th>\n",
       "      <th>weak_rank_loss</th>\n",
       "      <th>xmax</th>\n",
       "      <th>xmin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.113996</td>\n",
       "      <td>76</td>\n",
       "      <td>1024</td>\n",
       "      <td>13.473684</td>\n",
       "      <td>16.097953</td>\n",
       "      <td>7.829635</td>\n",
       "      <td>0.991447</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.774392</td>\n",
       "      <td>3.064606</td>\n",
       "      <td>45.963947</td>\n",
       "      <td>success</td>\n",
       "      <td>1.750602</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>0</td>\n",
       "      <td>3.064606</td>\n",
       "      <td>2.529707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.194484</td>\n",
       "      <td>10</td>\n",
       "      <td>1024</td>\n",
       "      <td>102.400000</td>\n",
       "      <td>10.787483</td>\n",
       "      <td>3.886509</td>\n",
       "      <td>0.998182</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.460398</td>\n",
       "      <td>2.292342</td>\n",
       "      <td>8.748985</td>\n",
       "      <td>success</td>\n",
       "      <td>1.514048</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>0</td>\n",
       "      <td>2.292342</td>\n",
       "      <td>1.866512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.124354</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.124620</td>\n",
       "      <td>7.893820</td>\n",
       "      <td>0.927745</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.993275</td>\n",
       "      <td>3.994398</td>\n",
       "      <td>257.099425</td>\n",
       "      <td>success</td>\n",
       "      <td>1.998599</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>2</td>\n",
       "      <td>3.994398</td>\n",
       "      <td>3.270787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.127330</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.020088</td>\n",
       "      <td>10.216418</td>\n",
       "      <td>0.927802</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.083067</td>\n",
       "      <td>3.983423</td>\n",
       "      <td>258.221173</td>\n",
       "      <td>success</td>\n",
       "      <td>1.995852</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>0</td>\n",
       "      <td>3.983423</td>\n",
       "      <td>3.423457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.124529</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.741762</td>\n",
       "      <td>8.975403</td>\n",
       "      <td>0.927685</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.392134</td>\n",
       "      <td>4.062954</td>\n",
       "      <td>252.492299</td>\n",
       "      <td>success</td>\n",
       "      <td>2.015677</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>2</td>\n",
       "      <td>4.062954</td>\n",
       "      <td>3.344246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.093337</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>25.902872</td>\n",
       "      <td>16.502720</td>\n",
       "      <td>0.963767</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>6.429894</td>\n",
       "      <td>4.336107</td>\n",
       "      <td>316.104104</td>\n",
       "      <td>success</td>\n",
       "      <td>2.082332</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>0</td>\n",
       "      <td>4.336107</td>\n",
       "      <td>3.613455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.101943</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.168939</td>\n",
       "      <td>21.544822</td>\n",
       "      <td>0.927763</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>11.389646</td>\n",
       "      <td>4.098353</td>\n",
       "      <td>250.424878</td>\n",
       "      <td>success</td>\n",
       "      <td>2.024439</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>4.098353</td>\n",
       "      <td>3.732165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.126739</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.855527</td>\n",
       "      <td>4.730978</td>\n",
       "      <td>0.927905</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.761725</td>\n",
       "      <td>4.001735</td>\n",
       "      <td>256.991851</td>\n",
       "      <td>success</td>\n",
       "      <td>2.000434</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>4.001735</td>\n",
       "      <td>2.815774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.079860</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.768013</td>\n",
       "      <td>3.333208</td>\n",
       "      <td>0.927123</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.525507</td>\n",
       "      <td>2.039610</td>\n",
       "      <td>169.150866</td>\n",
       "      <td>success</td>\n",
       "      <td>1.428149</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>2.039610</td>\n",
       "      <td>1.087692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.106462</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.708461</td>\n",
       "      <td>4.165708</td>\n",
       "      <td>0.927651</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8.129729</td>\n",
       "      <td>1.432088</td>\n",
       "      <td>240.334707</td>\n",
       "      <td>success</td>\n",
       "      <td>1.196699</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>1.432088</td>\n",
       "      <td>1.254787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.096453</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.100467</td>\n",
       "      <td>4.761023</td>\n",
       "      <td>0.927754</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9.033489</td>\n",
       "      <td>1.477164</td>\n",
       "      <td>233.077643</td>\n",
       "      <td>success</td>\n",
       "      <td>1.215386</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>2</td>\n",
       "      <td>1.477164</td>\n",
       "      <td>1.264030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>33</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.063416</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.147480</td>\n",
       "      <td>3.962568</td>\n",
       "      <td>0.927693</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.117379</td>\n",
       "      <td>1.572817</td>\n",
       "      <td>218.617694</td>\n",
       "      <td>success</td>\n",
       "      <td>1.254120</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>2</td>\n",
       "      <td>1.572817</td>\n",
       "      <td>1.222359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>40</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.115314</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.292335</td>\n",
       "      <td>28.837501</td>\n",
       "      <td>0.927739</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>14.955150</td>\n",
       "      <td>3.954982</td>\n",
       "      <td>259.885786</td>\n",
       "      <td>success</td>\n",
       "      <td>1.988714</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>3.954982</td>\n",
       "      <td>3.715799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>41</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.125555</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.534922</td>\n",
       "      <td>17.046373</td>\n",
       "      <td>0.927956</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.359017</td>\n",
       "      <td>3.957185</td>\n",
       "      <td>259.342505</td>\n",
       "      <td>success</td>\n",
       "      <td>1.989267</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>3.957185</td>\n",
       "      <td>3.630112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>42</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.131041</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.185379</td>\n",
       "      <td>4.304288</td>\n",
       "      <td>0.927881</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.651996</td>\n",
       "      <td>3.972229</td>\n",
       "      <td>258.107083</td>\n",
       "      <td>success</td>\n",
       "      <td>1.993045</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>0</td>\n",
       "      <td>3.972229</td>\n",
       "      <td>2.713394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>43</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.096014</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>29.082985</td>\n",
       "      <td>17.809292</td>\n",
       "      <td>0.963831</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.505493</td>\n",
       "      <td>4.096012</td>\n",
       "      <td>334.537257</td>\n",
       "      <td>success</td>\n",
       "      <td>2.023861</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>0</td>\n",
       "      <td>4.096012</td>\n",
       "      <td>3.625959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>44</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.108937</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.652790</td>\n",
       "      <td>8.963690</td>\n",
       "      <td>0.927801</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.452113</td>\n",
       "      <td>4.090152</td>\n",
       "      <td>251.236454</td>\n",
       "      <td>success</td>\n",
       "      <td>2.022412</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>0</td>\n",
       "      <td>4.090152</td>\n",
       "      <td>3.359395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>45</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.123023</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.150814</td>\n",
       "      <td>7.340680</td>\n",
       "      <td>0.927891</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.700483</td>\n",
       "      <td>4.019118</td>\n",
       "      <td>255.713504</td>\n",
       "      <td>success</td>\n",
       "      <td>2.004774</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>2</td>\n",
       "      <td>4.019118</td>\n",
       "      <td>3.209707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>48</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.061406</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.551742</td>\n",
       "      <td>3.367903</td>\n",
       "      <td>0.927480</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.510348</td>\n",
       "      <td>1.772237</td>\n",
       "      <td>194.743908</td>\n",
       "      <td>success</td>\n",
       "      <td>1.331254</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>2</td>\n",
       "      <td>1.772237</td>\n",
       "      <td>1.162084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>51</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.098110</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.992245</td>\n",
       "      <td>3.424743</td>\n",
       "      <td>0.927591</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>6.348614</td>\n",
       "      <td>1.409133</td>\n",
       "      <td>244.501370</td>\n",
       "      <td>success</td>\n",
       "      <td>1.187069</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>1.409133</td>\n",
       "      <td>1.240735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>59</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.098842</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.390038</td>\n",
       "      <td>4.407082</td>\n",
       "      <td>0.927852</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9.130013</td>\n",
       "      <td>1.429662</td>\n",
       "      <td>240.672282</td>\n",
       "      <td>success</td>\n",
       "      <td>1.195685</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>2</td>\n",
       "      <td>1.429662</td>\n",
       "      <td>1.251513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>62</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.081537</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.280779</td>\n",
       "      <td>4.425481</td>\n",
       "      <td>0.927784</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8.310712</td>\n",
       "      <td>1.452847</td>\n",
       "      <td>236.919833</td>\n",
       "      <td>success</td>\n",
       "      <td>1.205341</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>1.452847</td>\n",
       "      <td>1.248106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>69</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.123481</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.790523</td>\n",
       "      <td>8.858224</td>\n",
       "      <td>0.927878</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.365055</td>\n",
       "      <td>3.971112</td>\n",
       "      <td>259.033756</td>\n",
       "      <td>success</td>\n",
       "      <td>1.992765</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>3.971112</td>\n",
       "      <td>3.329580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>70</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.127242</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.277039</td>\n",
       "      <td>4.970370</td>\n",
       "      <td>0.927872</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.834734</td>\n",
       "      <td>3.985666</td>\n",
       "      <td>257.706654</td>\n",
       "      <td>success</td>\n",
       "      <td>1.996413</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>3.985666</td>\n",
       "      <td>2.857747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>71</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.105547</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.767179</td>\n",
       "      <td>13.085979</td>\n",
       "      <td>0.927847</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.643682</td>\n",
       "      <td>3.991898</td>\n",
       "      <td>256.991730</td>\n",
       "      <td>success</td>\n",
       "      <td>1.997973</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>3.991898</td>\n",
       "      <td>3.548455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>72</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.085621</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.328432</td>\n",
       "      <td>17.626116</td>\n",
       "      <td>0.963836</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>6.832108</td>\n",
       "      <td>4.189921</td>\n",
       "      <td>326.903334</td>\n",
       "      <td>success</td>\n",
       "      <td>2.046930</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>0</td>\n",
       "      <td>4.189921</td>\n",
       "      <td>3.597805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>73</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.128002</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.961790</td>\n",
       "      <td>11.928903</td>\n",
       "      <td>0.927942</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.042667</td>\n",
       "      <td>3.959012</td>\n",
       "      <td>259.439131</td>\n",
       "      <td>success</td>\n",
       "      <td>1.989727</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>3.959012</td>\n",
       "      <td>3.498544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>74</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.125337</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.552562</td>\n",
       "      <td>6.917689</td>\n",
       "      <td>0.927742</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.555890</td>\n",
       "      <td>3.970098</td>\n",
       "      <td>258.840853</td>\n",
       "      <td>success</td>\n",
       "      <td>1.992511</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>2</td>\n",
       "      <td>3.970098</td>\n",
       "      <td>3.169521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>77</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.090257</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.056578</td>\n",
       "      <td>3.753545</td>\n",
       "      <td>0.927421</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.285615</td>\n",
       "      <td>1.713050</td>\n",
       "      <td>201.574695</td>\n",
       "      <td>success</td>\n",
       "      <td>1.308835</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>1.713050</td>\n",
       "      <td>1.192656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>80</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.078467</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.504623</td>\n",
       "      <td>4.360876</td>\n",
       "      <td>0.927685</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.749042</td>\n",
       "      <td>1.482463</td>\n",
       "      <td>232.810414</td>\n",
       "      <td>success</td>\n",
       "      <td>1.217564</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>1.482463</td>\n",
       "      <td>1.267534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>88</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.083191</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.098084</td>\n",
       "      <td>3.466212</td>\n",
       "      <td>0.927631</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.024521</td>\n",
       "      <td>1.459790</td>\n",
       "      <td>235.682991</td>\n",
       "      <td>success</td>\n",
       "      <td>1.208218</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>1.459790</td>\n",
       "      <td>1.211384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>91</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.106149</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.306125</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>0.927776</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.753215</td>\n",
       "      <td>1.500085</td>\n",
       "      <td>229.181076</td>\n",
       "      <td>success</td>\n",
       "      <td>1.224780</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>1.500085</td>\n",
       "      <td>1.141945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>98</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.130034</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.372255</td>\n",
       "      <td>3.831962</td>\n",
       "      <td>0.927776</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.516946</td>\n",
       "      <td>3.993475</td>\n",
       "      <td>257.469446</td>\n",
       "      <td>success</td>\n",
       "      <td>1.998368</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>3.993475</td>\n",
       "      <td>2.571163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>99</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.115998</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.557879</td>\n",
       "      <td>14.092282</td>\n",
       "      <td>0.927985</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.175132</td>\n",
       "      <td>3.964589</td>\n",
       "      <td>259.438142</td>\n",
       "      <td>success</td>\n",
       "      <td>1.991128</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>3.964589</td>\n",
       "      <td>3.549999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>100</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.126862</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.842718</td>\n",
       "      <td>11.280242</td>\n",
       "      <td>0.927808</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.642130</td>\n",
       "      <td>3.968739</td>\n",
       "      <td>258.515345</td>\n",
       "      <td>success</td>\n",
       "      <td>1.992170</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>3.968739</td>\n",
       "      <td>3.466042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>101</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.113020</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>18.595689</td>\n",
       "      <td>11.273151</td>\n",
       "      <td>0.963684</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.017636</td>\n",
       "      <td>4.038535</td>\n",
       "      <td>339.571601</td>\n",
       "      <td>success</td>\n",
       "      <td>2.009611</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>0</td>\n",
       "      <td>4.038535</td>\n",
       "      <td>3.388552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>102</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.115697</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.557621</td>\n",
       "      <td>12.090616</td>\n",
       "      <td>0.927790</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.049605</td>\n",
       "      <td>4.151498</td>\n",
       "      <td>247.732229</td>\n",
       "      <td>success</td>\n",
       "      <td>2.037523</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>1</td>\n",
       "      <td>4.151498</td>\n",
       "      <td>3.520830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>103</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.126532</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.304636</td>\n",
       "      <td>4.356666</td>\n",
       "      <td>0.927899</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.664567</td>\n",
       "      <td>3.948434</td>\n",
       "      <td>259.298650</td>\n",
       "      <td>success</td>\n",
       "      <td>1.987067</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>0</td>\n",
       "      <td>3.948434</td>\n",
       "      <td>2.713910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>106</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.126533</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.604077</td>\n",
       "      <td>2.121728</td>\n",
       "      <td>0.927818</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.137876</td>\n",
       "      <td>1.319851</td>\n",
       "      <td>258.483991</td>\n",
       "      <td>success</td>\n",
       "      <td>1.148848</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>2</td>\n",
       "      <td>1.319851</td>\n",
       "      <td>1.137239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>109</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.127554</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.901352</td>\n",
       "      <td>1.991086</td>\n",
       "      <td>0.927918</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.720602</td>\n",
       "      <td>1.334181</td>\n",
       "      <td>255.879834</td>\n",
       "      <td>success</td>\n",
       "      <td>1.155068</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>2</td>\n",
       "      <td>1.334181</td>\n",
       "      <td>1.124122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>117</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.071467</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.276028</td>\n",
       "      <td>3.968103</td>\n",
       "      <td>0.927792</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.853185</td>\n",
       "      <td>1.536405</td>\n",
       "      <td>224.224352</td>\n",
       "      <td>success</td>\n",
       "      <td>1.239518</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>0</td>\n",
       "      <td>1.536405</td>\n",
       "      <td>1.234615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>120</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.071380</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.855661</td>\n",
       "      <td>3.758810</td>\n",
       "      <td>0.927612</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5.443160</td>\n",
       "      <td>1.546346</td>\n",
       "      <td>222.773102</td>\n",
       "      <td>success</td>\n",
       "      <td>1.243522</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>2</td>\n",
       "      <td>1.546346</td>\n",
       "      <td>1.231878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>131</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.099344</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.321750</td>\n",
       "      <td>3.488313</td>\n",
       "      <td>0.976074</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.276672</td>\n",
       "      <td>4.523579</td>\n",
       "      <td>78.322638</td>\n",
       "      <td>success</td>\n",
       "      <td>2.126871</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4.523579</td>\n",
       "      <td>0.469861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>136</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.095891</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.064553</td>\n",
       "      <td>6.483698</td>\n",
       "      <td>0.969644</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.359922</td>\n",
       "      <td>11.725030</td>\n",
       "      <td>30.617642</td>\n",
       "      <td>success</td>\n",
       "      <td>3.424183</td>\n",
       "      <td>under-trained</td>\n",
       "      <td>0</td>\n",
       "      <td>11.725030</td>\n",
       "      <td>0.506993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    layer_id    name         D     M     N           Q      alpha  \\\n",
       "0          1  Linear  0.113996    76  1024   13.473684  16.097953   \n",
       "1          2  Linear  0.194484    10  1024  102.400000  10.787483   \n",
       "2         11  Linear  0.124354  1024  1024    1.000000  13.124620   \n",
       "3         12  Linear  0.127330  1024  1024    1.000000  17.020088   \n",
       "4         13  Linear  0.124529  1024  1024    1.000000  14.741762   \n",
       "5         14  Linear  0.093337  1024  2048    2.000000  25.902872   \n",
       "6         15  Linear  0.101943  1024  1024    1.000000  35.168939   \n",
       "7         16  Linear  0.126739  1024  1024    1.000000   7.855527   \n",
       "8         19  Linear  0.079860  1024  1024    1.000000  10.768013   \n",
       "9         22  Linear  0.106462  1024  1024    1.000000  26.708461   \n",
       "10        30  Linear  0.096453  1024  1024    1.000000  28.100467   \n",
       "11        33  Linear  0.063416  1024  1024    1.000000  20.147480   \n",
       "12        40  Linear  0.115314  1024  1024    1.000000  48.292335   \n",
       "13        41  Linear  0.125555  1024  1024    1.000000  28.534922   \n",
       "14        42  Linear  0.131041  1024  1024    1.000000   7.185379   \n",
       "15        43  Linear  0.096014  1024  2048    2.000000  29.082985   \n",
       "16        44  Linear  0.108937  1024  1024    1.000000  14.652790   \n",
       "17        45  Linear  0.123023  1024  1024    1.000000  12.150814   \n",
       "18        48  Linear  0.061406  1024  1024    1.000000  13.551742   \n",
       "19        51  Linear  0.098110  1024  1024    1.000000  22.992245   \n",
       "20        59  Linear  0.098842  1024  1024    1.000000  28.390038   \n",
       "21        62  Linear  0.081537  1024  1024    1.000000  27.280779   \n",
       "22        69  Linear  0.123481  1024  1024    1.000000  14.790523   \n",
       "23        70  Linear  0.127242  1024  1024    1.000000   8.277039   \n",
       "24        71  Linear  0.105547  1024  1024    1.000000  21.767179   \n",
       "25        72  Linear  0.085621  1024  2048    2.000000  28.328432   \n",
       "26        73  Linear  0.128002  1024  1024    1.000000  19.961790   \n",
       "27        74  Linear  0.125337  1024  1024    1.000000  11.552562   \n",
       "28        77  Linear  0.090257  1024  1024    1.000000  16.056578   \n",
       "29        80  Linear  0.078467  1024  1024    1.000000  25.504623   \n",
       "30        88  Linear  0.083191  1024  1024    1.000000  21.098084   \n",
       "31        91  Linear  0.106149  1024  1024    1.000000  15.306125   \n",
       "32        98  Linear  0.130034  1024  1024    1.000000   6.372255   \n",
       "33        99  Linear  0.115998  1024  1024    1.000000  23.557879   \n",
       "34       100  Linear  0.126862  1024  1024    1.000000  18.842718   \n",
       "35       101  Linear  0.113020  1024  2048    2.000000  18.595689   \n",
       "36       102  Linear  0.115697  1024  1024    1.000000  19.557621   \n",
       "37       103  Linear  0.126532  1024  1024    1.000000   7.304636   \n",
       "38       106  Linear  0.126533  1024  1024    1.000000  17.604077   \n",
       "39       109  Linear  0.127554  1024  1024    1.000000  15.901352   \n",
       "40       117  Linear  0.071467  1024  1024    1.000000  21.276028   \n",
       "41       120  Linear  0.071380  1024  1024    1.000000  19.855661   \n",
       "42       131  Linear  0.099344  1024  4096    4.000000   5.321750   \n",
       "43       136  Linear  0.095891  1024  4096    4.000000   6.064553   \n",
       "\n",
       "    alpha_weighted   entropy  has_esd  ...  rf      sigma  spectral_norm  \\\n",
       "0         7.829635  0.991447     True  ...   1   4.774392       3.064606   \n",
       "1         3.886509  0.998182     True  ...   1   3.460398       2.292342   \n",
       "2         7.893820  0.927745     True  ...   1   1.993275       3.994398   \n",
       "3        10.216418  0.927802     True  ...   1   3.083067       3.983423   \n",
       "4         8.975403  0.927685     True  ...   1   2.392134       4.062954   \n",
       "5        16.502720  0.963767     True  ...   1   6.429894       4.336107   \n",
       "6        21.544822  0.927763     True  ...   1  11.389646       4.098353   \n",
       "7         4.730978  0.927905     True  ...   1   0.761725       4.001735   \n",
       "8         3.333208  0.927123     True  ...   1   1.525507       2.039610   \n",
       "9         4.165708  0.927651     True  ...   1   8.129729       1.432088   \n",
       "10        4.761023  0.927754     True  ...   1   9.033489       1.477164   \n",
       "11        3.962568  0.927693     True  ...   1   5.117379       1.572817   \n",
       "12       28.837501  0.927739     True  ...   1  14.955150       3.954982   \n",
       "13       17.046373  0.927956     True  ...   1   7.359017       3.957185   \n",
       "14        4.304288  0.927881     True  ...   1   0.651996       3.972229   \n",
       "15       17.809292  0.963831     True  ...   1   7.505493       4.096012   \n",
       "16        8.963690  0.927801     True  ...   1   2.452113       4.090152   \n",
       "17        7.340680  0.927891     True  ...   1   1.700483       4.019118   \n",
       "18        3.367903  0.927480     True  ...   1   2.510348       1.772237   \n",
       "19        3.424743  0.927591     True  ...   1   6.348614       1.409133   \n",
       "20        4.407082  0.927852     True  ...   1   9.130013       1.429662   \n",
       "21        4.425481  0.927784     True  ...   1   8.310712       1.452847   \n",
       "22        8.858224  0.927878     True  ...   1   2.365055       3.971112   \n",
       "23        4.970370  0.927872     True  ...   1   0.834734       3.985666   \n",
       "24       13.085979  0.927847     True  ...   1   4.643682       3.991898   \n",
       "25       17.626116  0.963836     True  ...   1   6.832108       4.189921   \n",
       "26       11.928903  0.927942     True  ...   1   4.042667       3.959012   \n",
       "27        6.917689  0.927742     True  ...   1   1.555890       3.970098   \n",
       "28        3.753545  0.927421     True  ...   1   3.285615       1.713050   \n",
       "29        4.360876  0.927685     True  ...   1   7.749042       1.482463   \n",
       "30        3.466212  0.927631     True  ...   1   5.024521       1.459790   \n",
       "31        2.695652  0.927776     True  ...   1   2.753215       1.500085   \n",
       "32        3.831962  0.927776     True  ...   1   0.516946       3.993475   \n",
       "33       14.092282  0.927985     True  ...   1   5.175132       3.964589   \n",
       "34       11.280242  0.927808     True  ...   1   3.642130       3.968739   \n",
       "35       11.273151  0.963684     True  ...   1   3.017636       4.038535   \n",
       "36       12.090616  0.927790     True  ...   1   4.049605       4.151498   \n",
       "37        4.356666  0.927899     True  ...   1   0.664567       3.948434   \n",
       "38        2.121728  0.927818     True  ...   1   3.137876       1.319851   \n",
       "39        1.991086  0.927918     True  ...   1   2.720602       1.334181   \n",
       "40        3.968103  0.927792     True  ...   1   5.853185       1.536405   \n",
       "41        3.758810  0.927612     True  ...   1   5.443160       1.546346   \n",
       "42        3.488313  0.976074     True  ...   1   0.276672       4.523579   \n",
       "43        6.483698  0.969644     True  ...   1   0.359922      11.725030   \n",
       "\n",
       "    stable_rank   status    sv_max        warning  weak_rank_loss       xmax  \\\n",
       "0     45.963947  success  1.750602  under-trained               0   3.064606   \n",
       "1      8.748985  success  1.514048  under-trained               0   2.292342   \n",
       "2    257.099425  success  1.998599  under-trained               2   3.994398   \n",
       "3    258.221173  success  1.995852  under-trained               0   3.983423   \n",
       "4    252.492299  success  2.015677  under-trained               2   4.062954   \n",
       "5    316.104104  success  2.082332  under-trained               0   4.336107   \n",
       "6    250.424878  success  2.024439  under-trained               1   4.098353   \n",
       "7    256.991851  success  2.000434  under-trained               1   4.001735   \n",
       "8    169.150866  success  1.428149  under-trained               1   2.039610   \n",
       "9    240.334707  success  1.196699  under-trained               1   1.432088   \n",
       "10   233.077643  success  1.215386  under-trained               2   1.477164   \n",
       "11   218.617694  success  1.254120  under-trained               2   1.572817   \n",
       "12   259.885786  success  1.988714  under-trained               1   3.954982   \n",
       "13   259.342505  success  1.989267  under-trained               1   3.957185   \n",
       "14   258.107083  success  1.993045  under-trained               0   3.972229   \n",
       "15   334.537257  success  2.023861  under-trained               0   4.096012   \n",
       "16   251.236454  success  2.022412  under-trained               0   4.090152   \n",
       "17   255.713504  success  2.004774  under-trained               2   4.019118   \n",
       "18   194.743908  success  1.331254  under-trained               2   1.772237   \n",
       "19   244.501370  success  1.187069  under-trained               1   1.409133   \n",
       "20   240.672282  success  1.195685  under-trained               2   1.429662   \n",
       "21   236.919833  success  1.205341  under-trained               1   1.452847   \n",
       "22   259.033756  success  1.992765  under-trained               1   3.971112   \n",
       "23   257.706654  success  1.996413  under-trained               1   3.985666   \n",
       "24   256.991730  success  1.997973  under-trained               1   3.991898   \n",
       "25   326.903334  success  2.046930  under-trained               0   4.189921   \n",
       "26   259.439131  success  1.989727  under-trained               1   3.959012   \n",
       "27   258.840853  success  1.992511  under-trained               2   3.970098   \n",
       "28   201.574695  success  1.308835  under-trained               1   1.713050   \n",
       "29   232.810414  success  1.217564  under-trained               1   1.482463   \n",
       "30   235.682991  success  1.208218  under-trained               1   1.459790   \n",
       "31   229.181076  success  1.224780  under-trained               1   1.500085   \n",
       "32   257.469446  success  1.998368  under-trained               1   3.993475   \n",
       "33   259.438142  success  1.991128  under-trained               1   3.964589   \n",
       "34   258.515345  success  1.992170  under-trained               1   3.968739   \n",
       "35   339.571601  success  2.009611  under-trained               0   4.038535   \n",
       "36   247.732229  success  2.037523  under-trained               1   4.151498   \n",
       "37   259.298650  success  1.987067  under-trained               0   3.948434   \n",
       "38   258.483991  success  1.148848  under-trained               2   1.319851   \n",
       "39   255.879834  success  1.155068  under-trained               2   1.334181   \n",
       "40   224.224352  success  1.239518  under-trained               0   1.536405   \n",
       "41   222.773102  success  1.243522  under-trained               2   1.546346   \n",
       "42    78.322638  success  2.126871                              0   4.523579   \n",
       "43    30.617642  success  3.424183  under-trained               0  11.725030   \n",
       "\n",
       "        xmin  \n",
       "0   2.529707  \n",
       "1   1.866512  \n",
       "2   3.270787  \n",
       "3   3.423457  \n",
       "4   3.344246  \n",
       "5   3.613455  \n",
       "6   3.732165  \n",
       "7   2.815774  \n",
       "8   1.087692  \n",
       "9   1.254787  \n",
       "10  1.264030  \n",
       "11  1.222359  \n",
       "12  3.715799  \n",
       "13  3.630112  \n",
       "14  2.713394  \n",
       "15  3.625959  \n",
       "16  3.359395  \n",
       "17  3.209707  \n",
       "18  1.162084  \n",
       "19  1.240735  \n",
       "20  1.251513  \n",
       "21  1.248106  \n",
       "22  3.329580  \n",
       "23  2.857747  \n",
       "24  3.548455  \n",
       "25  3.597805  \n",
       "26  3.498544  \n",
       "27  3.169521  \n",
       "28  1.192656  \n",
       "29  1.267534  \n",
       "30  1.211384  \n",
       "31  1.141945  \n",
       "32  2.571163  \n",
       "33  3.549999  \n",
       "34  3.466042  \n",
       "35  3.388552  \n",
       "36  3.520830  \n",
       "37  2.713910  \n",
       "38  1.137239  \n",
       "39  1.124122  \n",
       "40  1.234615  \n",
       "41  1.231878  \n",
       "42  0.469861  \n",
       "43  0.506993  \n",
       "\n",
       "[44 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watcher = ww.WeightWatcher(model=model)\n",
    "details = watcher.analyze(plot=False)\n",
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_norm': 2.7709467323522996,\n",
       " 'alpha': 18.66897386077971,\n",
       " 'alpha_weighted': 8.048410570823066,\n",
       " 'log_alpha_norm': 8.663418134741027,\n",
       " 'log_spectral_norm': 0.45470587308592203,\n",
       " 'stable_rank': 233.03134457881845}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watcher.get_summary(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
