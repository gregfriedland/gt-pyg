{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawel/anaconda3/envs/gt/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.21.6\n",
      "Rdkit version: 2022.09.5\n",
      "Torch version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Standard\n",
    "import logging\n",
    "\n",
    "# Third party\n",
    "import numpy as np\n",
    "import rdkit\n",
    "from rdkit import RDLogger\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# GT-PyG\n",
    "from gt_pyg.data.utils import (\n",
    "    get_tensor_data, \n",
    "    get_node_dim, \n",
    "    get_edge_dim, \n",
    "    get_train_valid_test_data\n",
    ")\n",
    "from gt_pyg.nn.model import GraphTransformerNet\n",
    "\n",
    "# Turn off majority of RDKit warnings\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "\n",
    "# Set a random seed for a reproducibility purposes\n",
    "torch.manual_seed(192837465)\n",
    "\n",
    "# Setup the logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Log the used versions of RDkit and torch\n",
    "print(f'Numpy version: {np.__version__}')\n",
    "print(f'Rdkit version: {rdkit.__version__}')\n",
    "print(f'Torch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the ADME@TDC data\n",
    "\n",
    "**Note**: To use the code below, make sure that the chosen endpoint is a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available endpoints:\n",
      "\n",
      "1. caco2_wang\n",
      "2. hia_hou\n",
      "3. pgp_broccatelli\n",
      "4. bioavailability_ma\n",
      "5. lipophilicity_astrazeneca\n",
      "6. solubility_aqsoldb\n",
      "7. bbb_martins\n",
      "8. ppbr_az\n",
      "9. vdss_lombardo\n",
      "10. cyp2d6_veith\n",
      "11. cyp3a4_veith\n",
      "12. cyp2c9_veith\n",
      "13. cyp2d6_substrate_carbonmangels\n",
      "14. cyp3a4_substrate_carbonmangels\n",
      "15. cyp2c9_substrate_carbonmangels\n",
      "16. half_life_obach\n",
      "17. clearance_microsome_az\n",
      "18. clearance_hepatocyte_az\n",
      "19. herg\n",
      "20. ames\n",
      "21. dili\n",
      "22. ld50_zhu\n"
     ]
    }
   ],
   "source": [
    "from tdc import utils\n",
    "names = utils.retrieve_benchmark_names('ADMET_Group')\n",
    "output = \"\\n\".join([f\"{index}. {name}\" for index, name in enumerate(names, start=1)])\n",
    "print(\"Available endpoints:\\n\\n\" + output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression endpoints with MAE metric:\n",
    "1. caco2_wang (Best: 0.285 ± 0.005)\n",
    "2. lipophilicity_astrazeneca (Best: 0.535 ± 0.012)\n",
    "3. solubility_aqsoldb (Best: 0.776 ± 0.008)\n",
    "4. ppbr_az (Best: 9.185 ± 0.000)\n",
    "5. ld50_zhu (Best: 0.588 ± 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 637\n",
      "Number of validation examples: 91\n",
      "Number of test examples: 182\n"
     ]
    }
   ],
   "source": [
    "PE_DIM = 6\n",
    "(tr, va, te) = get_train_valid_test_data('caco2_wang', min_num_atoms=0)\n",
    "tr_dataset = get_tensor_data(tr.Drug.to_list(), tr.Y.to_list(), pe_dim=PE_DIM)\n",
    "va_dataset = get_tensor_data(va.Drug.to_list(), va.Y.to_list(), pe_dim=PE_DIM)\n",
    "te_dataset = get_tensor_data(te.Drug.to_list(), te.Y.to_list(), pe_dim=PE_DIM)\n",
    "NODE_DIM = get_node_dim()\n",
    "EDGE_DIM = get_edge_dim()\n",
    "\n",
    "print(f'Number of training examples: {len(tr_dataset)}')\n",
    "print(f'Number of validation examples: {len(va_dataset)}')\n",
    "print(f'Number of test examples: {len(te_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(tr_dataset, batch_size=64)\n",
    "val_loader = DataLoader(va_dataset, batch_size=64)\n",
    "test_loader = DataLoader(te_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and eval the GT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loss_func):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # randomly flip sign of eigenvectors\n",
    "        batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM)).float() - 1.0)\n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch, zero_var=False)\n",
    "        loss = loss_func(out.squeeze(), data.y)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, loss_func):\n",
    "    model.eval()\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        # randomly flip sign of eigenvectors\n",
    "        batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM)).float() - 1.0)\n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch)\n",
    "        total_error += loss_func(out.squeeze(), data.y).item()\n",
    "    return total_error / len(loader.dataset)\n",
    "\n",
    "train_loss = nn.L1Loss(reduction='mean')\n",
    "test_loss = nn.L1Loss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Graph Transformer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformerNet(\n",
      "  (node_emb): Linear(in_features=76, out_features=128, bias=True)\n",
      "  (edge_emb): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (pe_emb): Linear(in_features=6, out_features=128, bias=True)\n",
      "  (gt_layers): ModuleList(\n",
      "    (0): GTConv(128, 128, heads=8, aggrss: aggrs)\n",
      "    (1): GTConv(128, 128, heads=8, aggrss: aggrs)\n",
      "    (2): GTConv(128, 128, heads=8, aggrss: aggrs)\n",
      "    (3): GTConv(128, 128, heads=8, aggrss: aggrs)\n",
      "  )\n",
      "  (global_pool): MultiAggregation([\n",
      "    SumAggregation(),\n",
      "  ], mode=cat)\n",
      "  (mu_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (log_var_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of params: 710 k\n",
      "Epoch: 01, Loss: 254.1053, Val: 79.3010, Test: 78.4710\n",
      "Epoch: 02, Loss: 76.0497, Val: 73.4988, Test: 72.6591\n",
      "Epoch: 03, Loss: 64.1314, Val: 61.7732, Test: 57.9510\n",
      "Epoch: 04, Loss: 51.5420, Val: 48.0345, Test: 44.6130\n",
      "Epoch: 05, Loss: 44.8844, Val: 48.0324, Test: 45.5972\n",
      "Epoch: 06, Loss: 44.6917, Val: 46.8293, Test: 44.7837\n",
      "Epoch: 07, Loss: 42.7092, Val: 44.4894, Test: 42.4372\n",
      "Epoch: 08, Loss: 41.0560, Val: 42.3858, Test: 40.1532\n",
      "Epoch: 09, Loss: 38.8777, Val: 40.1157, Test: 38.2254\n",
      "Epoch: 10, Loss: 37.3598, Val: 37.7406, Test: 36.2491\n",
      "Epoch: 11, Loss: 34.8540, Val: 35.7157, Test: 33.9792\n",
      "Epoch: 12, Loss: 32.8258, Val: 33.5422, Test: 33.0523\n",
      "Epoch: 13, Loss: 31.9717, Val: 31.9795, Test: 31.9779\n",
      "Epoch: 14, Loss: 30.5177, Val: 30.7269, Test: 30.8389\n",
      "Epoch: 15, Loss: 29.8629, Val: 29.7777, Test: 29.9607\n",
      "Epoch: 16, Loss: 28.6261, Val: 28.6905, Test: 28.9838\n",
      "Epoch: 17, Loss: 26.7790, Val: 27.2219, Test: 27.6159\n",
      "Epoch: 18, Loss: 26.4050, Val: 26.0542, Test: 26.7689\n",
      "Epoch: 19, Loss: 25.6892, Val: 24.8175, Test: 25.9487\n",
      "Epoch: 20, Loss: 25.3781, Val: 23.4117, Test: 25.0469\n",
      "Epoch: 21, Loss: 24.1819, Val: 22.2140, Test: 24.0674\n",
      "Epoch: 22, Loss: 23.5998, Val: 21.2239, Test: 23.2185\n",
      "Epoch: 23, Loss: 22.8753, Val: 21.0641, Test: 22.4323\n",
      "Epoch: 24, Loss: 23.0506, Val: 20.6185, Test: 21.7326\n",
      "Epoch: 25, Loss: 22.2194, Val: 20.0244, Test: 21.0596\n",
      "Epoch: 26, Loss: 20.9897, Val: 19.0275, Test: 21.0176\n",
      "Epoch: 27, Loss: 20.9660, Val: 18.0247, Test: 20.0478\n",
      "Epoch: 28, Loss: 20.6049, Val: 17.5889, Test: 19.2263\n",
      "Epoch: 29, Loss: 20.5756, Val: 17.1742, Test: 18.9936\n",
      "Epoch: 30, Loss: 19.4789, Val: 17.1501, Test: 18.4565\n",
      "Epoch: 31, Loss: 19.0449, Val: 16.8696, Test: 18.1420\n",
      "Epoch: 32, Loss: 18.4710, Val: 16.7432, Test: 17.7979\n",
      "Epoch: 33, Loss: 17.7872, Val: 15.1623, Test: 17.5954\n",
      "Epoch: 34, Loss: 18.0856, Val: 15.7846, Test: 17.1644\n",
      "Epoch: 35, Loss: 17.3754, Val: 15.6339, Test: 17.0505\n",
      "Epoch: 36, Loss: 17.1157, Val: 15.6462, Test: 16.8550\n",
      "Epoch: 37, Loss: 16.7787, Val: 15.2373, Test: 16.5674\n",
      "Epoch: 38, Loss: 16.5217, Val: 14.7471, Test: 17.0250\n",
      "Epoch: 39, Loss: 15.9122, Val: 14.8097, Test: 16.4509\n",
      "Epoch: 40, Loss: 15.6419, Val: 14.7453, Test: 16.2121\n",
      "Epoch: 41, Loss: 15.9401, Val: 14.6130, Test: 15.8768\n",
      "Epoch: 42, Loss: 14.9600, Val: 14.9719, Test: 15.4060\n",
      "Epoch: 43, Loss: 15.3392, Val: 13.6407, Test: 15.5631\n",
      "Epoch: 44, Loss: 15.3057, Val: 14.7485, Test: 16.1999\n",
      "Epoch: 45, Loss: 14.8641, Val: 13.8950, Test: 15.5637\n",
      "Epoch: 46, Loss: 14.7852, Val: 13.7651, Test: 14.5257\n",
      "Epoch: 47, Loss: 14.0184, Val: 13.4507, Test: 14.3542\n",
      "Epoch: 48, Loss: 14.5770, Val: 13.8747, Test: 14.6634\n",
      "Epoch: 49, Loss: 13.9324, Val: 13.4506, Test: 14.8085\n",
      "Epoch: 50, Loss: 13.9867, Val: 14.1085, Test: 14.3412\n",
      "Epoch: 51, Loss: 13.6136, Val: 13.5958, Test: 13.9113\n",
      "Epoch: 52, Loss: 13.7192, Val: 13.2569, Test: 13.8804\n",
      "Epoch: 53, Loss: 13.4921, Val: 12.3762, Test: 14.0076\n",
      "Epoch: 54, Loss: 13.1725, Val: 13.2321, Test: 14.3057\n",
      "Epoch: 55, Loss: 13.3313, Val: 12.2694, Test: 13.5760\n",
      "Epoch: 56, Loss: 13.1331, Val: 11.7193, Test: 13.2068\n",
      "Epoch: 57, Loss: 12.8397, Val: 12.9821, Test: 13.1800\n",
      "Epoch: 58, Loss: 13.0315, Val: 11.4577, Test: 13.2843\n",
      "Epoch: 59, Loss: 12.8416, Val: 12.1036, Test: 12.9932\n",
      "Epoch: 60, Loss: 12.5097, Val: 12.0169, Test: 13.2561\n",
      "Epoch: 61, Loss: 12.8294, Val: 11.8931, Test: 12.6185\n",
      "Epoch: 62, Loss: 12.7724, Val: 11.1420, Test: 12.8122\n",
      "Epoch: 63, Loss: 12.5085, Val: 11.6802, Test: 12.4337\n",
      "Epoch: 64, Loss: 12.4763, Val: 11.0099, Test: 12.7748\n",
      "Epoch: 65, Loss: 12.1573, Val: 10.9633, Test: 12.2775\n",
      "Epoch: 66, Loss: 12.2498, Val: 11.3804, Test: 12.3546\n",
      "Epoch: 67, Loss: 12.1449, Val: 12.3846, Test: 13.1453\n",
      "Epoch: 68, Loss: 11.6975, Val: 12.0401, Test: 12.6113\n",
      "Epoch: 69, Loss: 12.0540, Val: 11.3414, Test: 12.5147\n",
      "Epoch: 70, Loss: 12.1703, Val: 11.2742, Test: 12.2650\n",
      "Epoch: 71, Loss: 11.7593, Val: 11.2725, Test: 12.1502\n",
      "Epoch: 72, Loss: 11.7237, Val: 11.0695, Test: 12.1924\n",
      "Epoch: 73, Loss: 11.7375, Val: 11.5683, Test: 12.4628\n",
      "Epoch: 74, Loss: 11.6792, Val: 11.3354, Test: 12.9120\n",
      "Epoch: 75, Loss: 11.7667, Val: 10.9729, Test: 12.0339\n",
      "Epoch: 76, Loss: 11.7647, Val: 11.6961, Test: 12.1831\n",
      "Epoch: 77, Loss: 11.4265, Val: 10.5104, Test: 11.8493\n",
      "Epoch: 78, Loss: 11.1231, Val: 10.9472, Test: 11.9595\n",
      "Epoch: 79, Loss: 11.1413, Val: 11.1394, Test: 11.9951\n",
      "Epoch: 80, Loss: 11.1376, Val: 10.8862, Test: 11.8826\n",
      "Epoch: 81, Loss: 11.2050, Val: 10.2379, Test: 11.8995\n",
      "Epoch: 82, Loss: 11.0766, Val: 10.6378, Test: 11.8325\n",
      "Epoch: 83, Loss: 10.8883, Val: 10.3920, Test: 11.6401\n",
      "Epoch: 84, Loss: 10.8830, Val: 10.3745, Test: 11.9680\n",
      "Epoch: 85, Loss: 10.8134, Val: 11.2040, Test: 11.7186\n",
      "Epoch: 86, Loss: 10.8764, Val: 10.6958, Test: 11.8604\n",
      "Epoch: 87, Loss: 10.8861, Val: 10.5133, Test: 11.6837\n",
      "Epoch: 88, Loss: 10.6907, Val: 10.0169, Test: 11.5985\n",
      "Epoch: 89, Loss: 10.7087, Val: 10.4531, Test: 11.8093\n",
      "Epoch: 90, Loss: 11.0583, Val: 10.9754, Test: 11.5703\n",
      "Epoch: 91, Loss: 10.7754, Val: 9.7920, Test: 11.6624\n",
      "Epoch: 92, Loss: 10.7829, Val: 10.1807, Test: 11.6899\n",
      "Epoch: 93, Loss: 10.6087, Val: 10.6301, Test: 11.5212\n",
      "Epoch: 94, Loss: 10.7608, Val: 10.6465, Test: 11.6282\n",
      "Epoch: 95, Loss: 10.6524, Val: 11.0104, Test: 11.6460\n",
      "Epoch: 96, Loss: 10.6807, Val: 10.6003, Test: 11.5960\n",
      "Epoch: 97, Loss: 10.6360, Val: 10.1100, Test: 11.6496\n",
      "Epoch: 98, Loss: 10.7670, Val: 10.3757, Test: 11.7567\n",
      "Epoch: 99, Loss: 10.7169, Val: 10.3629, Test: 11.3612\n",
      "Epoch: 100, Loss: 10.6541, Val: 9.8871, Test: 11.6342\n",
      "\n",
      "Model's performance on the test set\n",
      "===================================\n",
      "MAE=11.66239886682469\n",
      "Epoch=91\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphTransformerNet(node_dim_in=NODE_DIM,\n",
    "                            edge_dim_in=EDGE_DIM,\n",
    "                            pe_in_dim=PE_DIM,\n",
    "                            num_gt_layers=4, \n",
    "                            hidden_dim=128,\n",
    "                            num_heads=8,\n",
    "                            norm='bn',\n",
    "                            gt_aggregators=['sum'],\n",
    "                            aggregators=['sum'],\n",
    "                            dropout=0.1,\n",
    "                            act='relu').to(device)\n",
    "\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    model = torch_geometric.compile(model) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "print(model)\n",
    "print(f\"Number of params: {model.num_parameters()//1000} k\")\n",
    "\n",
    "best_epoch = 0\n",
    "best_validation_loss = np.inf\n",
    "test_set_mae = np.inf\n",
    "for epoch in range(1, 101):\n",
    "    tr_loss = train(epoch, loss_func=train_loss)\n",
    "    va_loss = test(val_loader, loss_func=test_loss)\n",
    "    te_loss = test(test_loader, loss_func=test_loss)\n",
    "    scheduler.step(va_loss)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {tr_loss:.4f}, Val: {va_loss:.4f}, '\n",
    "          f'Test: {te_loss:.4f}')\n",
    "    if va_loss < best_validation_loss:\n",
    "        best_epoch = epoch\n",
    "        best_validation_loss = va_loss\n",
    "        test_set_mae = te_loss\n",
    "        \n",
    "print(\"\\nModel's performance on the test set\\n\"\n",
    "        \"===================================\\n\"\n",
    "        f'MAE={test_set_mae}\\n'\n",
    "        f'Epoch={best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slighlty optimized Graph Transformer architecture\n",
    "\n",
    "1. `gelu` activation is used instead of `relu`\n",
    "2. Multiaggregator used for global pooling\n",
    "3. Multiaggregator used for message passing\n",
    "\n",
    "Number of params 873k instead of 701k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformerNet(\n",
      "  (node_emb): Linear(in_features=76, out_features=128, bias=True)\n",
      "  (edge_emb): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (pe_emb): Linear(in_features=6, out_features=128, bias=True)\n",
      "  (gt_layers): ModuleList(\n",
      "    (0): GTConv(128, 128, heads=8, aggrs: sum,mean,max,std)\n",
      "    (1): GTConv(128, 128, heads=8, aggrs: sum,mean,max,std)\n",
      "    (2): GTConv(128, 128, heads=8, aggrs: sum,mean,max,std)\n",
      "    (3): GTConv(128, 128, heads=8, aggrs: sum,mean,max,std)\n",
      "    (4): GTConv(128, 128, heads=8, aggrs: sum,mean,max,std)\n",
      "  )\n",
      "  (global_pool): MultiAggregation([\n",
      "    SumAggregation(),\n",
      "    MeanAggregation(),\n",
      "    MaxAggregation(),\n",
      "    StdAggregation(),\n",
      "  ], mode=cat)\n",
      "  (mu_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (log_var_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of params: 1220 k\n",
      "Epoch: 01, Loss: 2.1228, Val: 1.0761, Test: 1.1913\n",
      "Epoch: 02, Loss: 0.8862, Val: 1.0942, Test: 1.1208\n",
      "Epoch: 03, Loss: 0.6313, Val: 0.5283, Test: 0.6279\n",
      "Epoch: 04, Loss: 0.6005, Val: 0.5209, Test: 0.7022\n",
      "Epoch: 05, Loss: 0.6332, Val: 0.5882, Test: 0.6582\n",
      "Epoch: 06, Loss: 0.5715, Val: 0.7701, Test: 0.8224\n",
      "Epoch: 07, Loss: 0.5059, Val: 0.9715, Test: 1.0561\n",
      "Epoch: 08, Loss: 0.5012, Val: 0.9235, Test: 1.0338\n",
      "Epoch: 09, Loss: 0.4182, Val: 0.7712, Test: 0.9083\n",
      "Epoch: 10, Loss: 0.5170, Val: 0.5228, Test: 0.6073\n",
      "Epoch: 11, Loss: 0.5150, Val: 0.7075, Test: 0.8186\n",
      "Epoch: 12, Loss: 0.5674, Val: 0.6121, Test: 0.6608\n",
      "Epoch: 13, Loss: 0.5837, Val: 0.7229, Test: 0.7750\n",
      "Epoch: 14, Loss: 0.5792, Val: 0.6421, Test: 0.6972\n",
      "Epoch: 15, Loss: 0.5562, Val: 0.5100, Test: 0.5786\n",
      "Epoch: 16, Loss: 0.4786, Val: 0.5039, Test: 0.5079\n",
      "Epoch: 17, Loss: 0.4611, Val: 0.4730, Test: 0.5258\n",
      "Epoch: 18, Loss: 0.4531, Val: 0.6354, Test: 0.7274\n",
      "Epoch: 19, Loss: 0.4573, Val: 0.6374, Test: 0.7435\n",
      "Epoch: 20, Loss: 0.4672, Val: 0.5282, Test: 0.5507\n",
      "Epoch: 21, Loss: 0.4378, Val: 0.5323, Test: 0.5406\n",
      "Epoch: 22, Loss: 0.3946, Val: 0.3811, Test: 0.4398\n",
      "Epoch: 23, Loss: 0.4007, Val: 0.4080, Test: 0.4611\n",
      "Epoch: 24, Loss: 0.4261, Val: 0.3952, Test: 0.4257\n",
      "Epoch: 25, Loss: 0.4231, Val: 0.4756, Test: 0.5219\n",
      "Epoch: 26, Loss: 0.3640, Val: 0.4327, Test: 0.4719\n",
      "Epoch: 27, Loss: 0.3686, Val: 0.3723, Test: 0.4558\n",
      "Epoch: 28, Loss: 0.3607, Val: 0.4033, Test: 0.4914\n",
      "Epoch: 29, Loss: 0.3543, Val: 0.5217, Test: 0.5742\n",
      "Epoch: 30, Loss: 0.3501, Val: 0.5105, Test: 0.5667\n",
      "Epoch: 31, Loss: 0.3616, Val: 0.4141, Test: 0.4538\n",
      "Epoch: 32, Loss: 0.4299, Val: 0.4668, Test: 0.4775\n",
      "Epoch: 33, Loss: 0.4247, Val: 0.3970, Test: 0.4044\n",
      "Epoch: 34, Loss: 0.4005, Val: 0.4565, Test: 0.4477\n",
      "Epoch: 35, Loss: 0.3869, Val: 0.3917, Test: 0.5142\n",
      "Epoch: 36, Loss: 0.3370, Val: 0.4209, Test: 0.4702\n",
      "Epoch: 37, Loss: 0.3457, Val: 0.3882, Test: 0.4213\n",
      "Epoch: 38, Loss: 0.3216, Val: 0.3734, Test: 0.4700\n",
      "Epoch: 39, Loss: 0.3113, Val: 0.3511, Test: 0.3807\n",
      "Epoch: 40, Loss: 0.2777, Val: 0.3309, Test: 0.3631\n",
      "Epoch: 41, Loss: 0.2429, Val: 0.3387, Test: 0.3914\n",
      "Epoch: 42, Loss: 0.2213, Val: 0.3265, Test: 0.3750\n",
      "Epoch: 43, Loss: 0.2163, Val: 0.3695, Test: 0.4224\n",
      "Epoch: 44, Loss: 0.2122, Val: 0.3658, Test: 0.4421\n",
      "Epoch: 45, Loss: 0.2193, Val: 0.3261, Test: 0.3791\n",
      "Epoch: 46, Loss: 0.1963, Val: 0.2981, Test: 0.3675\n",
      "Epoch: 47, Loss: 0.1885, Val: 0.3098, Test: 0.3840\n",
      "Epoch: 48, Loss: 0.2031, Val: 0.3390, Test: 0.3843\n",
      "Epoch: 49, Loss: 0.2082, Val: 0.3439, Test: 0.3965\n",
      "Epoch: 50, Loss: 0.2067, Val: 0.3699, Test: 0.4374\n",
      "Epoch: 51, Loss: 0.2314, Val: 0.3258, Test: 0.3657\n",
      "Epoch: 52, Loss: 0.2183, Val: 0.3142, Test: 0.3749\n",
      "Epoch: 53, Loss: 0.2169, Val: 0.3332, Test: 0.3735\n",
      "Epoch: 54, Loss: 0.2142, Val: 0.3459, Test: 0.3639\n",
      "Epoch: 55, Loss: 0.2189, Val: 0.3369, Test: 0.4013\n",
      "Epoch: 56, Loss: 0.2380, Val: 0.3682, Test: 0.4016\n",
      "Epoch: 57, Loss: 0.2168, Val: 0.4144, Test: 0.4732\n",
      "Epoch: 58, Loss: 0.2135, Val: 0.3349, Test: 0.3788\n",
      "Epoch: 59, Loss: 0.1859, Val: 0.3260, Test: 0.3712\n",
      "Epoch: 60, Loss: 0.1610, Val: 0.3403, Test: 0.3934\n",
      "Epoch: 61, Loss: 0.1638, Val: 0.3193, Test: 0.3632\n",
      "Epoch: 62, Loss: 0.1538, Val: 0.3179, Test: 0.3774\n",
      "Epoch: 63, Loss: 0.1666, Val: 0.3234, Test: 0.3874\n",
      "Epoch: 64, Loss: 0.1603, Val: 0.3270, Test: 0.3899\n",
      "Epoch: 65, Loss: 0.1533, Val: 0.3392, Test: 0.3896\n",
      "Epoch: 66, Loss: 0.1416, Val: 0.3213, Test: 0.3730\n",
      "Epoch: 67, Loss: 0.1510, Val: 0.3129, Test: 0.3648\n",
      "Epoch: 68, Loss: 0.1466, Val: 0.3345, Test: 0.3796\n",
      "Epoch: 69, Loss: 0.1515, Val: 0.3289, Test: 0.3712\n",
      "Epoch: 70, Loss: 0.1419, Val: 0.3294, Test: 0.3777\n",
      "Epoch: 71, Loss: 0.1370, Val: 0.3269, Test: 0.3858\n",
      "Epoch: 72, Loss: 0.1288, Val: 0.3083, Test: 0.3573\n",
      "Epoch: 73, Loss: 0.1381, Val: 0.3233, Test: 0.3733\n",
      "Epoch: 74, Loss: 0.1348, Val: 0.3323, Test: 0.3750\n",
      "Epoch: 75, Loss: 0.1368, Val: 0.3175, Test: 0.3664\n",
      "Epoch: 76, Loss: 0.1250, Val: 0.3183, Test: 0.3687\n",
      "Epoch: 77, Loss: 0.1342, Val: 0.3276, Test: 0.3714\n",
      "Epoch: 78, Loss: 0.1325, Val: 0.3230, Test: 0.3712\n",
      "Epoch: 79, Loss: 0.1297, Val: 0.3161, Test: 0.3756\n",
      "Epoch: 80, Loss: 0.1364, Val: 0.3195, Test: 0.3777\n",
      "Epoch: 81, Loss: 0.1273, Val: 0.3248, Test: 0.3858\n",
      "Epoch: 82, Loss: 0.1249, Val: 0.3228, Test: 0.3857\n",
      "Epoch: 83, Loss: 0.1246, Val: 0.3163, Test: 0.3701\n",
      "Epoch: 84, Loss: 0.1215, Val: 0.3227, Test: 0.3840\n",
      "Epoch: 85, Loss: 0.1255, Val: 0.3194, Test: 0.3806\n",
      "Epoch: 86, Loss: 0.1232, Val: 0.3233, Test: 0.3633\n",
      "Epoch: 87, Loss: 0.1269, Val: 0.3229, Test: 0.3647\n",
      "Epoch: 88, Loss: 0.1234, Val: 0.3287, Test: 0.3743\n",
      "Epoch: 89, Loss: 0.1226, Val: 0.3183, Test: 0.3757\n",
      "Epoch: 90, Loss: 0.1215, Val: 0.3234, Test: 0.3658\n",
      "Epoch: 91, Loss: 0.1196, Val: 0.3184, Test: 0.3634\n",
      "Epoch: 92, Loss: 0.1200, Val: 0.3225, Test: 0.3752\n",
      "Epoch: 93, Loss: 0.1165, Val: 0.3247, Test: 0.3691\n",
      "Epoch: 94, Loss: 0.1179, Val: 0.3225, Test: 0.3648\n",
      "Epoch: 95, Loss: 0.1184, Val: 0.3203, Test: 0.3638\n",
      "Epoch: 96, Loss: 0.1142, Val: 0.3234, Test: 0.3759\n",
      "Epoch: 97, Loss: 0.1207, Val: 0.3222, Test: 0.3691\n",
      "Epoch: 98, Loss: 0.1199, Val: 0.3154, Test: 0.3723\n",
      "Epoch: 99, Loss: 0.1168, Val: 0.3215, Test: 0.3739\n",
      "Epoch: 100, Loss: 0.1192, Val: 0.3209, Test: 0.3681\n",
      "Epoch: 101, Loss: 0.1187, Val: 0.3192, Test: 0.3761\n",
      "Epoch: 102, Loss: 0.1183, Val: 0.3151, Test: 0.3749\n",
      "Epoch: 103, Loss: 0.1157, Val: 0.3202, Test: 0.3736\n",
      "Epoch: 104, Loss: 0.1168, Val: 0.3132, Test: 0.3768\n",
      "Epoch: 105, Loss: 0.1124, Val: 0.3201, Test: 0.3714\n",
      "Epoch: 106, Loss: 0.1132, Val: 0.3097, Test: 0.3644\n",
      "Epoch: 107, Loss: 0.1135, Val: 0.3176, Test: 0.3786\n",
      "Epoch: 108, Loss: 0.1162, Val: 0.3190, Test: 0.3729\n",
      "Epoch: 109, Loss: 0.1190, Val: 0.3172, Test: 0.3684\n",
      "Epoch: 110, Loss: 0.1185, Val: 0.3172, Test: 0.3698\n",
      "Epoch: 111, Loss: 0.1162, Val: 0.3181, Test: 0.3668\n",
      "Epoch: 112, Loss: 0.1197, Val: 0.3269, Test: 0.3774\n",
      "Epoch: 113, Loss: 0.1134, Val: 0.3235, Test: 0.3656\n",
      "Epoch: 114, Loss: 0.1163, Val: 0.3189, Test: 0.3764\n",
      "Epoch: 115, Loss: 0.1149, Val: 0.3237, Test: 0.3746\n",
      "Epoch: 116, Loss: 0.1166, Val: 0.3209, Test: 0.3772\n",
      "Epoch: 117, Loss: 0.1109, Val: 0.3234, Test: 0.3755\n",
      "Epoch: 118, Loss: 0.1157, Val: 0.3132, Test: 0.3726\n",
      "Epoch: 119, Loss: 0.1112, Val: 0.3098, Test: 0.3735\n",
      "Epoch: 120, Loss: 0.1087, Val: 0.3152, Test: 0.3765\n",
      "Epoch: 121, Loss: 0.1155, Val: 0.3199, Test: 0.3694\n",
      "Epoch: 122, Loss: 0.1134, Val: 0.3212, Test: 0.3746\n",
      "Epoch: 123, Loss: 0.1121, Val: 0.3172, Test: 0.3675\n",
      "Epoch: 124, Loss: 0.1168, Val: 0.3130, Test: 0.3693\n",
      "Epoch: 125, Loss: 0.1117, Val: 0.3206, Test: 0.3665\n",
      "Epoch: 126, Loss: 0.1093, Val: 0.3186, Test: 0.3762\n",
      "Epoch: 127, Loss: 0.1162, Val: 0.3216, Test: 0.3694\n",
      "Epoch: 128, Loss: 0.1121, Val: 0.3144, Test: 0.3726\n",
      "Epoch: 129, Loss: 0.1183, Val: 0.3231, Test: 0.3673\n",
      "Epoch: 130, Loss: 0.1196, Val: 0.3183, Test: 0.3715\n",
      "Epoch: 131, Loss: 0.1119, Val: 0.3220, Test: 0.3747\n",
      "Epoch: 132, Loss: 0.1122, Val: 0.3129, Test: 0.3697\n",
      "Epoch: 133, Loss: 0.1158, Val: 0.3233, Test: 0.3739\n",
      "Epoch: 134, Loss: 0.1159, Val: 0.3205, Test: 0.3671\n",
      "Epoch: 135, Loss: 0.1087, Val: 0.3187, Test: 0.3738\n",
      "Epoch: 136, Loss: 0.1162, Val: 0.3133, Test: 0.3750\n",
      "Epoch: 137, Loss: 0.1119, Val: 0.3248, Test: 0.3757\n",
      "Epoch: 138, Loss: 0.1117, Val: 0.3210, Test: 0.3748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 139, Loss: 0.1136, Val: 0.3134, Test: 0.3799\n",
      "Epoch: 140, Loss: 0.1135, Val: 0.3201, Test: 0.3676\n",
      "Epoch: 141, Loss: 0.1145, Val: 0.3205, Test: 0.3720\n",
      "Epoch: 142, Loss: 0.1153, Val: 0.3182, Test: 0.3802\n",
      "Epoch: 143, Loss: 0.1129, Val: 0.3245, Test: 0.3645\n",
      "Epoch: 144, Loss: 0.1123, Val: 0.3168, Test: 0.3659\n",
      "Epoch: 145, Loss: 0.1122, Val: 0.3196, Test: 0.3636\n",
      "Epoch: 146, Loss: 0.1120, Val: 0.3121, Test: 0.3663\n",
      "Epoch: 147, Loss: 0.1152, Val: 0.3195, Test: 0.3708\n",
      "Epoch: 148, Loss: 0.1105, Val: 0.3182, Test: 0.3764\n",
      "Epoch: 149, Loss: 0.1128, Val: 0.3229, Test: 0.3724\n",
      "Epoch: 150, Loss: 0.1132, Val: 0.3204, Test: 0.3762\n",
      "Epoch: 151, Loss: 0.1105, Val: 0.3154, Test: 0.3656\n",
      "Epoch: 152, Loss: 0.1109, Val: 0.3162, Test: 0.3698\n",
      "Epoch: 153, Loss: 0.1150, Val: 0.3184, Test: 0.3752\n",
      "Epoch: 154, Loss: 0.1134, Val: 0.3179, Test: 0.3714\n",
      "Epoch: 155, Loss: 0.1120, Val: 0.3223, Test: 0.3747\n",
      "Epoch: 156, Loss: 0.1110, Val: 0.3114, Test: 0.3659\n",
      "Epoch: 157, Loss: 0.1162, Val: 0.3206, Test: 0.3774\n",
      "Epoch: 158, Loss: 0.1105, Val: 0.3066, Test: 0.3661\n",
      "Epoch: 159, Loss: 0.1105, Val: 0.3134, Test: 0.3744\n",
      "Epoch: 160, Loss: 0.1159, Val: 0.3177, Test: 0.3741\n",
      "Epoch: 161, Loss: 0.1100, Val: 0.3223, Test: 0.3777\n",
      "Epoch: 162, Loss: 0.1153, Val: 0.3181, Test: 0.3737\n",
      "Epoch: 163, Loss: 0.1124, Val: 0.3184, Test: 0.3640\n",
      "Epoch: 164, Loss: 0.1121, Val: 0.3147, Test: 0.3681\n",
      "Epoch: 165, Loss: 0.1041, Val: 0.3180, Test: 0.3818\n",
      "Epoch: 166, Loss: 0.1104, Val: 0.3208, Test: 0.3717\n",
      "Epoch: 167, Loss: 0.1115, Val: 0.3221, Test: 0.3753\n",
      "Epoch: 168, Loss: 0.1122, Val: 0.3178, Test: 0.3739\n",
      "Epoch: 169, Loss: 0.1097, Val: 0.3178, Test: 0.3683\n",
      "Epoch: 170, Loss: 0.1116, Val: 0.3197, Test: 0.3744\n",
      "Epoch: 171, Loss: 0.1123, Val: 0.3164, Test: 0.3661\n",
      "Epoch: 172, Loss: 0.1173, Val: 0.3226, Test: 0.3688\n",
      "Epoch: 173, Loss: 0.1074, Val: 0.3213, Test: 0.3669\n",
      "Epoch: 174, Loss: 0.1082, Val: 0.3246, Test: 0.3755\n",
      "Epoch: 175, Loss: 0.1107, Val: 0.3244, Test: 0.3705\n",
      "Epoch: 176, Loss: 0.1128, Val: 0.3237, Test: 0.3648\n",
      "Epoch: 177, Loss: 0.1111, Val: 0.3112, Test: 0.3633\n",
      "Epoch: 178, Loss: 0.1078, Val: 0.3148, Test: 0.3782\n",
      "Epoch: 179, Loss: 0.1144, Val: 0.3170, Test: 0.3751\n",
      "Epoch: 180, Loss: 0.1120, Val: 0.3161, Test: 0.3760\n",
      "Epoch: 181, Loss: 0.1114, Val: 0.3108, Test: 0.3771\n",
      "Epoch: 182, Loss: 0.1111, Val: 0.3178, Test: 0.3747\n",
      "Epoch: 183, Loss: 0.1092, Val: 0.3204, Test: 0.3713\n",
      "Epoch: 184, Loss: 0.1070, Val: 0.3148, Test: 0.3691\n",
      "Epoch: 185, Loss: 0.1126, Val: 0.3088, Test: 0.3658\n",
      "Epoch: 186, Loss: 0.1127, Val: 0.3150, Test: 0.3643\n",
      "Epoch: 187, Loss: 0.1091, Val: 0.3112, Test: 0.3675\n",
      "Epoch: 188, Loss: 0.1094, Val: 0.3204, Test: 0.3732\n",
      "Epoch: 189, Loss: 0.1069, Val: 0.3190, Test: 0.3629\n",
      "Epoch: 190, Loss: 0.1117, Val: 0.3248, Test: 0.3769\n",
      "Epoch: 191, Loss: 0.1111, Val: 0.3129, Test: 0.3731\n",
      "Epoch: 192, Loss: 0.1065, Val: 0.3190, Test: 0.3756\n",
      "Epoch: 193, Loss: 0.1071, Val: 0.3189, Test: 0.3711\n",
      "Epoch: 194, Loss: 0.1041, Val: 0.3206, Test: 0.3773\n",
      "Epoch: 195, Loss: 0.1112, Val: 0.3235, Test: 0.3722\n",
      "Epoch: 196, Loss: 0.1086, Val: 0.3183, Test: 0.3691\n",
      "Epoch: 197, Loss: 0.1105, Val: 0.3208, Test: 0.3755\n",
      "Epoch: 198, Loss: 0.1117, Val: 0.3216, Test: 0.3704\n",
      "Epoch: 199, Loss: 0.1098, Val: 0.3211, Test: 0.3612\n",
      "Epoch: 200, Loss: 0.1077, Val: 0.3231, Test: 0.3758\n",
      "\n",
      "Model's performance on the test set\n",
      "===================================\n",
      "MAE=0.3675470247373476\n",
      "Epoch=46\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphTransformerNet(node_dim_in=NODE_DIM,\n",
    "                            edge_dim_in=EDGE_DIM,\n",
    "                            pe_in_dim=PE_DIM,\n",
    "                            num_gt_layers=5, \n",
    "                            hidden_dim=128,\n",
    "                            num_heads=8,\n",
    "                            norm='ln',\n",
    "                            gt_aggregators=['sum', 'mean', 'max', 'std'],\n",
    "                            aggregators=['sum','mean','max', 'std'],\n",
    "                            dropout=0.1,\n",
    "                            act='gelu').to(device)\n",
    "\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    model = torch_geometric.compile(model) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "print(model)\n",
    "print(f\"Number of params: {model.num_parameters()//1000} k\")\n",
    "\n",
    "\n",
    "best_epoch = 0\n",
    "best_validation_loss = np.inf\n",
    "test_set_mae = np.inf\n",
    "for epoch in range(1, 201):\n",
    "    tr_loss = train(epoch, loss_func=train_loss)\n",
    "    va_loss = test(val_loader, loss_func=test_loss)\n",
    "    te_loss = test(test_loader, loss_func=test_loss)\n",
    "    scheduler.step(va_loss)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {tr_loss:.4f}, Val: {va_loss:.4f}, '\n",
    "          f'Test: {te_loss:.4f}')\n",
    "    if va_loss < best_validation_loss:\n",
    "        best_epoch = epoch\n",
    "        best_validation_loss = va_loss\n",
    "        test_set_mae = te_loss\n",
    "        \n",
    "print(\"\\nModel's performance on the test set\\n\"\n",
    "        \"===================================\\n\"\n",
    "        f'MAE={test_set_mae}\\n'\n",
    "        f'Epoch={best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
