{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawel/anaconda3/envs/gt/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.21.6\n",
      "Rdkit version: 2022.09.5\n",
      "Torch version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "# Standard\n",
    "import logging\n",
    "\n",
    "# Third party\n",
    "import numpy as np\n",
    "import rdkit\n",
    "from rdkit import RDLogger\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# GT-PyG\n",
    "from gt_pyg.data.utils import (\n",
    "    get_tensor_data, \n",
    "    get_node_dim, \n",
    "    get_edge_dim, \n",
    "    get_train_valid_test_data\n",
    ")\n",
    "from gt_pyg.nn.model import GraphTransformerNet\n",
    "\n",
    "# Turn off majority of RDKit warnings\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "\n",
    "# Set a random seed for a reproducibility purposes\n",
    "torch.manual_seed(192837465)\n",
    "\n",
    "# Setup the logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Log the used versions of RDkit and torch\n",
    "print(f'Numpy version: {np.__version__}')\n",
    "print(f'Rdkit version: {rdkit.__version__}')\n",
    "print(f'Torch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the ADME@TDC data\n",
    "\n",
    "**Note**: To use the code below, make sure that the chosen endpoint is a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available endpoints:\n",
      "\n",
      "1. caco2_wang\n",
      "2. hia_hou\n",
      "3. pgp_broccatelli\n",
      "4. bioavailability_ma\n",
      "5. lipophilicity_astrazeneca\n",
      "6. solubility_aqsoldb\n",
      "7. bbb_martins\n",
      "8. ppbr_az\n",
      "9. vdss_lombardo\n",
      "10. cyp2d6_veith\n",
      "11. cyp3a4_veith\n",
      "12. cyp2c9_veith\n",
      "13. cyp2d6_substrate_carbonmangels\n",
      "14. cyp3a4_substrate_carbonmangels\n",
      "15. cyp2c9_substrate_carbonmangels\n",
      "16. half_life_obach\n",
      "17. clearance_microsome_az\n",
      "18. clearance_hepatocyte_az\n",
      "19. herg\n",
      "20. ames\n",
      "21. dili\n",
      "22. ld50_zhu\n"
     ]
    }
   ],
   "source": [
    "from tdc import utils\n",
    "names = utils.retrieve_benchmark_names('ADMET_Group')\n",
    "output = \"\\n\".join([f\"{index}. {name}\" for index, name in enumerate(names, start=1)])\n",
    "print(\"Available endpoints:\\n\\n\" + output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression endpoints with MAE metric:\n",
    "1. caco2_wang (Best: 0.285 ± 0.005)\n",
    "2. lipophilicity_astrazeneca (Best: 0.535 ± 0.012)\n",
    "3. solubility_aqsoldb (Best: 0.776 ± 0.008)\n",
    "4. ppbr_az (Best: 9.185 ± 0.000)\n",
    "5. ld50_zhu (Best: 0.588 ± 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 2940\n",
      "Number of validation examples: 420\n",
      "Number of test examples: 840\n"
     ]
    }
   ],
   "source": [
    "PE_DIM = 6\n",
    "(tr, va, te) = get_train_valid_test_data('lipophilicity_astrazeneca', min_num_atoms=0)\n",
    "tr_dataset = get_tensor_data(tr.Drug.to_list(), tr.Y.to_list(), pe_dim=PE_DIM)\n",
    "va_dataset = get_tensor_data(va.Drug.to_list(), va.Y.to_list(), pe_dim=PE_DIM)\n",
    "te_dataset = get_tensor_data(te.Drug.to_list(), te.Y.to_list(), pe_dim=PE_DIM)\n",
    "NODE_DIM = get_node_dim()\n",
    "EDGE_DIM = get_edge_dim()\n",
    "\n",
    "print(f'Number of training examples: {len(tr_dataset)}')\n",
    "print(f'Number of validation examples: {len(va_dataset)}')\n",
    "print(f'Number of test examples: {len(te_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(tr_dataset, batch_size=64)\n",
    "val_loader = DataLoader(va_dataset, batch_size=64)\n",
    "test_loader = DataLoader(te_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and eval the GT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loss_func):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # randomly flip sign of eigenvectors\n",
    "        batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM)).float() - 1.0)\n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch, zero_var=False)\n",
    "        loss = loss_func(out.squeeze(), data.y)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, loss_func):\n",
    "    model.eval()\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        # randomly flip sign of eigenvectors\n",
    "        batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM)).float() - 1.0)\n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch)\n",
    "        total_error += loss_func(out.squeeze(), data.y).item()\n",
    "    return total_error / len(loader.dataset)\n",
    "\n",
    "train_loss = nn.L1Loss(reduction='mean')\n",
    "test_loss = nn.L1Loss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Graph Transformer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformerNet(\n",
      "  (node_emb): Linear(in_features=76, out_features=128, bias=True)\n",
      "  (edge_emb): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (pe_emb): Linear(in_features=6, out_features=128, bias=True)\n",
      "  (gt_layers): ModuleList(\n",
      "    (0): GTConv(128, 128, heads=8, aggrss: aggrs)\n",
      "    (1): GTConv(128, 128, heads=8, aggrss: aggrs)\n",
      "    (2): GTConv(128, 128, heads=8, aggrss: aggrs)\n",
      "    (3): GTConv(128, 128, heads=8, aggrss: aggrs)\n",
      "  )\n",
      "  (global_pool): MultiAggregation([\n",
      "    SumAggregation(),\n",
      "  ], mode=cat)\n",
      "  (mu_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (log_var_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of params: 710 k\n",
      "Epoch: 01, Loss: 254.1053, Val: 79.3010, Test: 78.4710\n",
      "Epoch: 02, Loss: 76.0497, Val: 73.4988, Test: 72.6591\n",
      "Epoch: 03, Loss: 64.1314, Val: 61.7732, Test: 57.9510\n",
      "Epoch: 04, Loss: 51.5420, Val: 48.0345, Test: 44.6130\n",
      "Epoch: 05, Loss: 44.8844, Val: 48.0324, Test: 45.5972\n",
      "Epoch: 06, Loss: 44.6917, Val: 46.8293, Test: 44.7837\n",
      "Epoch: 07, Loss: 42.7092, Val: 44.4894, Test: 42.4372\n",
      "Epoch: 08, Loss: 41.0560, Val: 42.3858, Test: 40.1532\n",
      "Epoch: 09, Loss: 38.8777, Val: 40.1157, Test: 38.2254\n",
      "Epoch: 10, Loss: 37.3598, Val: 37.7406, Test: 36.2491\n",
      "Epoch: 11, Loss: 34.8540, Val: 35.7157, Test: 33.9792\n",
      "Epoch: 12, Loss: 32.8258, Val: 33.5422, Test: 33.0523\n",
      "Epoch: 13, Loss: 31.9717, Val: 31.9795, Test: 31.9779\n",
      "Epoch: 14, Loss: 30.5177, Val: 30.7269, Test: 30.8389\n",
      "Epoch: 15, Loss: 29.8629, Val: 29.7777, Test: 29.9607\n",
      "Epoch: 16, Loss: 28.6261, Val: 28.6905, Test: 28.9838\n",
      "Epoch: 17, Loss: 26.7790, Val: 27.2219, Test: 27.6159\n",
      "Epoch: 18, Loss: 26.4050, Val: 26.0542, Test: 26.7689\n",
      "Epoch: 19, Loss: 25.6892, Val: 24.8175, Test: 25.9487\n",
      "Epoch: 20, Loss: 25.3781, Val: 23.4117, Test: 25.0469\n",
      "Epoch: 21, Loss: 24.1819, Val: 22.2140, Test: 24.0674\n",
      "Epoch: 22, Loss: 23.5998, Val: 21.2239, Test: 23.2185\n",
      "Epoch: 23, Loss: 22.8753, Val: 21.0641, Test: 22.4323\n",
      "Epoch: 24, Loss: 23.0506, Val: 20.6185, Test: 21.7326\n",
      "Epoch: 25, Loss: 22.2194, Val: 20.0244, Test: 21.0596\n",
      "Epoch: 26, Loss: 20.9897, Val: 19.0275, Test: 21.0176\n",
      "Epoch: 27, Loss: 20.9660, Val: 18.0247, Test: 20.0478\n",
      "Epoch: 28, Loss: 20.6049, Val: 17.5889, Test: 19.2263\n",
      "Epoch: 29, Loss: 20.5756, Val: 17.1742, Test: 18.9936\n",
      "Epoch: 30, Loss: 19.4789, Val: 17.1501, Test: 18.4565\n",
      "Epoch: 31, Loss: 19.0449, Val: 16.8696, Test: 18.1420\n",
      "Epoch: 32, Loss: 18.4710, Val: 16.7432, Test: 17.7979\n",
      "Epoch: 33, Loss: 17.7872, Val: 15.1623, Test: 17.5954\n",
      "Epoch: 34, Loss: 18.0856, Val: 15.7846, Test: 17.1644\n",
      "Epoch: 35, Loss: 17.3754, Val: 15.6339, Test: 17.0505\n",
      "Epoch: 36, Loss: 17.1157, Val: 15.6462, Test: 16.8550\n",
      "Epoch: 37, Loss: 16.7787, Val: 15.2373, Test: 16.5674\n",
      "Epoch: 38, Loss: 16.5217, Val: 14.7471, Test: 17.0250\n",
      "Epoch: 39, Loss: 15.9122, Val: 14.8097, Test: 16.4509\n",
      "Epoch: 40, Loss: 15.6419, Val: 14.7453, Test: 16.2121\n",
      "Epoch: 41, Loss: 15.9401, Val: 14.6130, Test: 15.8768\n",
      "Epoch: 42, Loss: 14.9600, Val: 14.9719, Test: 15.4060\n",
      "Epoch: 43, Loss: 15.3392, Val: 13.6407, Test: 15.5631\n",
      "Epoch: 44, Loss: 15.3057, Val: 14.7485, Test: 16.1999\n",
      "Epoch: 45, Loss: 14.8641, Val: 13.8950, Test: 15.5637\n",
      "Epoch: 46, Loss: 14.7852, Val: 13.7651, Test: 14.5257\n",
      "Epoch: 47, Loss: 14.0184, Val: 13.4507, Test: 14.3542\n",
      "Epoch: 48, Loss: 14.5770, Val: 13.8747, Test: 14.6634\n",
      "Epoch: 49, Loss: 13.9324, Val: 13.4506, Test: 14.8085\n",
      "Epoch: 50, Loss: 13.9867, Val: 14.1085, Test: 14.3412\n",
      "Epoch: 51, Loss: 13.6136, Val: 13.5958, Test: 13.9113\n",
      "Epoch: 52, Loss: 13.7192, Val: 13.2569, Test: 13.8804\n",
      "Epoch: 53, Loss: 13.4921, Val: 12.3762, Test: 14.0076\n",
      "Epoch: 54, Loss: 13.1725, Val: 13.2321, Test: 14.3057\n",
      "Epoch: 55, Loss: 13.3313, Val: 12.2694, Test: 13.5760\n",
      "Epoch: 56, Loss: 13.1331, Val: 11.7193, Test: 13.2068\n",
      "Epoch: 57, Loss: 12.8397, Val: 12.9821, Test: 13.1800\n",
      "Epoch: 58, Loss: 13.0315, Val: 11.4577, Test: 13.2843\n",
      "Epoch: 59, Loss: 12.8416, Val: 12.1036, Test: 12.9932\n",
      "Epoch: 60, Loss: 12.5097, Val: 12.0169, Test: 13.2561\n",
      "Epoch: 61, Loss: 12.8294, Val: 11.8931, Test: 12.6185\n",
      "Epoch: 62, Loss: 12.7724, Val: 11.1420, Test: 12.8122\n",
      "Epoch: 63, Loss: 12.5085, Val: 11.6802, Test: 12.4337\n",
      "Epoch: 64, Loss: 12.4763, Val: 11.0099, Test: 12.7748\n",
      "Epoch: 65, Loss: 12.1573, Val: 10.9633, Test: 12.2775\n",
      "Epoch: 66, Loss: 12.2498, Val: 11.3804, Test: 12.3546\n",
      "Epoch: 67, Loss: 12.1449, Val: 12.3846, Test: 13.1453\n",
      "Epoch: 68, Loss: 11.6975, Val: 12.0401, Test: 12.6113\n",
      "Epoch: 69, Loss: 12.0540, Val: 11.3414, Test: 12.5147\n",
      "Epoch: 70, Loss: 12.1703, Val: 11.2742, Test: 12.2650\n",
      "Epoch: 71, Loss: 11.7593, Val: 11.2725, Test: 12.1502\n",
      "Epoch: 72, Loss: 11.7237, Val: 11.0695, Test: 12.1924\n",
      "Epoch: 73, Loss: 11.7375, Val: 11.5683, Test: 12.4628\n",
      "Epoch: 74, Loss: 11.6792, Val: 11.3354, Test: 12.9120\n",
      "Epoch: 75, Loss: 11.7667, Val: 10.9729, Test: 12.0339\n",
      "Epoch: 76, Loss: 11.7647, Val: 11.6961, Test: 12.1831\n",
      "Epoch: 77, Loss: 11.4265, Val: 10.5104, Test: 11.8493\n",
      "Epoch: 78, Loss: 11.1231, Val: 10.9472, Test: 11.9595\n",
      "Epoch: 79, Loss: 11.1413, Val: 11.1394, Test: 11.9951\n",
      "Epoch: 80, Loss: 11.1376, Val: 10.8862, Test: 11.8826\n",
      "Epoch: 81, Loss: 11.2050, Val: 10.2379, Test: 11.8995\n",
      "Epoch: 82, Loss: 11.0766, Val: 10.6378, Test: 11.8325\n",
      "Epoch: 83, Loss: 10.8883, Val: 10.3920, Test: 11.6401\n",
      "Epoch: 84, Loss: 10.8830, Val: 10.3745, Test: 11.9680\n",
      "Epoch: 85, Loss: 10.8134, Val: 11.2040, Test: 11.7186\n",
      "Epoch: 86, Loss: 10.8764, Val: 10.6958, Test: 11.8604\n",
      "Epoch: 87, Loss: 10.8861, Val: 10.5133, Test: 11.6837\n",
      "Epoch: 88, Loss: 10.6907, Val: 10.0169, Test: 11.5985\n",
      "Epoch: 89, Loss: 10.7087, Val: 10.4531, Test: 11.8093\n",
      "Epoch: 90, Loss: 11.0583, Val: 10.9754, Test: 11.5703\n",
      "Epoch: 91, Loss: 10.7754, Val: 9.7920, Test: 11.6624\n",
      "Epoch: 92, Loss: 10.7829, Val: 10.1807, Test: 11.6899\n",
      "Epoch: 93, Loss: 10.6087, Val: 10.6301, Test: 11.5212\n",
      "Epoch: 94, Loss: 10.7608, Val: 10.6465, Test: 11.6282\n",
      "Epoch: 95, Loss: 10.6524, Val: 11.0104, Test: 11.6460\n",
      "Epoch: 96, Loss: 10.6807, Val: 10.6003, Test: 11.5960\n",
      "Epoch: 97, Loss: 10.6360, Val: 10.1100, Test: 11.6496\n",
      "Epoch: 98, Loss: 10.7670, Val: 10.3757, Test: 11.7567\n",
      "Epoch: 99, Loss: 10.7169, Val: 10.3629, Test: 11.3612\n",
      "Epoch: 100, Loss: 10.6541, Val: 9.8871, Test: 11.6342\n",
      "\n",
      "Model's performance on the test set\n",
      "===================================\n",
      "MAE=11.66239886682469\n",
      "Epoch=91\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphTransformerNet(node_dim_in=NODE_DIM,\n",
    "                            edge_dim_in=EDGE_DIM,\n",
    "                            pe_in_dim=PE_DIM,\n",
    "                            num_gt_layers=4, \n",
    "                            hidden_dim=128,\n",
    "                            num_heads=8,\n",
    "                            norm='bn',\n",
    "                            gt_aggregators=['sum'],\n",
    "                            aggregators=['sum'],\n",
    "                            dropout=0.1,\n",
    "                            act='relu').to(device)\n",
    "\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    model = torch_geometric.compile(model) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "print(model)\n",
    "print(f\"Number of params: {model.num_parameters()//1000} k\")\n",
    "\n",
    "best_epoch = 0\n",
    "best_validation_loss = np.inf\n",
    "test_set_mae = np.inf\n",
    "for epoch in range(1, 101):\n",
    "    tr_loss = train(epoch, loss_func=train_loss)\n",
    "    va_loss = test(val_loader, loss_func=test_loss)\n",
    "    te_loss = test(test_loader, loss_func=test_loss)\n",
    "    scheduler.step(va_loss)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {tr_loss:.4f}, Val: {va_loss:.4f}, '\n",
    "          f'Test: {te_loss:.4f}')\n",
    "    if va_loss < best_validation_loss:\n",
    "        best_epoch = epoch\n",
    "        best_validation_loss = va_loss\n",
    "        test_set_mae = te_loss\n",
    "        \n",
    "print(\"\\nModel's performance on the test set\\n\"\n",
    "        \"===================================\\n\"\n",
    "        f'MAE={test_set_mae}\\n'\n",
    "        f'Epoch={best_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slighlty optimized Graph Transformer architecture\n",
    "\n",
    "1. `gelu` activation is used instead of `relu`\n",
    "2. Multiaggregator used for global pooling\n",
    "3. Multiaggregator used for message passing\n",
    "\n",
    "Number of params 873k instead of 701k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformerNet(\n",
      "  (node_emb): Linear(in_features=76, out_features=128, bias=True)\n",
      "  (edge_emb): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (pe_emb): Linear(in_features=6, out_features=128, bias=True)\n",
      "  (gt_layers): ModuleList(\n",
      "    (0): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "    (1): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "    (2): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "    (3): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "  )\n",
      "  (global_pool): MultiAggregation([\n",
      "    SumAggregation(),\n",
      "    MeanAggregation(),\n",
      "    MaxAggregation(),\n",
      "    StdAggregation(),\n",
      "  ], mode=cat)\n",
      "  (mu_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (log_var_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of params: 873 k\n",
      "Epoch: 01, Loss: 1.0608, Val: 1.4705, Test: 1.4241\n",
      "Epoch: 02, Loss: 0.8457, Val: 0.8637, Test: 0.9060\n",
      "Epoch: 03, Loss: 0.7757, Val: 0.9492, Test: 0.9897\n",
      "Epoch: 04, Loss: 0.6943, Val: 0.7363, Test: 0.7893\n",
      "Epoch: 05, Loss: 0.6641, Val: 1.3622, Test: 1.3756\n",
      "Epoch: 06, Loss: 0.6519, Val: 0.7376, Test: 0.7568\n",
      "Epoch: 07, Loss: 0.5999, Val: 0.6104, Test: 0.6823\n",
      "Epoch: 08, Loss: 0.5784, Val: 0.5495, Test: 0.5848\n",
      "Epoch: 09, Loss: 0.5486, Val: 0.6734, Test: 0.7022\n",
      "Epoch: 10, Loss: 0.5570, Val: 0.6580, Test: 0.6798\n",
      "Epoch: 11, Loss: 0.5415, Val: 0.5787, Test: 0.5964\n",
      "Epoch: 12, Loss: 0.5300, Val: 0.6857, Test: 0.6864\n",
      "Epoch: 13, Loss: 0.5192, Val: 0.6945, Test: 0.6825\n",
      "Epoch: 14, Loss: 0.5032, Val: 0.6316, Test: 0.6431\n",
      "Epoch: 15, Loss: 0.5099, Val: 0.8310, Test: 0.8428\n",
      "Epoch: 16, Loss: 0.4982, Val: 0.6386, Test: 0.6594\n",
      "Epoch: 17, Loss: 0.5073, Val: 0.6826, Test: 0.7063\n",
      "Epoch: 18, Loss: 0.5006, Val: 0.5058, Test: 0.5667\n",
      "Epoch: 19, Loss: 0.5123, Val: 0.6351, Test: 0.6340\n",
      "Epoch: 20, Loss: 0.4938, Val: 0.6374, Test: 0.6623\n",
      "Epoch: 21, Loss: 0.4988, Val: 0.5865, Test: 0.6124\n",
      "Epoch: 22, Loss: 0.4868, Val: 0.6330, Test: 0.6464\n",
      "Epoch: 23, Loss: 0.4875, Val: 0.5676, Test: 0.6211\n",
      "Epoch: 24, Loss: 0.4733, Val: 0.5430, Test: 0.5745\n",
      "Epoch: 25, Loss: 0.4804, Val: 0.5011, Test: 0.5472\n",
      "Epoch: 26, Loss: 0.4862, Val: 0.5207, Test: 0.5732\n",
      "Epoch: 27, Loss: 0.4771, Val: 0.5389, Test: 0.5614\n",
      "Epoch: 28, Loss: 0.4776, Val: 0.5984, Test: 0.6297\n",
      "Epoch: 29, Loss: 0.4687, Val: 0.5455, Test: 0.5435\n",
      "Epoch: 30, Loss: 0.4833, Val: 0.5074, Test: 0.5363\n",
      "Epoch: 31, Loss: 0.4810, Val: 0.5621, Test: 0.5945\n",
      "Epoch: 32, Loss: 0.4707, Val: 0.5748, Test: 0.6005\n",
      "Epoch: 33, Loss: 0.4738, Val: 0.5249, Test: 0.5609\n",
      "Epoch: 34, Loss: 0.4602, Val: 0.5287, Test: 0.5582\n",
      "Epoch: 35, Loss: 0.4562, Val: 0.5197, Test: 0.5550\n",
      "Epoch: 36, Loss: 0.4556, Val: 0.4653, Test: 0.5231\n",
      "Epoch: 37, Loss: 0.4653, Val: 0.5706, Test: 0.6080\n",
      "Epoch: 38, Loss: 0.4790, Val: 0.5945, Test: 0.6298\n",
      "Epoch: 39, Loss: 0.4757, Val: 0.5458, Test: 0.5576\n",
      "Epoch: 40, Loss: 0.4934, Val: 0.5566, Test: 0.5758\n",
      "Epoch: 41, Loss: 0.4648, Val: 0.5641, Test: 0.5808\n",
      "Epoch: 42, Loss: 0.4745, Val: 0.5554, Test: 0.5718\n",
      "Epoch: 43, Loss: 0.4673, Val: 0.5916, Test: 0.6053\n",
      "Epoch: 44, Loss: 0.4773, Val: 0.6065, Test: 0.6277\n",
      "Epoch: 45, Loss: 0.4739, Val: 0.5380, Test: 0.5967\n",
      "Epoch: 46, Loss: 0.4624, Val: 0.5577, Test: 0.6012\n",
      "Epoch: 47, Loss: 0.4531, Val: 0.5372, Test: 0.5495\n",
      "Epoch: 48, Loss: 0.4197, Val: 0.4995, Test: 0.5218\n",
      "Epoch: 49, Loss: 0.4064, Val: 0.4369, Test: 0.4771\n",
      "Epoch: 50, Loss: 0.3929, Val: 0.4612, Test: 0.4758\n",
      "Epoch: 51, Loss: 0.3828, Val: 0.4411, Test: 0.4770\n",
      "Epoch: 52, Loss: 0.3749, Val: 0.4558, Test: 0.4864\n",
      "Epoch: 53, Loss: 0.3796, Val: 0.4465, Test: 0.4775\n",
      "Epoch: 54, Loss: 0.3693, Val: 0.4266, Test: 0.4626\n",
      "Epoch: 55, Loss: 0.3579, Val: 0.4442, Test: 0.4774\n",
      "Epoch: 56, Loss: 0.3641, Val: 0.4425, Test: 0.4719\n",
      "Epoch: 57, Loss: 0.3515, Val: 0.4371, Test: 0.4520\n",
      "Epoch: 58, Loss: 0.3499, Val: 0.4596, Test: 0.4664\n",
      "Epoch: 59, Loss: 0.3452, Val: 0.4398, Test: 0.4621\n",
      "Epoch: 60, Loss: 0.3431, Val: 0.4464, Test: 0.4784\n",
      "Epoch: 61, Loss: 0.3458, Val: 0.4581, Test: 0.4773\n",
      "Epoch: 62, Loss: 0.3452, Val: 0.4309, Test: 0.4697\n",
      "Epoch: 63, Loss: 0.3407, Val: 0.4351, Test: 0.4642\n",
      "Epoch: 64, Loss: 0.3330, Val: 0.4508, Test: 0.4764\n",
      "Epoch: 65, Loss: 0.3390, Val: 0.4421, Test: 0.4822\n",
      "Epoch: 66, Loss: 0.3243, Val: 0.4103, Test: 0.4504\n",
      "Epoch: 67, Loss: 0.3136, Val: 0.4160, Test: 0.4551\n",
      "Epoch: 68, Loss: 0.2976, Val: 0.4041, Test: 0.4466\n",
      "Epoch: 69, Loss: 0.2922, Val: 0.4217, Test: 0.4531\n",
      "Epoch: 70, Loss: 0.2930, Val: 0.4023, Test: 0.4478\n",
      "Epoch: 71, Loss: 0.2887, Val: 0.4124, Test: 0.4553\n",
      "Epoch: 72, Loss: 0.2927, Val: 0.4105, Test: 0.4404\n",
      "Epoch: 73, Loss: 0.2857, Val: 0.4131, Test: 0.4459\n",
      "Epoch: 74, Loss: 0.2804, Val: 0.4203, Test: 0.4521\n",
      "Epoch: 75, Loss: 0.2768, Val: 0.4176, Test: 0.4482\n",
      "Epoch: 76, Loss: 0.2773, Val: 0.4010, Test: 0.4304\n",
      "Epoch: 77, Loss: 0.2806, Val: 0.4124, Test: 0.4416\n",
      "Epoch: 78, Loss: 0.2742, Val: 0.4076, Test: 0.4377\n",
      "Epoch: 79, Loss: 0.2696, Val: 0.4054, Test: 0.4347\n",
      "Epoch: 80, Loss: 0.2669, Val: 0.3986, Test: 0.4368\n",
      "Epoch: 81, Loss: 0.2678, Val: 0.4011, Test: 0.4313\n",
      "Epoch: 82, Loss: 0.2676, Val: 0.4233, Test: 0.4488\n",
      "Epoch: 83, Loss: 0.2645, Val: 0.4151, Test: 0.4394\n",
      "Epoch: 84, Loss: 0.2662, Val: 0.3942, Test: 0.4382\n",
      "Epoch: 85, Loss: 0.2623, Val: 0.3937, Test: 0.4341\n",
      "Epoch: 86, Loss: 0.2553, Val: 0.3959, Test: 0.4356\n",
      "Epoch: 87, Loss: 0.2575, Val: 0.4103, Test: 0.4361\n",
      "Epoch: 88, Loss: 0.2594, Val: 0.4052, Test: 0.4346\n",
      "Epoch: 89, Loss: 0.2526, Val: 0.3987, Test: 0.4293\n",
      "Epoch: 90, Loss: 0.2496, Val: 0.4200, Test: 0.4587\n",
      "Epoch: 91, Loss: 0.2575, Val: 0.4173, Test: 0.4514\n",
      "Epoch: 92, Loss: 0.2590, Val: 0.3974, Test: 0.4364\n",
      "Epoch: 93, Loss: 0.2527, Val: 0.4129, Test: 0.4399\n",
      "Epoch: 94, Loss: 0.2522, Val: 0.3949, Test: 0.4311\n",
      "Epoch: 95, Loss: 0.2465, Val: 0.4136, Test: 0.4389\n",
      "Epoch: 96, Loss: 0.2484, Val: 0.3984, Test: 0.4338\n",
      "Epoch: 97, Loss: 0.2421, Val: 0.3843, Test: 0.4233\n",
      "Epoch: 98, Loss: 0.2341, Val: 0.3887, Test: 0.4218\n",
      "Epoch: 99, Loss: 0.2248, Val: 0.3859, Test: 0.4238\n",
      "Epoch: 100, Loss: 0.2236, Val: 0.3799, Test: 0.4189\n",
      "Epoch: 101, Loss: 0.2233, Val: 0.3916, Test: 0.4267\n",
      "Epoch: 102, Loss: 0.2241, Val: 0.3903, Test: 0.4216\n",
      "Epoch: 103, Loss: 0.2207, Val: 0.3833, Test: 0.4213\n",
      "Epoch: 104, Loss: 0.2141, Val: 0.3878, Test: 0.4176\n",
      "Epoch: 105, Loss: 0.2193, Val: 0.3825, Test: 0.4216\n",
      "Epoch: 106, Loss: 0.2177, Val: 0.3875, Test: 0.4231\n",
      "Epoch: 107, Loss: 0.2185, Val: 0.3851, Test: 0.4223\n",
      "Epoch: 108, Loss: 0.2177, Val: 0.3862, Test: 0.4201\n",
      "Epoch: 109, Loss: 0.2143, Val: 0.3893, Test: 0.4177\n",
      "Epoch: 110, Loss: 0.2153, Val: 0.3906, Test: 0.4144\n",
      "Epoch: 111, Loss: 0.2120, Val: 0.3860, Test: 0.4184\n",
      "Epoch: 112, Loss: 0.2144, Val: 0.3924, Test: 0.4240\n",
      "Epoch: 113, Loss: 0.2123, Val: 0.3927, Test: 0.4225\n",
      "Epoch: 114, Loss: 0.2010, Val: 0.3924, Test: 0.4218\n",
      "Epoch: 115, Loss: 0.2054, Val: 0.3912, Test: 0.4206\n",
      "Epoch: 116, Loss: 0.2008, Val: 0.3880, Test: 0.4174\n",
      "Epoch: 117, Loss: 0.2023, Val: 0.3930, Test: 0.4209\n",
      "Epoch: 118, Loss: 0.2027, Val: 0.3961, Test: 0.4213\n",
      "Epoch: 119, Loss: 0.2010, Val: 0.3923, Test: 0.4214\n",
      "Epoch: 120, Loss: 0.2025, Val: 0.3957, Test: 0.4230\n",
      "Epoch: 121, Loss: 0.2016, Val: 0.3884, Test: 0.4203\n",
      "Epoch: 122, Loss: 0.2015, Val: 0.3879, Test: 0.4225\n",
      "Epoch: 123, Loss: 0.1974, Val: 0.3919, Test: 0.4193\n",
      "Epoch: 124, Loss: 0.1986, Val: 0.3875, Test: 0.4191\n",
      "Epoch: 125, Loss: 0.1982, Val: 0.3857, Test: 0.4153\n",
      "Epoch: 126, Loss: 0.2002, Val: 0.3906, Test: 0.4218\n",
      "Epoch: 127, Loss: 0.1897, Val: 0.3909, Test: 0.4208\n",
      "Epoch: 128, Loss: 0.1929, Val: 0.3864, Test: 0.4190\n",
      "Epoch: 129, Loss: 0.1929, Val: 0.3864, Test: 0.4165\n",
      "Epoch: 130, Loss: 0.1940, Val: 0.3857, Test: 0.4204\n",
      "Epoch: 131, Loss: 0.1924, Val: 0.3875, Test: 0.4154\n",
      "Epoch: 132, Loss: 0.1922, Val: 0.3892, Test: 0.4189\n",
      "Epoch: 133, Loss: 0.1889, Val: 0.3861, Test: 0.4171\n",
      "Epoch: 134, Loss: 0.1916, Val: 0.3852, Test: 0.4153\n",
      "Epoch: 135, Loss: 0.1905, Val: 0.3872, Test: 0.4186\n",
      "Epoch: 136, Loss: 0.1896, Val: 0.3846, Test: 0.4162\n",
      "Epoch: 137, Loss: 0.1879, Val: 0.3850, Test: 0.4170\n",
      "Epoch: 138, Loss: 0.1915, Val: 0.3833, Test: 0.4156\n",
      "Epoch: 139, Loss: 0.1891, Val: 0.3885, Test: 0.4180\n",
      "Epoch: 140, Loss: 0.1884, Val: 0.3867, Test: 0.4176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 141, Loss: 0.1876, Val: 0.3820, Test: 0.4163\n",
      "Epoch: 142, Loss: 0.1868, Val: 0.3839, Test: 0.4167\n",
      "Epoch: 143, Loss: 0.1871, Val: 0.3891, Test: 0.4168\n",
      "Epoch: 144, Loss: 0.1851, Val: 0.3823, Test: 0.4151\n",
      "Epoch: 145, Loss: 0.1876, Val: 0.3826, Test: 0.4161\n",
      "Epoch: 146, Loss: 0.1849, Val: 0.3819, Test: 0.4174\n",
      "Epoch: 147, Loss: 0.1843, Val: 0.3819, Test: 0.4159\n",
      "Epoch: 148, Loss: 0.1862, Val: 0.3813, Test: 0.4145\n",
      "Epoch: 149, Loss: 0.1848, Val: 0.3790, Test: 0.4141\n",
      "Epoch: 150, Loss: 0.1853, Val: 0.3831, Test: 0.4158\n",
      "Epoch: 151, Loss: 0.1825, Val: 0.3810, Test: 0.4135\n",
      "Epoch: 152, Loss: 0.1865, Val: 0.3792, Test: 0.4150\n",
      "Epoch: 153, Loss: 0.1854, Val: 0.3816, Test: 0.4128\n",
      "Epoch: 154, Loss: 0.1837, Val: 0.3799, Test: 0.4146\n",
      "Epoch: 155, Loss: 0.1880, Val: 0.3829, Test: 0.4160\n",
      "Epoch: 156, Loss: 0.1846, Val: 0.3826, Test: 0.4147\n",
      "Epoch: 157, Loss: 0.1865, Val: 0.3824, Test: 0.4152\n",
      "Epoch: 158, Loss: 0.1852, Val: 0.3785, Test: 0.4149\n",
      "Epoch: 159, Loss: 0.1850, Val: 0.3820, Test: 0.4152\n",
      "Epoch: 160, Loss: 0.1858, Val: 0.3819, Test: 0.4128\n",
      "Epoch: 161, Loss: 0.1809, Val: 0.3790, Test: 0.4147\n",
      "Epoch: 162, Loss: 0.1823, Val: 0.3827, Test: 0.4143\n",
      "Epoch: 163, Loss: 0.1814, Val: 0.3820, Test: 0.4145\n",
      "Epoch: 164, Loss: 0.1855, Val: 0.3810, Test: 0.4159\n",
      "Epoch: 165, Loss: 0.1833, Val: 0.3830, Test: 0.4145\n",
      "Epoch: 166, Loss: 0.1827, Val: 0.3845, Test: 0.4164\n",
      "Epoch: 167, Loss: 0.1814, Val: 0.3833, Test: 0.4171\n",
      "Epoch: 168, Loss: 0.1829, Val: 0.3830, Test: 0.4139\n",
      "Epoch: 169, Loss: 0.1810, Val: 0.3845, Test: 0.4179\n",
      "Epoch: 170, Loss: 0.1779, Val: 0.3857, Test: 0.4156\n",
      "Epoch: 171, Loss: 0.1806, Val: 0.3836, Test: 0.4149\n",
      "Epoch: 172, Loss: 0.1813, Val: 0.3856, Test: 0.4169\n",
      "Epoch: 173, Loss: 0.1798, Val: 0.3828, Test: 0.4165\n",
      "Epoch: 174, Loss: 0.1828, Val: 0.3843, Test: 0.4142\n",
      "Epoch: 175, Loss: 0.1873, Val: 0.3822, Test: 0.4165\n",
      "Epoch: 176, Loss: 0.1823, Val: 0.3841, Test: 0.4157\n",
      "Epoch: 177, Loss: 0.1837, Val: 0.3829, Test: 0.4155\n",
      "Epoch: 178, Loss: 0.1834, Val: 0.3814, Test: 0.4147\n",
      "Epoch: 179, Loss: 0.1806, Val: 0.3807, Test: 0.4148\n",
      "Epoch: 180, Loss: 0.1813, Val: 0.3835, Test: 0.4153\n",
      "Epoch: 181, Loss: 0.1822, Val: 0.3817, Test: 0.4145\n",
      "Epoch: 182, Loss: 0.1802, Val: 0.3819, Test: 0.4146\n",
      "Epoch: 183, Loss: 0.1836, Val: 0.3804, Test: 0.4144\n",
      "Epoch: 184, Loss: 0.1834, Val: 0.3831, Test: 0.4156\n",
      "Epoch: 185, Loss: 0.1858, Val: 0.3825, Test: 0.4157\n",
      "Epoch: 186, Loss: 0.1807, Val: 0.3835, Test: 0.4146\n",
      "Epoch: 187, Loss: 0.1844, Val: 0.3815, Test: 0.4155\n",
      "Epoch: 188, Loss: 0.1810, Val: 0.3829, Test: 0.4158\n",
      "Epoch: 189, Loss: 0.1791, Val: 0.3818, Test: 0.4168\n",
      "Epoch: 190, Loss: 0.1802, Val: 0.3832, Test: 0.4160\n",
      "Epoch: 191, Loss: 0.1815, Val: 0.3806, Test: 0.4171\n",
      "Epoch: 192, Loss: 0.1795, Val: 0.3821, Test: 0.4162\n",
      "Epoch: 193, Loss: 0.1786, Val: 0.3800, Test: 0.4157\n",
      "Epoch: 194, Loss: 0.1795, Val: 0.3805, Test: 0.4172\n",
      "Epoch: 195, Loss: 0.1815, Val: 0.3802, Test: 0.4162\n",
      "Epoch: 196, Loss: 0.1829, Val: 0.3801, Test: 0.4148\n",
      "Epoch: 197, Loss: 0.1828, Val: 0.3805, Test: 0.4145\n",
      "Epoch: 198, Loss: 0.1809, Val: 0.3789, Test: 0.4147\n",
      "Epoch: 199, Loss: 0.1777, Val: 0.3818, Test: 0.4148\n",
      "Epoch: 200, Loss: 0.1782, Val: 0.3817, Test: 0.4146\n",
      "\n",
      "Model's performance on the test set\n",
      "===================================\n",
      "MAE=0.41487740391776673\n",
      "Epoch=158\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphTransformerNet(node_dim_in=NODE_DIM,\n",
    "                            edge_dim_in=EDGE_DIM,\n",
    "                            pe_in_dim=PE_DIM,\n",
    "                            num_gt_layers=4, \n",
    "                            hidden_dim=128,\n",
    "                            num_heads=8,\n",
    "                            norm='ln',\n",
    "                            gt_aggregators=['sum', 'mean'],\n",
    "                            aggregators=['sum','mean','max', 'std'],\n",
    "                            dropout=0.1,\n",
    "                            act='gelu').to(device)\n",
    "\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    model = torch_geometric.compile(model) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "print(model)\n",
    "print(f\"Number of params: {model.num_parameters()//1000} k\")\n",
    "\n",
    "\n",
    "best_epoch = 0\n",
    "best_validation_loss = np.inf\n",
    "test_set_mae = np.inf\n",
    "for epoch in range(1, 201):\n",
    "    tr_loss = train(epoch, loss_func=train_loss)\n",
    "    va_loss = test(val_loader, loss_func=test_loss)\n",
    "    te_loss = test(test_loader, loss_func=test_loss)\n",
    "    scheduler.step(va_loss)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {tr_loss:.4f}, Val: {va_loss:.4f}, '\n",
    "          f'Test: {te_loss:.4f}')\n",
    "    if va_loss < best_validation_loss:\n",
    "        best_epoch = epoch\n",
    "        best_validation_loss = va_loss\n",
    "        test_set_mae = te_loss\n",
    "        \n",
    "print(\"\\nModel's performance on the test set\\n\"\n",
    "        \"===================================\\n\"\n",
    "        f'MAE={test_set_mae}\\n'\n",
    "        f'Epoch={best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch is available but CUDA is not. Defaulting to SciPy for SVD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_id</th>\n",
       "      <th>name</th>\n",
       "      <th>D</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>Q</th>\n",
       "      <th>alpha</th>\n",
       "      <th>alpha_weighted</th>\n",
       "      <th>entropy</th>\n",
       "      <th>has_esd</th>\n",
       "      <th>...</th>\n",
       "      <th>rf</th>\n",
       "      <th>sigma</th>\n",
       "      <th>spectral_norm</th>\n",
       "      <th>stable_rank</th>\n",
       "      <th>status</th>\n",
       "      <th>sv_max</th>\n",
       "      <th>warning</th>\n",
       "      <th>weak_rank_loss</th>\n",
       "      <th>xmax</th>\n",
       "      <th>xmin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.067975</td>\n",
       "      <td>76</td>\n",
       "      <td>128</td>\n",
       "      <td>1.684211</td>\n",
       "      <td>3.277394</td>\n",
       "      <td>4.026596</td>\n",
       "      <td>0.880096</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.569348</td>\n",
       "      <td>16.927663</td>\n",
       "      <td>7.085930</td>\n",
       "      <td>success</td>\n",
       "      <td>4.114324</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>16.927663</td>\n",
       "      <td>2.309725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.169242</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>2.642475</td>\n",
       "      <td>1.919284</td>\n",
       "      <td>0.945221</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.519396</td>\n",
       "      <td>5.325014</td>\n",
       "      <td>4.777690</td>\n",
       "      <td>success</td>\n",
       "      <td>2.307599</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>5.325014</td>\n",
       "      <td>1.223018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.058695</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.613188</td>\n",
       "      <td>3.193995</td>\n",
       "      <td>0.833103</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.280820</td>\n",
       "      <td>16.682456</td>\n",
       "      <td>10.798983</td>\n",
       "      <td>success</td>\n",
       "      <td>4.084416</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>16.682456</td>\n",
       "      <td>1.720429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.051392</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.685112</td>\n",
       "      <td>3.097238</td>\n",
       "      <td>0.845849</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.312917</td>\n",
       "      <td>14.239204</td>\n",
       "      <td>12.499730</td>\n",
       "      <td>success</td>\n",
       "      <td>3.773487</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>14.239204</td>\n",
       "      <td>1.942548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.089777</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.363919</td>\n",
       "      <td>3.915031</td>\n",
       "      <td>0.891117</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.058406</td>\n",
       "      <td>5.368865</td>\n",
       "      <td>23.248182</td>\n",
       "      <td>success</td>\n",
       "      <td>2.317081</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>5.368865</td>\n",
       "      <td>2.346537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.092734</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.811916</td>\n",
       "      <td>3.019194</td>\n",
       "      <td>0.933881</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.551462</td>\n",
       "      <td>6.194998</td>\n",
       "      <td>29.722648</td>\n",
       "      <td>success</td>\n",
       "      <td>2.488975</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>6.194998</td>\n",
       "      <td>2.269211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.094187</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.385702</td>\n",
       "      <td>3.254417</td>\n",
       "      <td>0.802269</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.202125</td>\n",
       "      <td>23.127790</td>\n",
       "      <td>7.721125</td>\n",
       "      <td>success</td>\n",
       "      <td>4.809136</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>23.127790</td>\n",
       "      <td>1.099728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.089884</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.555140</td>\n",
       "      <td>2.975651</td>\n",
       "      <td>0.878510</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.571347</td>\n",
       "      <td>6.870682</td>\n",
       "      <td>22.390432</td>\n",
       "      <td>success</td>\n",
       "      <td>2.621199</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>6.870682</td>\n",
       "      <td>2.481076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.055797</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.512242</td>\n",
       "      <td>1.845517</td>\n",
       "      <td>0.835333</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.259347</td>\n",
       "      <td>5.427620</td>\n",
       "      <td>11.910944</td>\n",
       "      <td>success</td>\n",
       "      <td>2.329725</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>5.427620</td>\n",
       "      <td>0.591549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.077458</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.698165</td>\n",
       "      <td>2.083067</td>\n",
       "      <td>0.833296</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.362050</td>\n",
       "      <td>5.916039</td>\n",
       "      <td>11.570880</td>\n",
       "      <td>success</td>\n",
       "      <td>2.432291</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>5.916039</td>\n",
       "      <td>0.902335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.071286</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.397413</td>\n",
       "      <td>1.459784</td>\n",
       "      <td>0.848598</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.220950</td>\n",
       "      <td>4.063493</td>\n",
       "      <td>15.186295</td>\n",
       "      <td>success</td>\n",
       "      <td>2.015811</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4.063493</td>\n",
       "      <td>0.494237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>33</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.089596</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.717829</td>\n",
       "      <td>1.423164</td>\n",
       "      <td>0.863690</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.294605</td>\n",
       "      <td>3.339181</td>\n",
       "      <td>18.910961</td>\n",
       "      <td>success</td>\n",
       "      <td>1.827343</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3.339181</td>\n",
       "      <td>0.663697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>40</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.061216</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.088303</td>\n",
       "      <td>2.830887</td>\n",
       "      <td>0.869401</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.409550</td>\n",
       "      <td>8.253687</td>\n",
       "      <td>18.397658</td>\n",
       "      <td>success</td>\n",
       "      <td>2.872923</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>8.253687</td>\n",
       "      <td>2.016178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>41</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.078886</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.996196</td>\n",
       "      <td>2.716842</td>\n",
       "      <td>0.872427</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.377246</td>\n",
       "      <td>8.067963</td>\n",
       "      <td>19.131699</td>\n",
       "      <td>success</td>\n",
       "      <td>2.840416</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>8.067963</td>\n",
       "      <td>1.928376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>42</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.112418</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.472309</td>\n",
       "      <td>2.434532</td>\n",
       "      <td>0.889472</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.459096</td>\n",
       "      <td>5.024902</td>\n",
       "      <td>26.513269</td>\n",
       "      <td>success</td>\n",
       "      <td>2.241629</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>5.024902</td>\n",
       "      <td>1.769061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>43</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.098773</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.035752</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>0.934986</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.607150</td>\n",
       "      <td>6.859228</td>\n",
       "      <td>28.602240</td>\n",
       "      <td>success</td>\n",
       "      <td>2.619013</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>6.859228</td>\n",
       "      <td>2.539173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>44</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.057035</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.896037</td>\n",
       "      <td>3.010779</td>\n",
       "      <td>0.857252</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.364893</td>\n",
       "      <td>10.955203</td>\n",
       "      <td>13.551238</td>\n",
       "      <td>success</td>\n",
       "      <td>3.309865</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>10.955203</td>\n",
       "      <td>1.807973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>45</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.106586</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.744078</td>\n",
       "      <td>2.365907</td>\n",
       "      <td>0.876800</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.275763</td>\n",
       "      <td>7.280923</td>\n",
       "      <td>21.654632</td>\n",
       "      <td>success</td>\n",
       "      <td>2.698318</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>7.280923</td>\n",
       "      <td>1.520541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>48</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.072460</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.330776</td>\n",
       "      <td>1.646230</td>\n",
       "      <td>0.843438</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.205343</td>\n",
       "      <td>5.085121</td>\n",
       "      <td>15.003355</td>\n",
       "      <td>success</td>\n",
       "      <td>2.255021</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>5.085121</td>\n",
       "      <td>0.568472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>51</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.063396</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.609699</td>\n",
       "      <td>1.931362</td>\n",
       "      <td>0.836099</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.321940</td>\n",
       "      <td>5.496306</td>\n",
       "      <td>13.421843</td>\n",
       "      <td>success</td>\n",
       "      <td>2.344420</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>5.496306</td>\n",
       "      <td>0.891251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>59</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.087383</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.812820</td>\n",
       "      <td>1.516502</td>\n",
       "      <td>0.861309</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336632</td>\n",
       "      <td>3.460503</td>\n",
       "      <td>18.222780</td>\n",
       "      <td>success</td>\n",
       "      <td>1.860243</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3.460503</td>\n",
       "      <td>0.755623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>62</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.096735</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.192707</td>\n",
       "      <td>1.536882</td>\n",
       "      <td>0.868029</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.478488</td>\n",
       "      <td>3.029512</td>\n",
       "      <td>20.515976</td>\n",
       "      <td>success</td>\n",
       "      <td>1.740549</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3.029512</td>\n",
       "      <td>0.971700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>69</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.074407</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.152826</td>\n",
       "      <td>2.871479</td>\n",
       "      <td>0.873049</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.406846</td>\n",
       "      <td>8.142603</td>\n",
       "      <td>19.132857</td>\n",
       "      <td>success</td>\n",
       "      <td>2.853525</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>8.142603</td>\n",
       "      <td>2.003364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>70</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.084220</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.168730</td>\n",
       "      <td>2.940711</td>\n",
       "      <td>0.871472</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.417372</td>\n",
       "      <td>8.473076</td>\n",
       "      <td>18.403433</td>\n",
       "      <td>success</td>\n",
       "      <td>2.910855</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>8.473076</td>\n",
       "      <td>2.067284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>71</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.121898</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.746751</td>\n",
       "      <td>2.642455</td>\n",
       "      <td>0.887403</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.549350</td>\n",
       "      <td>5.073010</td>\n",
       "      <td>26.528682</td>\n",
       "      <td>success</td>\n",
       "      <td>2.252334</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>5.073010</td>\n",
       "      <td>1.994913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>72</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.084481</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.528141</td>\n",
       "      <td>3.008680</td>\n",
       "      <td>0.934842</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.454068</td>\n",
       "      <td>7.124693</td>\n",
       "      <td>27.056527</td>\n",
       "      <td>success</td>\n",
       "      <td>2.669212</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>7.124693</td>\n",
       "      <td>2.140142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>73</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.056019</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.063981</td>\n",
       "      <td>2.972167</td>\n",
       "      <td>0.867758</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.390056</td>\n",
       "      <td>9.333280</td>\n",
       "      <td>16.607143</td>\n",
       "      <td>success</td>\n",
       "      <td>3.055042</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>9.333280</td>\n",
       "      <td>1.954064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>74</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.081662</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.422182</td>\n",
       "      <td>3.040218</td>\n",
       "      <td>0.876596</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.494426</td>\n",
       "      <td>7.733672</td>\n",
       "      <td>19.899706</td>\n",
       "      <td>success</td>\n",
       "      <td>2.780948</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>7.733672</td>\n",
       "      <td>2.242963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>77</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.051870</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.549198</td>\n",
       "      <td>2.177682</td>\n",
       "      <td>0.827407</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.282843</td>\n",
       "      <td>7.149273</td>\n",
       "      <td>10.642282</td>\n",
       "      <td>success</td>\n",
       "      <td>2.673813</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>7.149273</td>\n",
       "      <td>0.772487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>80</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.067152</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.564554</td>\n",
       "      <td>2.224985</td>\n",
       "      <td>0.815671</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.312911</td>\n",
       "      <td>7.372107</td>\n",
       "      <td>10.491767</td>\n",
       "      <td>success</td>\n",
       "      <td>2.715162</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>7.372107</td>\n",
       "      <td>0.876451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>88</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.082558</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.786066</td>\n",
       "      <td>1.441981</td>\n",
       "      <td>0.867014</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.315735</td>\n",
       "      <td>3.292827</td>\n",
       "      <td>17.400814</td>\n",
       "      <td>success</td>\n",
       "      <td>1.814615</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3.292827</td>\n",
       "      <td>0.635142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>91</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.107166</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.702031</td>\n",
       "      <td>1.620013</td>\n",
       "      <td>0.874297</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.636875</td>\n",
       "      <td>2.739057</td>\n",
       "      <td>20.423805</td>\n",
       "      <td>success</td>\n",
       "      <td>1.655010</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2.739057</td>\n",
       "      <td>0.993960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>98</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.056371</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.149242</td>\n",
       "      <td>3.384733</td>\n",
       "      <td>0.864488</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.429848</td>\n",
       "      <td>11.878930</td>\n",
       "      <td>13.550305</td>\n",
       "      <td>success</td>\n",
       "      <td>3.446582</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>11.878930</td>\n",
       "      <td>2.176120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>99</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.065148</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.858577</td>\n",
       "      <td>3.009936</td>\n",
       "      <td>0.860826</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.328553</td>\n",
       "      <td>11.296640</td>\n",
       "      <td>14.295064</td>\n",
       "      <td>success</td>\n",
       "      <td>3.361047</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>11.296640</td>\n",
       "      <td>1.770015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>100</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.128475</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.736339</td>\n",
       "      <td>2.523037</td>\n",
       "      <td>0.891918</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.526609</td>\n",
       "      <td>4.734453</td>\n",
       "      <td>27.134252</td>\n",
       "      <td>success</td>\n",
       "      <td>2.175880</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4.734453</td>\n",
       "      <td>1.817009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>101</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.096480</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.046408</td>\n",
       "      <td>3.367549</td>\n",
       "      <td>0.937107</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.621846</td>\n",
       "      <td>6.795660</td>\n",
       "      <td>27.102150</td>\n",
       "      <td>success</td>\n",
       "      <td>2.606849</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>6.795660</td>\n",
       "      <td>2.364292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>102</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.065219</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.089273</td>\n",
       "      <td>3.183867</td>\n",
       "      <td>0.859943</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.445434</td>\n",
       "      <td>10.730511</td>\n",
       "      <td>15.031858</td>\n",
       "      <td>success</td>\n",
       "      <td>3.275746</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>10.730511</td>\n",
       "      <td>2.316772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>103</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.140127</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.737851</td>\n",
       "      <td>3.421963</td>\n",
       "      <td>0.897846</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.149098</td>\n",
       "      <td>3.948063</td>\n",
       "      <td>32.193475</td>\n",
       "      <td>success</td>\n",
       "      <td>1.986973</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3.948063</td>\n",
       "      <td>2.350787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>106</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.140004</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.324327</td>\n",
       "      <td>0.552614</td>\n",
       "      <td>0.896504</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.992069</td>\n",
       "      <td>1.269961</td>\n",
       "      <td>33.224015</td>\n",
       "      <td>success</td>\n",
       "      <td>1.126926</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1.269961</td>\n",
       "      <td>0.750608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>109</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.147496</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.699419</td>\n",
       "      <td>0.396383</td>\n",
       "      <td>0.897301</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492844</td>\n",
       "      <td>1.279816</td>\n",
       "      <td>33.123957</td>\n",
       "      <td>success</td>\n",
       "      <td>1.131289</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1.279816</td>\n",
       "      <td>0.564812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>117</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.070806</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.805850</td>\n",
       "      <td>1.601395</td>\n",
       "      <td>0.855734</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.354156</td>\n",
       "      <td>3.721639</td>\n",
       "      <td>15.424815</td>\n",
       "      <td>success</td>\n",
       "      <td>1.929155</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3.721639</td>\n",
       "      <td>0.711643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>120</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.064256</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.958163</td>\n",
       "      <td>1.743988</td>\n",
       "      <td>0.859570</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.391633</td>\n",
       "      <td>3.886433</td>\n",
       "      <td>14.810732</td>\n",
       "      <td>success</td>\n",
       "      <td>1.971404</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3.886433</td>\n",
       "      <td>0.760637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>131</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.512769</td>\n",
       "      <td>3.373935</td>\n",
       "      <td>0.767634</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.183450</td>\n",
       "      <td>22.014872</td>\n",
       "      <td>3.952881</td>\n",
       "      <td>success</td>\n",
       "      <td>4.692001</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>22.014872</td>\n",
       "      <td>0.313634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>136</td>\n",
       "      <td>Linear</td>\n",
       "      <td>0.104998</td>\n",
       "      <td>128</td>\n",
       "      <td>512</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.020015</td>\n",
       "      <td>3.105285</td>\n",
       "      <td>0.867372</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250552</td>\n",
       "      <td>10.671732</td>\n",
       "      <td>5.160776</td>\n",
       "      <td>success</td>\n",
       "      <td>3.266762</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>10.671732</td>\n",
       "      <td>0.295071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    layer_id    name         D    M    N          Q     alpha  alpha_weighted  \\\n",
       "0          1  Linear  0.067975   76  128   1.684211  3.277394        4.026596   \n",
       "1          2  Linear  0.169242   10  128  12.800000  2.642475        1.919284   \n",
       "2         11  Linear  0.058695  128  128   1.000000  2.613188        3.193995   \n",
       "3         12  Linear  0.051392  128  128   1.000000  2.685112        3.097238   \n",
       "4         13  Linear  0.089777  128  128   1.000000  5.363919        3.915031   \n",
       "5         14  Linear  0.092734  128  256   2.000000  3.811916        3.019194   \n",
       "6         15  Linear  0.094187  128  128   1.000000  2.385702        3.254417   \n",
       "7         16  Linear  0.089884  128  128   1.000000  3.555140        2.975651   \n",
       "8         19  Linear  0.055797  128  128   1.000000  2.512242        1.845517   \n",
       "9         22  Linear  0.077458  128  128   1.000000  2.698165        2.083067   \n",
       "10        30  Linear  0.071286  128  128   1.000000  2.397413        1.459784   \n",
       "11        33  Linear  0.089596  128  128   1.000000  2.717829        1.423164   \n",
       "12        40  Linear  0.061216  128  128   1.000000  3.088303        2.830887   \n",
       "13        41  Linear  0.078886  128  128   1.000000  2.996196        2.716842   \n",
       "14        42  Linear  0.112418  128  128   1.000000  3.472309        2.434532   \n",
       "15        43  Linear  0.098773  128  256   2.000000  4.035752        3.375000   \n",
       "16        44  Linear  0.057035  128  128   1.000000  2.896037        3.010779   \n",
       "17        45  Linear  0.106586  128  128   1.000000  2.744078        2.365907   \n",
       "18        48  Linear  0.072460  128  128   1.000000  2.330776        1.646230   \n",
       "19        51  Linear  0.063396  128  128   1.000000  2.609699        1.931362   \n",
       "20        59  Linear  0.087383  128  128   1.000000  2.812820        1.516502   \n",
       "21        62  Linear  0.096735  128  128   1.000000  3.192707        1.536882   \n",
       "22        69  Linear  0.074407  128  128   1.000000  3.152826        2.871479   \n",
       "23        70  Linear  0.084220  128  128   1.000000  3.168730        2.940711   \n",
       "24        71  Linear  0.121898  128  128   1.000000  3.746751        2.642455   \n",
       "25        72  Linear  0.084481  128  256   2.000000  3.528141        3.008680   \n",
       "26        73  Linear  0.056019  128  128   1.000000  3.063981        2.972167   \n",
       "27        74  Linear  0.081662  128  128   1.000000  3.422182        3.040218   \n",
       "28        77  Linear  0.051870  128  128   1.000000  2.549198        2.177682   \n",
       "29        80  Linear  0.067152  128  128   1.000000  2.564554        2.224985   \n",
       "30        88  Linear  0.082558  128  128   1.000000  2.786066        1.441981   \n",
       "31        91  Linear  0.107166  128  128   1.000000  3.702031        1.620013   \n",
       "32        98  Linear  0.056371  128  128   1.000000  3.149242        3.384733   \n",
       "33        99  Linear  0.065148  128  128   1.000000  2.858577        3.009936   \n",
       "34       100  Linear  0.128475  128  128   1.000000  3.736339        2.523037   \n",
       "35       101  Linear  0.096480  128  256   2.000000  4.046408        3.367549   \n",
       "36       102  Linear  0.065219  128  128   1.000000  3.089273        3.183867   \n",
       "37       103  Linear  0.140127  128  128   1.000000  5.737851        3.421963   \n",
       "38       106  Linear  0.140004  128  128   1.000000  5.324327        0.552614   \n",
       "39       109  Linear  0.147496  128  128   1.000000  3.699419        0.396383   \n",
       "40       117  Linear  0.070806  128  128   1.000000  2.805850        1.601395   \n",
       "41       120  Linear  0.064256  128  128   1.000000  2.958163        1.743988   \n",
       "42       131  Linear  0.055040  128  512   4.000000  2.512769        3.373935   \n",
       "43       136  Linear  0.104998  128  512   4.000000  3.020015        3.105285   \n",
       "\n",
       "     entropy  has_esd  ...  rf     sigma  spectral_norm  stable_rank   status  \\\n",
       "0   0.880096     True  ...   1  0.569348      16.927663     7.085930  success   \n",
       "1   0.945221     True  ...   1  0.519396       5.325014     4.777690  success   \n",
       "2   0.833103     True  ...   1  0.280820      16.682456    10.798983  success   \n",
       "3   0.845849     True  ...   1  0.312917      14.239204    12.499730  success   \n",
       "4   0.891117     True  ...   1  1.058406       5.368865    23.248182  success   \n",
       "5   0.933881     True  ...   1  0.551462       6.194998    29.722648  success   \n",
       "6   0.802269     True  ...   1  0.202125      23.127790     7.721125  success   \n",
       "7   0.878510     True  ...   1  0.571347       6.870682    22.390432  success   \n",
       "8   0.835333     True  ...   1  0.259347       5.427620    11.910944  success   \n",
       "9   0.833296     True  ...   1  0.362050       5.916039    11.570880  success   \n",
       "10  0.848598     True  ...   1  0.220950       4.063493    15.186295  success   \n",
       "11  0.863690     True  ...   1  0.294605       3.339181    18.910961  success   \n",
       "12  0.869401     True  ...   1  0.409550       8.253687    18.397658  success   \n",
       "13  0.872427     True  ...   1  0.377246       8.067963    19.131699  success   \n",
       "14  0.889472     True  ...   1  0.459096       5.024902    26.513269  success   \n",
       "15  0.934986     True  ...   1  0.607150       6.859228    28.602240  success   \n",
       "16  0.857252     True  ...   1  0.364893      10.955203    13.551238  success   \n",
       "17  0.876800     True  ...   1  0.275763       7.280923    21.654632  success   \n",
       "18  0.843438     True  ...   1  0.205343       5.085121    15.003355  success   \n",
       "19  0.836099     True  ...   1  0.321940       5.496306    13.421843  success   \n",
       "20  0.861309     True  ...   1  0.336632       3.460503    18.222780  success   \n",
       "21  0.868029     True  ...   1  0.478488       3.029512    20.515976  success   \n",
       "22  0.873049     True  ...   1  0.406846       8.142603    19.132857  success   \n",
       "23  0.871472     True  ...   1  0.417372       8.473076    18.403433  success   \n",
       "24  0.887403     True  ...   1  0.549350       5.073010    26.528682  success   \n",
       "25  0.934842     True  ...   1  0.454068       7.124693    27.056527  success   \n",
       "26  0.867758     True  ...   1  0.390056       9.333280    16.607143  success   \n",
       "27  0.876596     True  ...   1  0.494426       7.733672    19.899706  success   \n",
       "28  0.827407     True  ...   1  0.282843       7.149273    10.642282  success   \n",
       "29  0.815671     True  ...   1  0.312911       7.372107    10.491767  success   \n",
       "30  0.867014     True  ...   1  0.315735       3.292827    17.400814  success   \n",
       "31  0.874297     True  ...   1  0.636875       2.739057    20.423805  success   \n",
       "32  0.864488     True  ...   1  0.429848      11.878930    13.550305  success   \n",
       "33  0.860826     True  ...   1  0.328553      11.296640    14.295064  success   \n",
       "34  0.891918     True  ...   1  0.526609       4.734453    27.134252  success   \n",
       "35  0.937107     True  ...   1  0.621846       6.795660    27.102150  success   \n",
       "36  0.859943     True  ...   1  0.445434      10.730511    15.031858  success   \n",
       "37  0.897846     True  ...   1  1.149098       3.948063    32.193475  success   \n",
       "38  0.896504     True  ...   1  0.992069       1.269961    33.224015  success   \n",
       "39  0.897301     True  ...   1  0.492844       1.279816    33.123957  success   \n",
       "40  0.855734     True  ...   1  0.354156       3.721639    15.424815  success   \n",
       "41  0.859570     True  ...   1  0.391633       3.886433    14.810732  success   \n",
       "42  0.767634     True  ...   1  0.183450      22.014872     3.952881  success   \n",
       "43  0.867372     True  ...   1  0.250552      10.671732     5.160776  success   \n",
       "\n",
       "      sv_max  warning  weak_rank_loss       xmax      xmin  \n",
       "0   4.114324                        0  16.927663  2.309725  \n",
       "1   2.307599                        0   5.325014  1.223018  \n",
       "2   4.084416                        0  16.682456  1.720429  \n",
       "3   3.773487                        1  14.239204  1.942548  \n",
       "4   2.317081                        0   5.368865  2.346537  \n",
       "5   2.488975                        0   6.194998  2.269211  \n",
       "6   4.809136                        0  23.127790  1.099728  \n",
       "7   2.621199                        0   6.870682  2.481076  \n",
       "8   2.329725                        0   5.427620  0.591549  \n",
       "9   2.432291                        0   5.916039  0.902335  \n",
       "10  2.015811                        0   4.063493  0.494237  \n",
       "11  1.827343                        0   3.339181  0.663697  \n",
       "12  2.872923                        1   8.253687  2.016178  \n",
       "13  2.840416                        0   8.067963  1.928376  \n",
       "14  2.241629                        0   5.024902  1.769061  \n",
       "15  2.619013                        0   6.859228  2.539173  \n",
       "16  3.309865                        0  10.955203  1.807973  \n",
       "17  2.698318                        0   7.280923  1.520541  \n",
       "18  2.255021                        0   5.085121  0.568472  \n",
       "19  2.344420                        1   5.496306  0.891251  \n",
       "20  1.860243                        0   3.460503  0.755623  \n",
       "21  1.740549                        0   3.029512  0.971700  \n",
       "22  2.853525                        0   8.142603  2.003364  \n",
       "23  2.910855                        0   8.473076  2.067284  \n",
       "24  2.252334                        0   5.073010  1.994913  \n",
       "25  2.669212                        0   7.124693  2.140142  \n",
       "26  3.055042                        0   9.333280  1.954064  \n",
       "27  2.780948                        0   7.733672  2.242963  \n",
       "28  2.673813                        0   7.149273  0.772487  \n",
       "29  2.715162                        0   7.372107  0.876451  \n",
       "30  1.814615                        0   3.292827  0.635142  \n",
       "31  1.655010                        0   2.739057  0.993960  \n",
       "32  3.446582                        0  11.878930  2.176120  \n",
       "33  3.361047                        0  11.296640  1.770015  \n",
       "34  2.175880                        0   4.734453  1.817009  \n",
       "35  2.606849                        0   6.795660  2.364292  \n",
       "36  3.275746                        0  10.730511  2.316772  \n",
       "37  1.986973                        0   3.948063  2.350787  \n",
       "38  1.126926                        0   1.269961  0.750608  \n",
       "39  1.131289                        0   1.279816  0.564812  \n",
       "40  1.929155                        0   3.721639  0.711643  \n",
       "41  1.971404                        0   3.886433  0.760637  \n",
       "42  4.692001                        0  22.014872  0.313634  \n",
       "43  3.266762                        0  10.671732  0.295071  \n",
       "\n",
       "[44 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weightwatcher as ww\n",
    "\n",
    "watcher = ww.WeightWatcher(model=model)\n",
    "details = watcher.analyze(plot=False)\n",
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log_norm': 2.012117284692378,\n",
       " 'alpha': 3.215042345276656,\n",
       " 'alpha_weighted': 2.504157234497073,\n",
       " 'log_alpha_norm': 3.0239557866120674,\n",
       " 'log_spectral_norm': 0.8036157617573815,\n",
       " 'stable_rank': 18.009767940281925}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watcher.get_summary(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
