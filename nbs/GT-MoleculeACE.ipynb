{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pawel/anaconda3/envs/gt/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version: 1.21.6\n",
      "Rdkit version: 2022.09.5\n",
      "Torch version: 1.13.1\n",
      "TorchMetrics version: 0.11.4\n"
     ]
    }
   ],
   "source": [
    "# Standard\n",
    "import logging\n",
    "\n",
    "# Third party\n",
    "import numpy as np\n",
    "import rdkit\n",
    "from rdkit import RDLogger\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torchmetrics\n",
    "from torchmetrics import MeanSquaredError\n",
    "from pytorch_lamb import Lamb\n",
    "\n",
    "# GT-PyG\n",
    "from gt_pyg.data.utils import (\n",
    "    get_tensor_data, \n",
    "    get_node_dim,\n",
    "    get_edge_dim,\n",
    "    get_molecule_ace_datasets\n",
    ")\n",
    "from gt_pyg.nn.model import GraphTransformerNet\n",
    "\n",
    "# Turn off majority of RDKit warnings\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL)\n",
    "\n",
    "\n",
    "# Set a random seed for a reproducibility purposes\n",
    "torch.manual_seed(192837465)\n",
    "\n",
    "# Setup the logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Log the used versions of RDkit and torch\n",
    "print(f'Numpy version: {np.__version__}')\n",
    "print(f'Rdkit version: {rdkit.__version__}')\n",
    "print(f'Torch version: {torch.__version__}')\n",
    "print(f'TorchMetrics version: {torchmetrics.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the MoleculeACE data\n",
    "\n",
    "**Note**: To use the code below, make sure that the chosen endpoint is a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 478\n",
      "Number of validation examples: 120\n",
      "Number of test examples: 152\n"
     ]
    }
   ],
   "source": [
    "PE_DIM = 6\n",
    "(tr, va, te) = get_molecule_ace_datasets('CHEMBL2034_Ki', min_num_atoms=0, \n",
    "                                         training_fraction=0.8, valid_fraction=0.2)\n",
    "tr_dataset = get_tensor_data(tr.SMILES.to_list(), tr.Y.to_list(), pe_dim=PE_DIM)\n",
    "va_dataset = get_tensor_data(va.SMILES.to_list(), va.Y.to_list(), pe_dim=PE_DIM)\n",
    "te_dataset = get_tensor_data(te.SMILES.to_list(), te.Y.to_list(), pe_dim=PE_DIM)\n",
    "NODE_DIM = get_node_dim()\n",
    "EDGE_DIM = get_edge_dim()\n",
    "\n",
    "print(f'Number of training examples: {len(tr_dataset)}')\n",
    "print(f'Number of validation examples: {len(va_dataset)}')\n",
    "print(f'Number of test examples: {len(te_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(tr_dataset, batch_size=64)\n",
    "val_loader = DataLoader(va_dataset, batch_size=512)\n",
    "test_loader = DataLoader(te_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loss_func):\n",
    "    \"\"\"\n",
    "    RMSE = MSE ** 0.5\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_mse = MeanSquaredError()\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # randomly flip sign of eigenvectors\n",
    "        batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM)).float() - 1.0)\n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch, zero_var=False)\n",
    "        loss = loss_func(out.squeeze(), data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_mse.update(out.squeeze(), data.y)\n",
    "\n",
    "    return train_mse.compute()**0.5\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    \"\"\"\n",
    "    RMSE = MSE ** 0.5\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_mse = MeanSquaredError()\n",
    "\n",
    "    total_error = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        # randomly flip sign of eigenvectors\n",
    "        batch_pe = data.pe * (2 * torch.randint(low=0, high=2, size=(1, PE_DIM)).float() - 1.0)\n",
    "        (out,_) = model(data.x, data.edge_index, data.edge_attr, batch_pe, data.batch)\n",
    "        \n",
    "        test_mse.update(out.squeeze(), data.y)\n",
    "        \n",
    "    return test_mse.compute() ** 0.5\n",
    "\n",
    "train_loss = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slighlty optimized Graph Transformer architecture\n",
    "\n",
    "1. `gelu` activation is used instead of `relu`\n",
    "2. Multiaggregator used for global pooling\n",
    "3. Multiaggregator used for message passing\n",
    "4. Lamb optmizer for this [paper](https://arxiv.org/abs/1904.00962), after this [repo](https://github.com/cybertronai/pytorch-lamb)\n",
    "\n",
    "Number of params 873k instead of 709k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformerNet(\n",
      "  (node_emb): Linear(in_features=76, out_features=128, bias=False)\n",
      "  (edge_emb): Linear(in_features=10, out_features=128, bias=False)\n",
      "  (pe_emb): Linear(in_features=6, out_features=128, bias=False)\n",
      "  (gt_layers): ModuleList(\n",
      "    (0): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "    (1): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "    (2): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "    (3): GTConv(128, 128, heads=8, aggrs: sum,mean)\n",
      "  )\n",
      "  (global_pool): MultiAggregation([\n",
      "    SumAggregation(),\n",
      "    MeanAggregation(),\n",
      "    MaxAggregation(),\n",
      "    StdAggregation(),\n",
      "  ], mode=cat)\n",
      "  (mu_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (log_var_mlp): MLP(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of params: 873 k\n",
      "Epoch: 01, Loss: 1.5983, Val: 1.4130, Test: 1.2706\n",
      "Epoch: 02, Loss: 1.1113, Val: 2.5528, Test: 2.3509\n",
      "Epoch: 03, Loss: 1.0373, Val: 1.8718, Test: 1.7425\n",
      "Epoch: 04, Loss: 1.0921, Val: 2.0328, Test: 1.8280\n",
      "Epoch: 05, Loss: 0.8414, Val: 1.3030, Test: 1.1410\n",
      "Epoch: 06, Loss: 0.8168, Val: 1.1238, Test: 1.0680\n",
      "Epoch: 07, Loss: 0.7653, Val: 0.9784, Test: 0.9918\n",
      "Epoch: 08, Loss: 0.7504, Val: 1.2863, Test: 1.2946\n",
      "Epoch: 09, Loss: 0.7306, Val: 1.2628, Test: 1.2029\n",
      "Epoch: 10, Loss: 0.6772, Val: 0.9470, Test: 0.9418\n",
      "Epoch: 11, Loss: 0.7109, Val: 1.0486, Test: 1.0212\n",
      "Epoch: 12, Loss: 0.7202, Val: 0.9255, Test: 0.9489\n",
      "Epoch: 13, Loss: 0.7139, Val: 0.7943, Test: 0.8346\n",
      "Epoch: 14, Loss: 0.6778, Val: 0.9775, Test: 0.9577\n",
      "Epoch: 15, Loss: 0.6631, Val: 0.8260, Test: 0.9141\n",
      "Epoch: 16, Loss: 0.6205, Val: 1.0378, Test: 1.0287\n",
      "Epoch: 17, Loss: 0.5644, Val: 0.9875, Test: 1.0342\n",
      "Epoch: 18, Loss: 0.5872, Val: 1.0595, Test: 1.0486\n",
      "Epoch: 19, Loss: 0.5676, Val: 1.0316, Test: 1.0448\n",
      "Epoch: 20, Loss: 0.5881, Val: 0.8302, Test: 0.9105\n",
      "Epoch: 21, Loss: 0.5070, Val: 0.7855, Test: 0.8804\n",
      "Epoch: 22, Loss: 0.4972, Val: 0.6937, Test: 0.8355\n",
      "Epoch: 23, Loss: 0.5425, Val: 0.9237, Test: 0.8806\n",
      "Epoch: 24, Loss: 0.5485, Val: 0.7734, Test: 0.8464\n",
      "Epoch: 25, Loss: 0.4703, Val: 0.8356, Test: 0.8388\n",
      "Epoch: 26, Loss: 0.4878, Val: 0.8386, Test: 0.8854\n",
      "Epoch: 27, Loss: 0.4619, Val: 0.8774, Test: 0.9157\n",
      "Epoch: 28, Loss: 0.4873, Val: 0.7836, Test: 0.8312\n",
      "Epoch: 29, Loss: 0.4665, Val: 0.8482, Test: 0.8482\n",
      "Epoch: 30, Loss: 0.4774, Val: 1.2483, Test: 1.1583\n",
      "Epoch: 31, Loss: 0.5285, Val: 0.7639, Test: 0.8574\n",
      "Epoch: 32, Loss: 0.4662, Val: 0.8082, Test: 0.9007\n",
      "Epoch: 33, Loss: 0.4642, Val: 0.8475, Test: 0.8957\n",
      "Epoch: 34, Loss: 0.4372, Val: 0.7175, Test: 0.8191\n",
      "Epoch: 35, Loss: 0.3771, Val: 0.7790, Test: 0.8319\n",
      "Epoch: 36, Loss: 0.3520, Val: 0.7649, Test: 0.8190\n",
      "Epoch: 37, Loss: 0.3678, Val: 0.7424, Test: 0.7898\n",
      "Epoch: 38, Loss: 0.3246, Val: 0.8417, Test: 0.8462\n",
      "Epoch: 39, Loss: 0.3383, Val: 0.6942, Test: 0.7850\n",
      "Epoch: 40, Loss: 0.3058, Val: 0.7180, Test: 0.8132\n",
      "Epoch: 41, Loss: 0.3316, Val: 0.7172, Test: 0.8365\n",
      "Epoch: 42, Loss: 0.3080, Val: 0.7462, Test: 0.8077\n",
      "Epoch: 43, Loss: 0.3135, Val: 0.7406, Test: 0.8249\n",
      "Epoch: 44, Loss: 0.2885, Val: 0.7232, Test: 0.8629\n",
      "Epoch: 45, Loss: 0.2831, Val: 0.7258, Test: 0.8134\n",
      "Epoch: 46, Loss: 0.2542, Val: 0.7067, Test: 0.8012\n",
      "Epoch: 47, Loss: 0.2288, Val: 0.6821, Test: 0.7830\n",
      "Epoch: 48, Loss: 0.2316, Val: 0.6988, Test: 0.7868\n",
      "Epoch: 49, Loss: 0.2231, Val: 0.6790, Test: 0.7634\n",
      "Epoch: 50, Loss: 0.2177, Val: 0.7022, Test: 0.8021\n",
      "\n",
      "Model's performance on the test set\n",
      "===================================\n",
      "RMSE=0.7634494304656982\n",
      "Epoch=49\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphTransformerNet(node_dim_in=NODE_DIM,\n",
    "                            edge_dim_in=EDGE_DIM,\n",
    "                            pe_in_dim=PE_DIM,\n",
    "                            num_gt_layers=4, \n",
    "                            hidden_dim=128,\n",
    "                            num_heads=8,\n",
    "                            norm='bn',\n",
    "                            gt_aggregators=['sum', 'mean'],\n",
    "                            aggregators=['sum','mean','max', 'std'],\n",
    "                            dropout=0.1,\n",
    "                            act='gelu').to(device)\n",
    "\n",
    "if int(torch.__version__.split('.')[0]) >= 2:\n",
    "    model = torch_geometric.compile(model) \n",
    "\n",
    "optimizer = Lamb(model.parameters(),\n",
    "                 lr=0.005,\n",
    "                 weight_decay=0.05,\n",
    "                 betas=(.9, .999), \n",
    "                 adam=False)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10,\n",
    "                              min_lr=0.00001)\n",
    "\n",
    "print(model)\n",
    "print(f\"Number of params: {model.num_parameters()//1000} k\")\n",
    "\n",
    "\n",
    "best_epoch = 0\n",
    "best_validation_loss = np.inf\n",
    "test_set_rmse = np.inf\n",
    "for epoch in range(1, 51):\n",
    "    tr_loss = train(epoch, loss_func=train_loss)\n",
    "    va_loss = test(val_loader)\n",
    "    te_loss = test(test_loader)\n",
    "    scheduler.step(va_loss)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {tr_loss:.4f}, Val: {va_loss:.4f}, '\n",
    "          f'Test: {te_loss:.4f}')\n",
    "    if va_loss < best_validation_loss:\n",
    "        best_epoch = epoch\n",
    "        best_validation_loss = va_loss\n",
    "        test_set_rmse = te_loss\n",
    "        \n",
    "print(\"\\nModel's performance on the test set\\n\"\n",
    "        \"===================================\\n\"\n",
    "        f'RMSE={test_set_rmse}\\n'\n",
    "        f'Epoch={best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
